This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
1-introduction/1-authentication.ipynb
1-introduction/2-environment_setup.ipynb
1-introduction/3-quick_start.ipynb
2-notebooks/1-chat_completion/1-basic-chat-completion.ipynb
2-notebooks/1-chat_completion/2-embeddings.ipynb
2-notebooks/1-chat_completion/3-basic-rag.ipynb
2-notebooks/1-chat_completion/4-phi-4.ipynb
2-notebooks/2-agent_service/1-basics.ipynb
2-notebooks/2-agent_service/2-code_interpreter.ipynb
2-notebooks/2-agent_service/3-file-search.ipynb
2-notebooks/2-agent_service/4-bing_grounding.ipynb
2-notebooks/2-agent_service/5-agents-aisearch.ipynb
2-notebooks/2-agent_service/6-agents-az-functions.ipynb
2-notebooks/3-quality_attributes/1-Observability.ipynb
2-notebooks/3-quality_attributes/2-evaluation.ipynb
3-ai-native-e2e-sample/backend/README.md
3-ai-native-e2e-sample/frontend/README.md
assets/diagrams/agent-tools-flow.mmd
assets/diagrams/agent-tools.mmd
assets/diagrams/e2e-architecture.mmd
assets/diagrams/rag-flow.mmd
assets/overrides/index.html
index.md
integrations/azure-services.md
llms.txt
README.md
workshop/architecture.md
workshop/README.md

================================================================
Files
================================================================

================
File: 1-introduction/1-authentication.ipynb
================
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c198c79",
   "metadata": {},
   "source": [
    "# Troubleshooting Authentication with DefaultAzureCredential\n",
    "\n",
    "## Overview\n",
    "\n",
    "DefaultAzureCredential is the recommended way to handle authentication in Azure applications. It provides a streamlined authentication flow by trying multiple credential types in sequence until one succeeds. This notebook will help you troubleshoot common authentication issues and ensure proper setup.\n",
    "\n",
    "## Understanding DefaultAzureCredential\n",
    "\n",
    "DefaultAzureCredential attempts authentication methods in the following order:\n",
    "\n",
    "1. Environment Credentials\n",
    "2. Workload Identity (in Kubernetes)\n",
    "3. Managed Identity\n",
    "4. Azure CLI Credentials\n",
    "5. Azure PowerShell Credentials\n",
    "6. Visual Studio Code Credentials\n",
    "7. Interactive Browser Authentication (as fallback)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Ensure you have the following installed:\n",
    "- Azure CLI\n",
    "- Azure Developer CLI (optional)\n",
    "- Python Virtual Environment or Conda (use `uv venv` or `conda create`)\n",
    "- Required role assignments (Azure AI Developer)\n",
    "- Jupyter Notebook environment - kernel configured to use Python 3.8 or later\n",
    "\n",
    "## Authentication Methods\n",
    "\n",
    "### 1. Using Azure CLI (Recommended for Local Development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c8ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install azure-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7cb3d",
   "metadata": {},
   "source": [
    "# First, we'll authenticate using Azure CLI\n",
    "This is the recommended approach for local development.\n",
    "\n",
    "When you run the code below, you will be redirected to:\n",
    "- Either the Azure portal in your browser to complete the login \n",
    "- Or use Windows login if you're already signed in to your machine\n",
    "\n",
    "The code will:\n",
    "1. Load environment variables from .env file, including the TENANT_ID\n",
    "2. Use Azure CLI to log in to your specific tenant  \n",
    "3. Test authentication by attempting to get a token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe28d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get tenant ID from environment variable\n",
    "tenant_id = os.getenv(\"TENANT_ID\")\n",
    "\n",
    "# Azure login with specific tenant\n",
    "!az login --tenant {tenant_id}\n",
    "\n",
    "# Get subscription ID from connection string\n",
    "conn_str = os.getenv(\"PROJECT_CONNECTION_STRING\")\n",
    "subscription_id = conn_str.split(';')[1] if conn_str else None\n",
    "\n",
    "if subscription_id:\n",
    "    # Set the subscription\n",
    "    !az account set --subscription {subscription_id}\n",
    "    print(f\"‚úì Successfully set subscription: {subscription_id}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Could not get subscription ID from PROJECT_CONNECTION_STRING\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c3458a",
   "metadata": {},
   "source": [
    "# Next, we'll test the authentication by attempting to get a token using DefaultAzureCredential\n",
    "\n",
    "DefaultAzureCredential will try multiple authentication methods in this order:\n",
    "1. Environment credentials (if environment variables are set)\n",
    "2. Managed Identity credentials (if running in Azure)\n",
    "3. Shared Token Cache credentials (from previous logins) \n",
    "4. Visual Studio Code credentials (if using VS Code)\n",
    "5. Azure CLI credentials (which we just set up)\n",
    "\n",
    "The code below will:\n",
    "1. Create a DefaultAzureCredential instance\n",
    "2. Try to get a token for Azure Cognitive Services\n",
    "3. Print success message if token is acquired\n",
    "\n",
    ">Note: You may see some warning/error messages as it tries different authentication methods - \n",
    ">this is normal and can be ignored as long as you see \"Successfully acquired token!\" at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9140263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then use DefaultAzureCredential in your code\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AccessToken\n",
    "import logging\n",
    "\n",
    "# Enable detailed logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Test token acquisition\n",
    "    token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    print(\"Successfully acquired token!\")\n",
    "except Exception as e:\n",
    "    print(f\"Authentication failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad83ef2",
   "metadata": {},
   "source": [
    "# Now that you have successfully authenticated, you can proceed to [1-environment_setup.ipynb](1-environment_setup.ipynb), or try the additional authentication methods or troubleshoot below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d717f84a",
   "metadata": {},
   "source": [
    "### 2. Using Visual Studio Code (optional)\n",
    "\n",
    "If you're using VS Code with the Azure extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc79867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, VisualStudioCodeCredential\n",
    "\n",
    "try:\n",
    "    # Explicitly try VS Code credentials\n",
    "    vscode_credential = VisualStudioCodeCredential()\n",
    "    token = vscode_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    print(\"Successfully authenticated with VS Code credentials!\")\n",
    "except Exception as e:\n",
    "    print(f\"VS Code authentication failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286b93f",
   "metadata": {},
   "source": [
    "### 3. Using Service Principal (Optional - have to set environment variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e24f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set these environment variables before running your application\n",
    "required_env_vars = {\n",
    "    \"AZURE_CLIENT_ID\": \"your-client-id\",\n",
    "    \"AZURE_CLIENT_SECRET\": \"your-client-secret\",\n",
    "    \"AZURE_TENANT_ID\": \"your-tenant-id\"\n",
    "}\n",
    "\n",
    "# Verify environment variables are set\n",
    "def check_env_vars():\n",
    "    missing_vars = [var for var, _ in required_env_vars.items() \n",
    "                   if not os.getenv(var)]\n",
    "    if missing_vars:\n",
    "        print(f\"Missing environment variables: {', '.join(missing_vars)}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "if check_env_vars():\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Test authentication\n",
    "    try:\n",
    "        token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "        print(\"Successfully authenticated using environment credentials!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Authentication failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69d5e09",
   "metadata": {},
   "source": [
    "## Troubleshooting Steps\n",
    "\n",
    "### 1. Verify Role Assignments\n",
    "\n",
    "```bash\n",
    "# Check role assignments for your user/service principal\n",
    "az role assignment list --assignee \"your-email@domain.com\" --output table\n",
    "```\n",
    "\n",
    "### 2. Debug Token Acquisition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dfd0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Set up detailed logging\n",
    "logger = logging.getLogger('azure.identity')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Use a basic StreamHandler instead of LoggingHandler\n",
    "handler = logging.StreamHandler()\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "def test_authentication():\n",
    "    try:\n",
    "        credential = DefaultAzureCredential(logging_enable=True)\n",
    "        token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "        print(f\"Authentication successful!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Authentication failed with error: {str(e)}\")\n",
    "        print(\"\\nTroubleshooting steps:\")\n",
    "        print(\"1. Verify you're logged in: 'az account show'\")\n",
    "        print(\"2. Check role assignments: 'az role assignment list'\")\n",
    "        print(\"3. Verify tenant ID: 'az account show --query tenantId'\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "test_authentication()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d917029e",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Verify Azure AI Developer Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc256cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_ai_developer_role(subscription_id, resource_group, resource_name):\n",
    "    from azure.mgmt.authorization import AuthorizationManagementClient\n",
    "    \n",
    "    auth_client = AuthorizationManagementClient(\n",
    "        credential=DefaultAzureCredential(),\n",
    "        subscription_id=subscription_id\n",
    "    )\n",
    "    \n",
    "    resource_id = f\"/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.CognitiveServices/accounts/{resource_name}\"\n",
    "    \n",
    "    assignments = auth_client.role_assignments.list_for_scope(resource_id)\n",
    "    \n",
    "    ai_developer_role_id = \"a97b65f3-24c7-4388-baec-2e87135dc908\"  # Azure AI Developer role ID\n",
    "    \n",
    "    for assignment in assignments:\n",
    "        if assignment.role_definition_id.endswith(ai_developer_role_id):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57658b99",
   "metadata": {},
   "source": [
    "## Common Issues and Solutions\n",
    "\n",
    "1. **Token Acquisition Failed**\n",
    "   - Verify Azure CLI login: `az login --tenant <tenant-id>`\n",
    "   - Check default subscription: `az account show`\n",
    "   - Ensure correct tenant: `az account set --subscription <subscription-id>`\n",
    "\n",
    "2. **Missing Role Assignments**\n",
    "   - Add Azure AI Developer role:\n",
    "   ```bash\n",
    "   az role assignment create --assignee \"user@domain.com\" \\\n",
    "       --role \"Azure AI Developer\" \\\n",
    "       --scope \"/subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.CognitiveServices/accounts/<resource-name>\"\n",
    "   ```\n",
    "\n",
    "3. **Environment Variable Issues**\n",
    "   - Verify environment variables are set correctly\n",
    "   - Check for typos in variable names\n",
    "   - Ensure no extra spaces in values\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. Always use environment variables for service principal credentials\n",
    "2. Implement proper error handling and logging\n",
    "3. Use managed identities when deploying to Azure services\n",
    "4. Regularly rotate service principal secrets\n",
    "5. Follow the principle of least privilege when assigning roles\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Azure Identity Documentation](https://docs.microsoft.com/python/api/overview/azure/identity-readme)\n",
    "- [DefaultAzureCredential Authentication Flow](https://docs.microsoft.com/azure/developer/python/azure-sdk-authenticate)\n",
    "- [Azure RBAC Documentation](https://docs.microsoft.com/azure/role-based-access-control/overview)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

================
File: 1-introduction/2-environment_setup.ipynb
================
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff53b1e0",
   "metadata": {},
   "source": [
    "# Environment Setup for Azure AI Foundry Workshop\n",
    "\n",
    "This notebook will guide you through setting up your environment for the Azure AI Foundry workshop.\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8 or later\n",
    "- Azure subscription with AI services access\n",
    "- Basic Python knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed):\n",
    "!pip install azure-identity azure-ai-projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a108d90",
   "metadata": {},
   "source": [
    "## Azure Authentication Setup\n",
    "First, we'll verify our Azure credentials and setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import ConnectionType\n",
    "import os\n",
    "\n",
    "# Initialize Azure credentials\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    print(\"‚úì Successfully initialized DefaultAzureCredential\")\n",
    "except Exception as e:\n",
    "    print(f\"√ó Error initializing credentials: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e22718",
   "metadata": {},
   "source": [
    "## Initialize AI Project Client\n",
    "\n",
    "> **Note:** Before proceeding, ensure you:\n",
    "> 1. Copy your `.env.local` file to `.env`\n",
    "> 2. Update the project connection string in your `.env` file\n",
    "> 3. Have a Hub and Project already provisioned in Azure AI Foundry\n",
    "\n",
    "You can find your project connection string in [Azure AI Foundry](https://ai.azure.com) under your project's settings:\n",
    "\n",
    "<img src=\"proj-conn-string.png\" alt=\"Project Connection String Location\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3e2dbc",
   "metadata": {},
   "source": [
    "## Understanding AIProjectClient\n",
    "\n",
    "The AIProjectClient is a key component for interacting with Azure AI services that:\n",
    "\n",
    "- **Manages Connections**: Lists and accesses Azure AI resources like OpenAI models\n",
    "- **Handles Authentication**: Securely connects using Azure credentials  \n",
    "- **Enables Model Access**: Provides interfaces to use AI models and deployments\n",
    "- **Manages Project Settings**: Controls configurations for your Azure AI project\n",
    "\n",
    "The client requires:\n",
    "- A project connection string (from Azure AI project settings)\n",
    "- Valid Azure credentials\n",
    "\n",
    "You can find your project connection string in Azure AI Studio under Project Settings:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Create AI Project client\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize AIProjectClient with connection string and credentials\n",
    "try:\n",
    "    client = AIProjectClient.from_connection_string(\n",
    "        conn_str=os.getenv(\"PROJECT_CONNECTION_STRING\"),\n",
    "        credential=credential\n",
    "    )\n",
    "    print(\"‚úì Successfully initialized AIProjectClient\")\n",
    "except Exception as e:\n",
    "    print(f\"√ó Error initializing client: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10962ac9",
   "metadata": {},
   "source": [
    "## Verify Access to Models\n",
    "Finally, let's verify we can access the available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff9d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the properties of all connections\n",
    "connections = client.connections.list()\n",
    "print(f\"====> Listing of all connections (found {len(connections)}):\")\n",
    "for connection in connections:\n",
    "    print(connection)\n",
    "\n",
    "# List the properties of all connections of a particular \"type\" (in this sample, Azure OpenAI connections)\n",
    "connections = client.connections.list(\n",
    "    connection_type=ConnectionType.AZURE_OPEN_AI,\n",
    ")\n",
    "print(f\"====> Listing of all Azure Open AI connections (found {len(connections)}):\")\n",
    "for connection in connections:\n",
    "    print(connection)\n",
    "\n",
    "# Get the properties of the default connection of a particular \"type\", with credentials\n",
    "connection = client.connections.get_default(\n",
    "    connection_type=ConnectionType.AZURE_AI_SERVICES,\n",
    "    include_credentials=True,  # Optional. Defaults to \"False\"\n",
    ")\n",
    "print(\"====> Get default Azure AI Services connection:\")\n",
    "print(connection)\n",
    "\n",
    "print(\"====> Get connection by name:\")\n",
    "print(connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c4895",
   "metadata": {},
   "source": [
    "## Validate Model and Search Connections\n",
    "The cell below validates that we have properly provisioned and connected to:\n",
    "1. Azure OpenAI models through our Azure OpenAI connection\n",
    "2. Azure AI Search through our Azure AI Search connection\n",
    "\n",
    "Both of these services will be essential for building our AI applications. The OpenAI models will provide the core language capabilities, while Azure AI Search will enable efficient information retrieval and knowledge base functionality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all connections and check for specific types\n",
    "conn_list = client.connections.list()\n",
    "search_conn_id = \"\"\n",
    "openai_conn_id = \"\"\n",
    "\n",
    "for conn in conn_list:\n",
    "    conn_type = str(conn.connection_type).split('.')[-1]  # Get the part after the dot\n",
    "    if conn_type == \"AZURE_AI_SEARCH\":\n",
    "        search_conn_id = conn.id\n",
    "    elif conn_type == \"AZURE_OPEN_AI\":\n",
    "        openai_conn_id = conn.id\n",
    "\n",
    "print(f\"\\n====> Connection IDs found:\")\n",
    "if not search_conn_id:\n",
    "    print(\"Azure AI Search: Not found - Please create an Azure AI Search connection\")\n",
    "else:\n",
    "    print(f\"Azure AI Search: {search_conn_id}\")\n",
    "    \n",
    "if not openai_conn_id:\n",
    "    print(\"Azure OpenAI: Not found - Please create an Azure OpenAI connection\") \n",
    "else:\n",
    "    print(f\"Azure OpenAI: {openai_conn_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

================
File: 1-introduction/3-quick_start.ipynb
================
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e86a0d",
   "metadata": {},
   "source": [
    "# Quick Start Guide - Azure AI Foundry\n",
    "\n",
    "This notebook provides a hands-on introduction to Azure AI Foundry. You'll learn how to:\n",
    "1. Initialize the AI Project client\n",
    "2. List available models\n",
    "3. Create a simple completion request\n",
    "4. Handle basic error scenarios\n",
    "\n",
    "## Prerequisites\n",
    "- Completed environment setup from previous notebook\n",
    "- Azure credentials configured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b65a7d",
   "metadata": {},
   "source": [
    "## Import Required Libraries and Setup\n",
    "\n",
    "In the next cell, we'll:\n",
    "1. Import the necessary Azure SDK libraries for authentication and AI Projects\n",
    "2. Import standard Python libraries for environment variables and JSON handling\n",
    "3. Initialize Azure credentials using DefaultAzureCredential\n",
    "   - This will automatically use your logged-in Azure CLI credentials\n",
    "   - Alternatively, it can use other authentication methods like environment variables or managed identity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a355de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize credentials\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd18d4ef",
   "metadata": {},
   "source": [
    "## Initialize AI Project Client\n",
    "\n",
    "> **Note:** Before proceeding, ensure you:\n",
    "> 1. Copy your `.env.local` file to `.env`\n",
    "> 2. Update the project connection string in your `.env` file\n",
    "> 3. Have a Hub and Project already provisioned in Azure AI Foundry\n",
    "\n",
    "You can find your project connection string in [Azure AI Foundry](https://ai.azure.com) under your project's settings:\n",
    "\n",
    "<img src=\"proj-conn-string.png\" alt=\"Project Connection String Location\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5ebd2",
   "metadata": {},
   "source": [
    "## Creating the AI Project Client\n",
    "\n",
    "In the next cell, we'll create an AI Project client using the connection string from our `.env` file.\n",
    "> **Note:** This example uses the synchronous client. For higher performance scenarios, you can also create an asynchronous client by importing `asyncio` and using the async methods from `AIProjectClient`.\n",
    "\n",
    "The client will be used to:\n",
    "- Connect to your Azure AI Project using the connection string\n",
    "- Authenticate using Azure credentials\n",
    "- Enable making inference requests to your deployed models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b96006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Create AI Project client\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    client = AIProjectClient.from_connection_string(\n",
    "        conn_str=os.getenv(\"PROJECT_CONNECTION_STRING\"),\n",
    "        credential=credential\n",
    "    )\n",
    "    print(\"‚úì Successfully initialized AIProjectClient\")\n",
    "except Exception as e:\n",
    "    print(f\"√ó Error initializing client: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77e602",
   "metadata": {},
   "source": [
    "## Create a Simple Completion\n",
    "Let's try a basic completion request:\n",
    "\n",
    "Now that we have an authenticated client, let's use it to make a chat completion request.\n",
    "The code below demonstrates how to:\n",
    "1. Get a ChatCompletionsClient from the azure-ai-inference package\n",
    "2. Use it to make a simple completion request\n",
    "\n",
    "We'll use the MODEL_DEPLOYMENT_NAME from our `.env` file, making it easy to switch between different\n",
    "deployed models without changing code. This could be an Azure OpenAI model, Microsoft model, or other providers\n",
    "that support chat completions.\n",
    "\n",
    "> Note: Make sure you have the azure-ai-inference package installed (from requirements.txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3774ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import UserMessage\n",
    "\n",
    "model_deployment_name = os.getenv(\"MODEL_DEPLOYMENT_NAME\")\n",
    "\n",
    "try:\n",
    "    chat_client = client.inference.get_chat_completions_client()\n",
    "    response = chat_client.complete(\n",
    "        model=model_deployment_name, \n",
    "        messages=[UserMessage(content=\"How to be healthy in one sentence?\")]\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7864f9",
   "metadata": {},
   "source": [
    "## Create a simple Agent\n",
    "\n",
    "Using AI Agent Service, we can create a simple agent to answer health related questions.\n",
    "\n",
    "Let's explore Azure AI Agent Service, a powerful tool for building intelligent agents.\n",
    "\n",
    "Azure AI Agent Service is a fully managed service that helps developers build, deploy, and scale AI agents\n",
    "without managing infrastructure. It combines large language models with tools that allow agents to:\n",
    "- Answer questions using RAG (Retrieval Augmented Generation)\n",
    "- Perform actions through tool calling \n",
    "- Automate complex workflows\n",
    "\n",
    "The code below demonstrates how to:\n",
    "1. Create an agent with a code interpreter tool\n",
    "2. Create a conversation thread\n",
    "3. Send a message requesting BMI analysis \n",
    "4. Process the request and get results\n",
    "5. Save any generated visualizations\n",
    "\n",
    "The agent will use the model specified in our .env file (MODEL_DEPLOYMENT_NAME) and will have access\n",
    "to a code interpreter tool for creating visualizations. This showcases how agents can combine\n",
    "natural language understanding with computational capabilities.\n",
    "\n",
    "> The visualization will be saved as a PNG file in the same folder as this notebook.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afb12b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import CodeInterpreterTool\n",
    "\n",
    "try:\n",
    "    # Create an agent with code interpreter\n",
    "    code_interpreter = CodeInterpreterTool()\n",
    "    agent = client.agents.create_agent(\n",
    "        model=model_deployment_name,\n",
    "        name=\"bmi-calculator\",\n",
    "        instructions=\"You are a health analyst who calculates BMI using US metrics (pounds, feet/inches). Use average US male measurements: 5'9\\\" (69 inches) and 198 pounds. Create a visualization showing where this BMI falls on the scale.\",\n",
    "        tools=code_interpreter.definitions,\n",
    "        tool_resources=code_interpreter.resources,\n",
    "    )\n",
    "    \n",
    "    thread = client.agents.create_thread()\n",
    "    \n",
    "    # Request BMI analysis\n",
    "    message = client.agents.create_message(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=\"Calculate BMI for an average US male (5'9\\\", 198 lbs). Create a visualization showing where this BMI falls on the standard BMI scale from 15 to 35. Include the standard BMI categories (Underweight, Normal, Overweight, Obese) in the visualization.\"\n",
    "    )\n",
    "    \n",
    "    # Process the request\n",
    "    run = client.agents.create_and_process_run(thread_id=thread.id, assistant_id=agent.id)\n",
    "    \n",
    "    # Get and save any visualizations\n",
    "    messages = client.agents.list_messages(thread_id=thread.id)\n",
    "    for image_content in messages.image_contents:\n",
    "        file_name = f\"bmi_analysis_{image_content.image_file.file_id}.png\"\n",
    "        client.agents.save_file(file_id=image_content.image_file.file_id, file_name=file_name)\n",
    "        print(f\"Analysis saved as: {file_name}\")\n",
    "    \n",
    "    # Print the analysis\n",
    "    if last_msg := messages.get_last_text_message_by_sender(\"assistant\"):\n",
    "        print(f\"Analysis: {last_msg.text.value}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    client.agents.delete_agent(agent.id)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

================
File: 2-notebooks/1-chat_completion/1-basic-chat-completion.ipynb
================
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "56be2132",
      "metadata": {},
      "source": [
        "# üçé Chat Completions with AIProjectClient üçè\n",
        "\n",
        "In this notebook, we'll demonstrate how to perform **Chat Completions** using the **Azure AI Foundry** SDK. We'll combine **`azure-ai-projects`** and **`azure-ai-inference`** packages to:\n",
        "\n",
        "1. **Initialize** an `AIProjectClient`.\n",
        "2. **Obtain** a Chat Completions client to do direct LLM calls.\n",
        "3. **Use** a **prompt template** to add system context.\n",
        "4. **Send** user prompts in a health & fitness theme.\n",
        "\n",
        "## üèãÔ∏è Health-Fitness Disclaimer\n",
        "> **This example is for demonstration only and does not provide real medical advice.** Always consult a professional for health or medical-related questions.\n",
        "\n",
        "## Prerequisites\n",
        "- Python 3.8+\n",
        "- `azure-ai-projects`, `azure-ai-inference`, `azure-identity`\n",
        "- A `.env` file containing:\n",
        "  ```bash\n",
        "  PROJECT_CONNECTION_STRING=<your-project-connection-string>\n",
        "  MODEL_DEPLOYMENT_NAME=<your-model-deployment-name>\n",
        "  ```\n",
        "\n",
        "Let's get started! üéâ\n",
        "\n",
        "<img src=\"./seq-diagrams/1-chat.png\" width=\"30%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce5fedcc",
      "metadata": {},
      "source": [
        "## 1. Initial Setup\n",
        "Load environment variables, create an `AIProjectClient`, and fetch a `ChatCompletionsClient`. We'll also define a **prompt template** to show how you might structure a system message.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07dd1b4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.inference.models import UserMessage, SystemMessage  # for chat messages\n",
        "\n",
        "# Load environment variables\n",
        "notebook_path = Path().absolute()\n",
        "parent_dir = notebook_path.parent\n",
        "load_dotenv(parent_dir / '.env')\n",
        "\n",
        "# Retrieve from environment\n",
        "connection_string = os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
        "model_deployment = os.environ.get(\"MODEL_DEPLOYMENT_NAME\")\n",
        "\n",
        "try:\n",
        "    # Create the project client\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        credential=DefaultAzureCredential(),\n",
        "        conn_str=connection_string,\n",
        "    )\n",
        "    print(\"‚úÖ Successfully created AIProjectClient\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error initializing client:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f03c9a87",
      "metadata": {},
      "source": [
        "### Prompt Template\n",
        "We'll define a quick **system** message that sets the context as a friendly, disclaimer-providing fitness assistant.\n",
        "\n",
        "```txt\n",
        "SYSTEM PROMPT (template):\n",
        "You are FitChat GPT, a helpful fitness assistant.\n",
        "Always remind users: I'm not a medical professional.\n",
        "Be friendly, provide general advice.\n",
        "...\n",
        "```\n",
        "\n",
        "We'll then pass user content as a **user** message.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab052b5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# We'll define a function that runs chat completions with a system prompt & user prompt\n",
        "def chat_with_fitness_assistant(user_input: str):\n",
        "    \"\"\"Use chat completions to get a response from our LLM, with system instructions.\"\"\"\n",
        "    # Our system message template\n",
        "    system_text = (\n",
        "        \"You are FitChat GPT, a friendly fitness assistant.\\n\"\n",
        "        \"Always remind users: I'm not a medical professional.\\n\"\n",
        "        \"Answer with empathy and disclaimers.\"\n",
        "    )\n",
        "\n",
        "    # We'll open the chat completions client\n",
        "    with project_client.inference.get_chat_completions_client() as chat_client:\n",
        "        # Construct messages: system + user\n",
        "        system_message = SystemMessage(content=system_text)\n",
        "        user_message = UserMessage(content=user_input)\n",
        "\n",
        "        # Send the request\n",
        "        response = chat_client.complete(\n",
        "            model=model_deployment,\n",
        "            messages=[system_message, user_message]\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content  # simplest approach: get top choice's content\n",
        "\n",
        "print(\"Defined a helper function to do chat completions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "273d7bdd",
      "metadata": {},
      "source": [
        "## 2. Try Chat Completions üéâ\n",
        "We'll call the function with a user question about health or fitness, and see the result. Feel free to modify the question or run multiple times!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee675bd5",
      "metadata": {},
      "outputs": [],
      "source": [
        "user_question = \"How can I start a beginner workout routine at home?\"\n",
        "reply = chat_with_fitness_assistant(user_question)\n",
        "print(\"üó£Ô∏è User:\", user_question)\n",
        "print(\"ü§ñ Assistant:\", reply)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6eff150",
      "metadata": {},
      "source": [
        "## 3. Another Example: Prompt Template with Fill-Ins üìù\n",
        "We can go a bit further and add placeholders in the system message. For instance, imagine we have a **userName** or **goal**. We'll show a minimal example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfec1e22",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_with_template(user_input: str, user_name: str, goal: str):\n",
        "    # Construct a system template with placeholders\n",
        "    system_template = (\n",
        "        \"You are FitChat GPT, an AI personal trainer for {name}.\\n\"\n",
        "        \"Your user wants to achieve: {goal}.\\n\"\n",
        "        \"Remind them you're not a medical professional. Offer friendly advice.\"\n",
        "    )\n",
        "\n",
        "    # Fill in placeholders\n",
        "    system_prompt = system_template.format(name=user_name, goal=goal)\n",
        "\n",
        "    with project_client.inference.get_chat_completions_client() as chat_client:\n",
        "        system_msg = SystemMessage(content=system_prompt)\n",
        "        user_msg = UserMessage(content=user_input)\n",
        "\n",
        "        response = chat_client.complete(\n",
        "            model=model_deployment,\n",
        "            messages=[system_msg, user_msg]\n",
        "        )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Let's try it out\n",
        "templated_user_input = \"What kind of home exercise do you recommend for a busy schedule?\"\n",
        "assistant_reply = chat_with_template(\n",
        "    templated_user_input,\n",
        "    user_name=\"Jordan\",\n",
        "    goal=\"increase muscle tone and endurance\"\n",
        ")\n",
        "print(\"üó£Ô∏è User:\", templated_user_input)\n",
        "print(\"ü§ñ Assistant:\", assistant_reply)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0066b883",
      "metadata": {},
      "source": [
        "## 4. Tips & Cleanup\n",
        "\n",
        "**Tips**:\n",
        "- Add **OpenTelemetry** for end-to-end tracing of your calls.\n",
        "- Evaluate your chat completions with `azure-ai-evaluation` to see how well your LLM is performing.\n",
        "- Combine **system** + **user** + possibly **assistant** messages if you want to pass prior conversation context.\n",
        "\n",
        "### No Cleanup Required\n",
        "Because we're using a direct chat completions client, we haven't created any persistent resources we need to tear down.\n",
        "In more advanced scenarios, you'd manage resources like Agents, Threads, or Tools.\n",
        "\n",
        "## üéâ Congratulations!\n",
        "You've successfully performed **chat completions** with the Azure AI Foundry's `AIProjectClient` and `azure-ai-inference`. You've also seen how to incorporate **prompt templates** to tailor your system instructions.\n",
        "\n",
        "Keep exploring, and stay healthy & fit! üèãÔ∏è"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

================
File: 2-notebooks/1-chat_completion/2-embeddings.ipynb
================
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ca3e8f66",
      "metadata": {},
      "source": [
        "# üèãÔ∏è Fun with Text and Image Embeddings üçé\n",
        "\n",
        "Welcome to our **Health & Fitness** embeddings notebook! In this tutorial, we'll show you how to:\n",
        "\n",
        "1. **Initialize** an `AIProjectClient` to access your Azure AI Foundry project.\n",
        "2. **Embed text** using `azure-ai-inference` (text embeddings).\n",
        "3. **Embed images** using `azure-ai-inference` (image embeddings).\n",
        "4. **Generate a health-themed image** (simple example) and display it.\n",
        "5. **Use a prompt template** for extra fun.\n",
        "\n",
        "We'll do it all with a playful üçè health theme. Let's jump in!\n",
        "\n",
        "> **Disclaimer**: We're not offering medical advice. This is purely educational & fun.\n",
        "\n",
        "<img src=\"./seq-diagrams/2-embeddings.png\" width=\"30%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffbd2ac6",
      "metadata": {},
      "source": [
        "## 1. Setup & Environment\n",
        "\n",
        "We'll import our libraries, load environment variables for:\n",
        "- `PROJECT_CONNECTION_STRING`: your project connection string.\n",
        "- `MODEL_DEPLOYMENT_NAME`: the name of your model deployment.\n",
        "\n",
        "Install these packages if you haven't already:\n",
        "```bash\n",
        "pip install azure-ai-projects azure-ai-inference azure-identity\n",
        "```\n",
        "Let's begin! üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50899c96",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.inference.models import ImageEmbeddingInput\n",
        "\n",
        "# Load environment variables (optional, if you keep them in a .env file)\n",
        "load_dotenv()\n",
        "\n",
        "# Retrieve from environment or fallback\n",
        "PROJECT_CONNECTION_STRING = os.environ.get(\"PROJECT_CONNECTION_STRING\", \"<your-connection-string>\")\n",
        "MODEL_DEPLOYMENT_NAME = os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"<your-deployment-name>\")\n",
        "\n",
        "# Initialize project client\n",
        "try:\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        credential=DefaultAzureCredential(),\n",
        "        conn_str=PROJECT_CONNECTION_STRING,\n",
        "    )\n",
        "    print(\"üéâ Successfully created AIProjectClient\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error creating AIProjectClient:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e970dcce",
      "metadata": {},
      "source": [
        "## 2. Text Embeddings\n",
        "\n",
        "We'll call `get_embeddings_client()` to retrieve the embeddings client from our `AIProjectClient`. Then we embed some fun health-themed phrases:\n",
        "\n",
        "- \"üçé An apple a day keeps the doctor away\"\n",
        "- \"üèãÔ∏è 15-minute HIIT workout routine\"\n",
        "- \"üßò Mindful breathing exercises\"\n",
        "\n",
        "This returns numeric vectors representing each phrase in semantic space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28466a95",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "text_phrases = [\n",
        "    \"An apple a day keeps the doctor away üçé\",\n",
        "    \"Quick 15-minute HIIT workout routine üèãÔ∏è\",\n",
        "    \"Mindful breathing exercises üßò\"\n",
        "]\n",
        "\n",
        "try:\n",
        "    with project_client.inference.get_embeddings_client() as embed_client:\n",
        "        response = embed_client.embed(\n",
        "            model=MODEL_DEPLOYMENT_NAME,\n",
        "            input=text_phrases\n",
        "        )\n",
        "\n",
        "        for item in response.data:\n",
        "            vec = item.embedding\n",
        "            sample_str = f\"[{vec[0]:.4f}, {vec[1]:.4f}, ..., {vec[-2]:.4f}, {vec[-1]:.4f}]\"\n",
        "            print(f\"Sentence {item.index}: '{text_phrases[item.index]}':\\n\" \\\n",
        "                  f\"  Embedding length={len(vec)}\\n\" \\\n",
        "                  f\"  Sample: {sample_str}\\n\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error embedding text:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad7b0402",
      "metadata": {},
      "source": [
        "## 3. Prompt Template Example üìù\n",
        "\n",
        "Even though our focus is embeddings, let's quickly show how you'd integrate a **prompt template**. Imagine we want to embed user text but prepend a little system context like ‚ÄúYou are a helpful fitness coach.‚Äù We'll do that here in a minimal way.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78f3e392",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "# A basic prompt template (system-style) we'll prepend to user text.\n",
        "TEMPLATE_SYSTEM = (\n",
        "    \"You are HealthFitGPT, a fitness guidance model.\\n\"\n",
        "    \"Please focus on healthy advice and disclaim you're not a doctor.\\n\\n\"\n",
        "    \"User message:\"  # We'll append user message after this.\n",
        ")\n",
        "\n",
        "def embed_with_template(user_text):\n",
        "    \"\"\"Embed user text with a system template in front.\"\"\"\n",
        "    content = TEMPLATE_SYSTEM + user_text\n",
        "    with project_client.inference.get_embeddings_client() as embed_client:\n",
        "        rsp = embed_client.embed(\n",
        "            model=MODEL_DEPLOYMENT_NAME,\n",
        "            input=[content]\n",
        "        )\n",
        "    return rsp.data[0].embedding\n",
        "\n",
        "sample_user_text = \"Can you suggest a quick home workout for busy moms?\"\n",
        "embedding_result = embed_with_template(sample_user_text)\n",
        "print(\"Embedding length:\", len(embedding_result))\n",
        "print(\"First few dims:\", embedding_result[:8])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2571c972",
      "metadata": {},
      "source": [
        "## 4. Image Embeddings\n",
        "\n",
        "Now let's embed an **image**! We'll treat the image as input to a model that returns a vector. The snippet below references a file named `gbb.jpeg` (replace with any local path). In a real scenario, this can be your own photo or user-uploaded image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "353bbc40",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "image_file_path = \"gbb.jpeg\"  # Replace with your local image path!\n",
        "try:\n",
        "    with project_client.inference.get_image_embeddings_client() as img_embed_client:\n",
        "        # Construct input for the image embeddings call\n",
        "        img_input = ImageEmbeddingInput.load(image_file=image_file_path, image_format=\"jpg\")\n",
        "        resp = img_embed_client.embed(\n",
        "            model=MODEL_DEPLOYMENT_NAME,\n",
        "            input=[img_input]\n",
        "        )\n",
        "\n",
        "        for item in resp.data:\n",
        "            vec = item.embedding\n",
        "            snippet = f\"[{vec[0]:.4f}, {vec[1]:.4f}, ..., {vec[-2]:.4f}, {vec[-1]:.4f}]\"\n",
        "            print(f\"Image index={item.index}, length={len(vec)}, sample={snippet}\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error embedding image:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb863ae9",
      "metadata": {},
      "source": [
        "## 5. Generate a Health-Related Image üèÉ\n",
        "\n",
        "Though distinct from embeddings, let's have some fun and **generate** a small health-themed image using the same `project_client`. The actual method name may vary depending on your installed version of `azure-ai-inference`. We'll pretend there's a `get_image_generation_client()` method. We'll pass a simple prompt describing a healthy scenario.\n",
        "\n",
        "We'll then display the returned image inline. (In real usage, you'd handle the raw bytes, save them, or pass them back to your application.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b07b38d6",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# This block is pseudo-code / example, depending on your actual method names:\n",
        "def generate_health_image(prompt=\"A simple cartoon of a happy person jogging outdoors\"):\n",
        "    try:\n",
        "        with project_client.inference.get_image_generation_client() as gen_client:\n",
        "            gen_response = gen_client.generate(\n",
        "                model=MODEL_DEPLOYMENT_NAME,\n",
        "                prompt=prompt,\n",
        "                # possibly other params like size, etc.\n",
        "            )\n",
        "            # We'll assume gen_response has a base64 image or raw bytes in .data[0].\n",
        "            # This is just an example structure:\n",
        "            if gen_response.data:\n",
        "                image_bytes = gen_response.data[0].binary  # or .content, .image_bytes, etc.\n",
        "                return image_bytes\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Error generating image:\", e)\n",
        "    return None\n",
        "\n",
        "# Let's do a quick run\n",
        "generated_img_data = generate_health_image()\n",
        "if generated_img_data:\n",
        "    display(Image(generated_img_data))\n",
        "else:\n",
        "    print(\"(No generated image data. This may be stub code for demonstration.)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3c6dece",
      "metadata": {},
      "source": [
        "## 6. Wrap-Up & Next Steps\n",
        "üéâ We've shown how to:\n",
        "- Set up `AIProjectClient`.\n",
        "- Get **text embeddings**.\n",
        "- Get **image embeddings**.\n",
        "- **Generate** a health-themed image.\n",
        "- Use a **prompt template** for some system context.\n",
        "\n",
        "**Where to go next?**\n",
        "- Explore `azure-ai-evaluation` for evaluating your embeddings.\n",
        "- Use `azure-core-tracing-opentelemetry` for end-to-end telemetry.\n",
        "- Build out a retrieval pipeline to compare similarity of embeddings.\n",
        "\n",
        "Have fun experimenting, and remember: always consult real health professionals for medical advice!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

================
File: 2-notebooks/1-chat_completion/3-basic-rag.ipynb
================
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3cf62fbc",
      "metadata": {},
      "source": [
        "# üçè Basic Retrieval-Augmented Generation (RAG) with AIProjectClient üçé\n",
        "\n",
        "In this notebook, we'll demonstrate a **basic RAG** flow using:\n",
        "- **`azure-ai-projects`** (AIProjectClient)\n",
        "- **`azure-ai-inference`** (Embeddings, ChatCompletions)\n",
        "- **`azure-ai-search`** (for vector or hybrid search)\n",
        "\n",
        "Our theme is **Health & Fitness** üçè so we‚Äôll create a simple set of health tips, embed them, store them in a search index, then do a query that retrieves relevant tips, and pass them to an LLM to produce a final answer.\n",
        "\n",
        "> **Disclaimer**: This is not medical advice. For real health questions, consult a professional.\n",
        "\n",
        "## What is RAG?\n",
        "Retrieval-Augmented Generation (RAG) is a technique where the LLM (Large Language Model) uses relevant retrieved text chunks from your data to craft a final answer. This helps ground the model's response in real data, reducing hallucinations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c26cbaa3",
      "metadata": {},
      "source": [
        "<img src=\"./seq-diagrams/3-basic-rag.png\" width=\"30%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99dfbd81",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "We'll import libraries, load environment variables, and create an `AIProjectClient`.\n",
        "\n",
        "### Prerequisites\n",
        "1. Python 3.8+\n",
        "2. `pip install azure-ai-projects azure-ai-inference azure-search-documents azure-identity`\n",
        "3. `.env` with:\n",
        "   ```bash\n",
        "   PROJECT_CONNECTION_STRING=<your-conn-string>\n",
        "   MODEL_DEPLOYMENT_NAME=<some-chat-model>\n",
        "   SEARCH_INDEX_NAME=<your-search-index>\n",
        "   ```\n",
        "4. An **Azure AI Search** connection in your project, or any index ready to store embeddings.\n",
        "5. You also have a deployed LLM for chat + an embeddings model deployment (like `text-embedding-ada-002` or any other embedding model).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7395b2e",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# azure-ai-projects\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# We'll embed with azure-ai-inference\n",
        "from azure.ai.inference import EmbeddingsClient, ChatCompletionsClient\n",
        "from azure.ai.inference.models import UserMessage, SystemMessage\n",
        "\n",
        "# For vector search or hybrid search\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.indexes import SearchIndexClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "conn_string = os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
        "chat_model = os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
        "search_index_name = os.environ.get(\"SEARCH_INDEX_NAME\", \"healthtips-index\")\n",
        "\n",
        "try:\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        credential=DefaultAzureCredential(),\n",
        "        conn_str=conn_string,\n",
        "    )\n",
        "    print(\"‚úÖ AIProjectClient created successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error creating AIProjectClient:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94b84bb8",
      "metadata": {},
      "source": [
        "## 2. Create Sample Health Data\n",
        "We'll create a few short doc chunks. In a real scenario, you might read from CSV or PDFs, chunk them up, embed them, and store them in your search index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eab53d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "health_tips = [\n",
        "    {\n",
        "        \"id\": \"doc1\",\n",
        "        \"content\": \"Daily 30-minute walks help maintain a healthy weight and reduce stress.\",\n",
        "        \"source\": \"General Fitness\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc2\",\n",
        "        \"content\": \"Stay hydrated by drinking 8-10 cups of water per day.\",\n",
        "        \"source\": \"General Fitness\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc3\",\n",
        "        \"content\": \"Consistent sleep patterns (7-9 hours) improve muscle recovery.\",\n",
        "        \"source\": \"General Fitness\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc4\",\n",
        "        \"content\": \"For cardio endurance, try interval training like HIIT.\",\n",
        "        \"source\": \"Workout Advice\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc5\",\n",
        "        \"content\": \"Warm up with dynamic stretches before running to reduce injury risk.\",\n",
        "        \"source\": \"Workout Advice\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc6\",\n",
        "        \"content\": \"Balanced diets typically include protein, whole grains, fruits, vegetables, and healthy fats.\",\n",
        "        \"source\": \"Nutrition\"\n",
        "    },\n",
        "]\n",
        "print(\"Created a small list of health tips.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29136555",
      "metadata": {},
      "source": [
        "## 3. Generate Embeddings + Store in Azure Search\n",
        "We'll show a minimal approach:\n",
        "1. Get embeddings client.\n",
        "2. Embed each doc.\n",
        "3. Upsert docs into Azure AI Search index with `embedding` field.\n",
        "\n",
        "### 3.1. Connect to Azure Search\n",
        "We'll do so by retrieving the default search connection from the project, then building a `SearchClient` from the `azure.search.documents` library.\n",
        "\n",
        "After that, we embed each doc, then upsert into your index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87d0ef9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.projects.models import ConnectionType\n",
        "\n",
        "# Get the default search connection, with credentials\n",
        "search_conn = project_client.connections.get_default(\n",
        "    connection_type=ConnectionType.AZURE_AI_SEARCH, include_credentials=True\n",
        ")\n",
        "if not search_conn:\n",
        "    raise RuntimeError(\"‚ùå No default Azure AI Search connection found!\")\n",
        "\n",
        "search_client = SearchClient(\n",
        "    endpoint=search_conn.endpoint_url,\n",
        "    index_name=search_index_name,\n",
        "    credential=AzureKeyCredential(search_conn.key)\n",
        ")\n",
        "print(\"‚úÖ Created Azure SearchClient.\")\n",
        "\n",
        "# Now create embeddings client\n",
        "embeddings_client = project_client.inference.get_embeddings_client()\n",
        "print(\"‚úÖ Created embeddings client.\")\n",
        "\n",
        "search_docs = []\n",
        "for doc in health_tips:\n",
        "    # embed doc content\n",
        "    emb_response = embeddings_client.embed(\n",
        "        input=[doc[\"content\"]]\n",
        "    )\n",
        "    emb_vec = emb_response.data[0].embedding\n",
        "\n",
        "    # We'll build a doc with 'id', 'content', 'source', 'embedding'\n",
        "    search_docs.append(\n",
        "        {\n",
        "            \"id\": doc[\"id\"],\n",
        "            \"content\": doc[\"content\"],\n",
        "            \"source\": doc[\"source\"],\n",
        "            \"embedding\": emb_vec,\n",
        "        }\n",
        "    )\n",
        "\n",
        "result = search_client.upload_documents(documents=search_docs)\n",
        "print(f\"Uploaded {len(search_docs)} docs to Search index '{search_index_name}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d05e5468",
      "metadata": {},
      "source": [
        "## 4. Basic RAG Flow\n",
        "### 4.1. Retrieve\n",
        "When a user queries, we:\n",
        "1. Embed user question.\n",
        "2. Search vector index with that embedding to get top docs.\n",
        "\n",
        "### 4.2. Generate answer\n",
        "We then pass the retrieved docs to the chat model.\n",
        "\n",
        "> In a real scenario, you'd have a more advanced approach to chunking & summarizing. We'll keep it simple.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c15c3aab",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rag_chat(query: str, top_k: int = 3) -> str:\n",
        "    # 1) Embed user query\n",
        "    user_vec = embeddings_client.embed(input=[query]).data[0].embedding\n",
        "\n",
        "    # 2) Vector search\n",
        "    # We'll assume your index has the vector field named 'embedding',\n",
        "    # using vector search or hybrid. We'll do a minimal example.\n",
        "\n",
        "    results = search_client.search(\n",
        "        search_text=\"\",  # for vector search we don't rely on textual search\n",
        "        vector=user_vec,\n",
        "        vector_fields=\"embedding\",\n",
        "        top=top_k\n",
        "    )\n",
        "\n",
        "    # gather the top docs\n",
        "    top_docs_content = []\n",
        "    for r in results:\n",
        "        # Each doc is a SearchResult object with doc: dict.\n",
        "        c = r[\"content\"]\n",
        "        s = r[\"source\"]\n",
        "        top_docs_content.append(f\"Source: {s} => {c}\")\n",
        "\n",
        "    # 3) Chat with retrieved docs.\n",
        "    # We'll pass a system message instructing the model to use them.\n",
        "    # We'll just do a minimal approach.\n",
        "\n",
        "    system_text = (\n",
        "        \"You are a health & fitness assistant.\\n\"\n",
        "        \"Answer user questions using ONLY the text from these docs.\\n\"\n",
        "        \"Docs:\\n\"\n",
        "        + \"\\n\".join(top_docs_content)\n",
        "        + \"\\nIf unsure, say 'I'm not sure'.\\n\"\n",
        "    )\n",
        "\n",
        "    with project_client.inference.get_chat_completions_client() as chat_client:\n",
        "        response = chat_client.complete(\n",
        "            model=chat_model,\n",
        "            messages=[\n",
        "                SystemMessage(content=system_text),\n",
        "                UserMessage(content=query)\n",
        "            ]\n",
        "        )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfecfb3c",
      "metadata": {},
      "source": [
        "## 5. Try a Query üéâ\n",
        "Let's do a question about cardio for busy people.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3937fdfc",
      "metadata": {},
      "outputs": [],
      "source": [
        "user_query = \"What's a good short cardio routine for me if I'm busy?\"\n",
        "answer = rag_chat(user_query)\n",
        "print(\"üó£Ô∏è User Query:\", user_query)\n",
        "print(\"ü§ñ RAG Answer:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6e562e6",
      "metadata": {},
      "source": [
        "## 6. Conclusion\n",
        "We've demonstrated a **basic RAG** pipeline with:\n",
        "- **Embedding** docs & storing them in **Azure AI Search**.\n",
        "- **Retrieving** top docs for user question.\n",
        "- **Chat** with the retrieved docs.\n",
        "\n",
        "üîé You can expand this by adding advanced chunking, more robust retrieval, and quality checks. Enjoy your healthy coding! üçé\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

================
File: 2-notebooks/1-chat_completion/4-phi-4.ipynb
================
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1e99a5b8",
      "metadata": {},
      "source": [
        "# üçé Phi-4 Model with AIProjectClient üçè\n",
        "\n",
        "**Phi-4** is a next-generation open model that aims to provide near GPT-4 capabilities at a fraction of the cost, making it ideal for many enterprise or personal use cases. It's especially great for chain-of-thought reasoning and RAG (Retrieval Augmented Generation) scenarios.\n",
        "\n",
        "In this notebook, you'll see how to:\n",
        "1. **Initialize** an `AIProjectClient` for your Azure AI Foundry environment.\n",
        "2. **Chat** with the **Phi-4** model using `azure-ai-inference`.\n",
        "3. **Show** a Health & Fitness example, featuring disclaimers and wellness Q&A.\n",
        "4. **Enjoy** the value proposition of a cheaper alternative to GPT-4 with strong reasoning capabilities. üèãÔ∏è\n",
        "\n",
        "> **Disclaimer**: This is not medical advice. Please consult professionals.\n",
        "\n",
        "## Why Phi-4?\n",
        "Phi-4 is a 14B-parameter model trained on curated data for high reasoning performance.\n",
        "- **Cost-Effective**: Get GPT-4-level performance for many tasks without the GPT-4 price.\n",
        "- **Reasoning & RAG**: Perfect for chain-of-thought reasoning steps and retrieval augmented generation workflows.\n",
        "- **Generous Context Window**: 16K tokens, enabling more context or longer user conversations.\n",
        "\n",
        "<img src=\"./seq-diagrams/4-phi-4.png\" width=\"30%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e93357dd",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "Below, we'll install and import the necessary libraries:\n",
        "- **azure-ai-projects**: For the `AIProjectClient`.\n",
        "- **azure-ai-inference**: For calling your model, specifically the chat completions.\n",
        "- **azure-identity**: For `DefaultAzureCredential`.\n",
        "\n",
        "Ensure you have a `.env` file with:\n",
        "```bash\n",
        "PROJECT_CONNECTION_STRING=<your-conn-string>\n",
        "SERVERLESS_MODEL_NAME=phi-4\n",
        "```\n",
        "Replace `<your-conn-string>` with your actual Azure AI Foundry project connection string.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b5634a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage, AssistantMessage\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "conn_string = os.getenv(\"PROJECT_CONNECTION_STRING\")\n",
        "phi4_deployment = os.getenv(\"SERVERLESS_MODEL_NAME\", \"phi-4\")\n",
        "\n",
        "try:\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        credential=DefaultAzureCredential(),\n",
        "        conn_str=conn_string,\n",
        "    )\n",
        "    print(\"‚úÖ AIProjectClient created successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error creating AIProjectClient:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "500d63ef",
      "metadata": {},
      "source": [
        "## 2. Chat with Phi-4 üçè\n",
        "We'll demonstrate a simple conversation using **Phi-4** in a health & fitness context. We'll define a system prompt that clarifies the role of the assistant. Then we'll ask some user queries.\n",
        "\n",
        "> Notice that Phi-4 is well-suited for chain-of-thought reasoning. We'll let it illustrate its reasoning steps for fun.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bcc4772",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_with_phi4(user_question, chain_of_thought=False):\n",
        "    \"\"\"Send a chat request to the Phi-4 model with optional chain-of-thought.\"\"\"\n",
        "    # We'll define a system message with disclaimers:\n",
        "    system_prompt = (\n",
        "        \"You are a Phi-4 AI assistant, focusing on health and fitness.\\n\"\n",
        "        \"Remind users that you are not a medical professional, but can provide general info.\\n\"\n",
        "    )\n",
        "\n",
        "    # We can optionally instruct the model to show chain-of-thought. (Use carefully in production.)\n",
        "    if chain_of_thought:\n",
        "        system_prompt += \"Please show your step-by-step reasoning in your answer.\\n\"\n",
        "\n",
        "    # We create messages for system + user.\n",
        "    system_msg = SystemMessage(content=system_prompt)\n",
        "    user_msg = UserMessage(content=user_question)\n",
        "\n",
        "    with project_client.inference.get_chat_completions_client() as chat_client:\n",
        "        response = chat_client.complete(\n",
        "            model=phi4_deployment,\n",
        "            messages=[system_msg, user_msg],\n",
        "            temperature=0.8,  # a bit creative\n",
        "            top_p=0.9,\n",
        "            max_tokens=400,\n",
        "        )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Example usage:\n",
        "question = \"I'm training for a 5K. Any tips on a weekly workout schedule?\"\n",
        "answer = chat_with_phi4(question, chain_of_thought=True)\n",
        "print(\"üó£Ô∏è User:\", question)\n",
        "print(\"ü§ñ Phi-4:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc68c40d",
      "metadata": {},
      "source": [
        "## 3. RAG-like Example (Stub)\n",
        "Phi-4 also excels in retrieval augmented generation scenarios, where you provide external context and let the model reason over it. Below is a **stub** example showing how you'd pass retrieved text as context.\n",
        "\n",
        "> In a real scenario, you'd embed & search for relevant passages, then feed them into the user/system message.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "419ea578",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_with_phi4_rag(user_question, retrieved_doc):\n",
        "    \"\"\"Simulate an RAG flow by appending retrieved context to the system prompt.\"\"\"\n",
        "    system_prompt = (\n",
        "        \"You are Phi-4, helpful fitness AI.\\n\"\n",
        "        \"We have some context from the user's knowledge base: \\n\"\n",
        "        f\"{retrieved_doc}\\n\"\n",
        "        \"Please use this context to help your answer. If the context doesn't help, say so.\\n\"\n",
        "    )\n",
        "\n",
        "    system_msg = SystemMessage(content=system_prompt)\n",
        "    user_msg = UserMessage(content=user_question)\n",
        "\n",
        "    with project_client.inference.get_chat_completions_client() as chat_client:\n",
        "        response = chat_client.complete(\n",
        "            model=phi4_deployment,\n",
        "            messages=[system_msg, user_msg],\n",
        "            temperature=0.3,\n",
        "            max_tokens=300,\n",
        "        )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Let's define a dummy doc snippet:\n",
        "doc_snippet = \"Recommended to run 3 times per week and mix with cross-training.\\n\" \\\n",
        "              \"Include rest days or active recovery days for muscle repair.\"\n",
        "\n",
        "user_q = \"How often should I run weekly to prepare for a 5K?\"\n",
        "rag_answer = chat_with_phi4_rag(user_q, doc_snippet)\n",
        "print(\"üó£Ô∏è User:\", user_q)\n",
        "print(\"ü§ñ Phi-4 (RAG):\", rag_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a33a375",
      "metadata": {},
      "source": [
        "## 4. Wrap-Up & Best Practices\n",
        "1. **Chain-of-Thought**: Great for debugging or certain QA tasks, but be mindful about revealing chain-of-thought to end users.\n",
        "2. **RAG**: Use `azure-ai-inference` with retrieval results to ground your answers.\n",
        "3. **OpenTelemetry**: Optionally integrate `opentelemetry-sdk` and `azure-core-tracing-opentelemetry` for full observability.\n",
        "4. **Evaluate**: Use `azure-ai-evaluation` to measure your model‚Äôs performance.\n",
        "5. **Cost & Performance**: Phi-4 aims to provide near GPT-4 performance at lower cost. Evaluate for your domain needs.\n",
        "\n",
        "## üéâ Congratulations!\n",
        "You've seen how to:\n",
        "- Use **Phi-4** with `AIProjectClient` and `azure-ai-inference`.\n",
        "- Create a **chat** flow with chain-of-thought.\n",
        "- Stub a **RAG** scenario.\n",
        "\n",
        "> Happy hacking! üèãÔ∏è\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

================
File: 2-notebooks/2-agent_service/1-basics.ipynb
================
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "72d4ac29",
      "metadata": {},
      "source": [
        "# üçè Fun & Fit Health Advisor Agent Tutorial üçé\n",
        "\n",
        "Welcome to our **Fun & Fit Health Advisor Agent** tutorial, where you'll use **Azure AI Foundry** SDKs to create a playful (yet carefully disclaimed!) health and fitness assistant. We'll:\n",
        "\n",
        "1. **Initialize** our project using **azure-ai-projects**.\n",
        "2. **Create an Agent** specialized in providing general wellness and nutritional advice (with disclaimers!).\n",
        "3. **Manage conversations** about fitness, nutrition, and general health topics.\n",
        "4. **Showcase logging and tracing** with **OpenTelemetry**.\n",
        "5. **Demonstrate** how to incorporate tools, safety disclaimers, and basic best practices.\n",
        "\n",
        "### ‚ö†Ô∏è Important Medical Disclaimer ‚ö†Ô∏è\n",
        "> **The health information provided by this notebook is for general educational and entertainment purposes only and is not intended as a substitute for professional medical advice, diagnosis, or treatment.** Always seek the advice of your physician or other qualified health provider with any questions you may have regarding a medical condition. Never disregard professional medical advice or delay seeking it because of something you read or receive from this notebook.\n",
        "\n",
        "\n",
        "## Prerequisites\n",
        "- Python 3.8+\n",
        "- `azure-ai-projects` and `azure-ai-inference` libraries\n",
        "- `azure-ai-evaluation` (optional) for advanced evaluation scenarios\n",
        "- `opentelemetry-sdk` and `azure-core-tracing-opentelemetry` (optional) for tracing\n",
        "- A `.env` file at the **parent directory** of this notebook containing:\n",
        "  ```\n",
        "  PROJECT_CONNECTION_STRING=<replace-with-your-project-connection-string>\n",
        "  MODEL_DEPLOYMENT_NAME=<replace-with-your-model-deployment-name>\n",
        "  TENANT_ID=<replace-with-your-tenant-id>\n",
        "  ```\n",
        "or, complete [the notebooks in introduction](../../1-introduction/3-quick_start.ipynb)\n",
        "## Let's Get Started\n",
        "We'll walk you through each cell with notes and diagrams to keep it fun. Let's begin!\n",
        "\n",
        "<img src=\"./seq-diagrams/1-basics.png\" width=\"30%\"/>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2e31c56",
      "metadata": {},
      "source": [
        "## 1. Initial Setup\n",
        "We'll start by importing needed libraries, loading environment variables, and initializing an **AIProjectClient** so we can do all the agent-related actions. Let's do it! üéâ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc2b306e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os  # For environment variables and path operations\n",
        "import time  # For adding delays if needed\n",
        "from pathlib import Path  # For cross-platform path handling\n",
        "\n",
        "# Import Azure and utility libraries\n",
        "from dotenv import load_dotenv  # For loading environment variables from .env file\n",
        "from azure.identity import DefaultAzureCredential  # For Azure authentication\n",
        "from azure.ai.projects import AIProjectClient  # Main client for AI Projects\n",
        "from azure.ai.projects.models import MessageTextContent  # For handling message content\n",
        "\n",
        "# Get the path to the .env file which is in the parent directory\n",
        "notebook_path = Path().absolute()  # Get absolute path of current notebook\n",
        "parent_dir = notebook_path.parent  # Get parent directory\n",
        "load_dotenv(parent_dir / '.env')  # Load environment variables from .env file\n",
        "\n",
        "# Initialize the AI Project Client using connection string from environment variables\n",
        "try:\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        # Use default Azure credentials for authentication\n",
        "        credential=DefaultAzureCredential(),\n",
        "        # Get the project connection string from environment variables\n",
        "        conn_str=os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
        "    )\n",
        "    print(\"‚úÖ Successfully initialized AIProjectClient\")\n",
        "except Exception as e:\n",
        "    # Print error message if client initialization fails\n",
        "    print(f\"‚ùå Error initializing project client: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f5aceb0",
      "metadata": {},
      "source": [
        "## 2. Creating our Fun & Fit Health Advisor Agent üèãÔ∏è\n",
        "\n",
        "We'll create an Agent specialized in general health and wellness. We'll explicitly mention disclaimers in its instructions, so it never forgets to keep it safe! The instructions also ask the agent to focus on general fitness, dietary tips, and always encourage the user to seek professional advice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "157d067c",
      "metadata": {
        "execution": {}
      },
      "outputs": [],
      "source": [
        "def create_health_advisor_agent():\n",
        "    \"\"\"Create a health advisor agent with disclaimers and basic instructions.\"\"\"\n",
        "    try:\n",
        "        # Get the model name from environment variables, default to gpt-4o-mini if not set\n",
        "        model_name = os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
        "\n",
        "        # Create a new agent using the AIProjectClient\n",
        "        # The agent will use the specified model and follow the given instructions\n",
        "        agent = project_client.agents.create_agent(\n",
        "            model=model_name,\n",
        "            name=\"fun-fit-health-advisor\",\n",
        "            # Define the agent's behavior and responsibilities\n",
        "            instructions=\"\"\"\n",
        "            You are a friendly AI Health Advisor.\n",
        "            You provide general health, fitness, and nutrition information, but always:\n",
        "            1. Include medical disclaimers.\n",
        "            2. Encourage the user to consult healthcare professionals.\n",
        "            3. Provide general, non-diagnostic advice around wellness, diet, and fitness.\n",
        "            4. Clearly remind them you're not a doctor.\n",
        "            5. Encourage safe and balanced approaches to exercise and nutrition.\n",
        "            \"\"\"\n",
        "        )\n",
        "        # Log success and return the created agent\n",
        "        print(f\"üéâ Created health advisor agent, ID: {agent.id}\")\n",
        "        return agent\n",
        "    except Exception as e:\n",
        "        # Handle any errors during agent creation\n",
        "        print(f\"‚ùå Error creating agent: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Create an instance of our health advisor agent\n",
        "health_advisor = create_health_advisor_agent()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fb1cb05",
      "metadata": {},
      "source": [
        "## 3. Managing Our Health Conversations üí¨\n",
        "A conversation (or *thread*) is where we'll store the user's messages and the agent's responses about health topics. Let's create a new thread dedicated to Health & Fitness Q&A.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "955161b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to create a new conversation thread for health discussions\n",
        "def start_health_conversation():\n",
        "    \"\"\"Create a new thread for health & fitness discussions.\"\"\"\n",
        "    try:\n",
        "        # Create a new empty thread using the project client\n",
        "        # A thread stores the back-and-forth messages between user and agent\n",
        "        thread = project_client.agents.create_thread()\n",
        "        \n",
        "        # Print success message with the thread ID for reference\n",
        "        print(f\"üìù Created a new conversation thread, ID: {thread.id}\")\n",
        "        \n",
        "        # Return the created thread object so we can use it later\n",
        "        return thread\n",
        "    except Exception as e:\n",
        "        # If thread creation fails, print error and return None\n",
        "        print(f\"‚ùå Error creating thread: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Initialize a new conversation thread that we'll use for our health Q&A\n",
        "health_thread = start_health_conversation()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bb2ad90",
      "metadata": {},
      "source": [
        "## 4. Asking Health & Fitness Questions üèÉ‚Äç‚ôÇÔ∏è\n",
        "We'll create messages from the user about typical health questions. For example, **\"How do I calculate my BMI?\"** or **\"What's a balanced meal for an active lifestyle?\"**. We'll let our Health Advisor Agent respond, always remembering that disclaimer!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "235b237b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_health_question(thread_id, user_question):\n",
        "    \"\"\"Add a user message to the conversation thread.\n",
        "    \n",
        "    Args:\n",
        "        thread_id: ID of the conversation thread\n",
        "        user_question: The health/fitness question from the user\n",
        "        \n",
        "    Returns:\n",
        "        Message object if successful, None if error occurs\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a new message in the thread from the user's perspective\n",
        "        # The role=\"user\" indicates this is a user message (vs assistant)\n",
        "        return project_client.agents.create_message(\n",
        "            thread_id=thread_id,\n",
        "            role=\"user\", \n",
        "            content=user_question\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error adding user message: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_thread_run(thread_id, agent_id):\n",
        "    \"\"\"Ask the agent to process the thread and generate a response.\n",
        "    \n",
        "    Args:\n",
        "        thread_id: ID of the conversation thread\n",
        "        agent_id: ID of the health advisor agent\n",
        "        \n",
        "    Returns:\n",
        "        Run object if successful, None if error occurs\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a new run to have the agent process the thread\n",
        "        run = project_client.agents.create_run(\n",
        "            thread_id=thread_id,\n",
        "            assistant_id=agent_id\n",
        "        )\n",
        "\n",
        "        # Poll the run status until completion or error\n",
        "        # Status can be: queued, in_progress, requires_action, completed, failed\n",
        "        while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:\n",
        "            time.sleep(1)  # Wait 1 second between status checks\n",
        "            run = project_client.agents.get_run(\n",
        "                thread_id=thread_id,\n",
        "                run_id=run.id\n",
        "            )\n",
        "\n",
        "        print(f\"ü§ñ Run completed with status: {run.status}\")\n",
        "        return run\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing thread run: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def view_thread_messages(thread_id):\n",
        "    \"\"\"Display all messages in the conversation thread in chronological order.\n",
        "    \n",
        "    Args:\n",
        "        thread_id: ID of the conversation thread to display\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get all messages in the thread\n",
        "        messages = project_client.agents.list_messages(thread_id=thread_id)\n",
        "        print(\"\\nüó£Ô∏è Conversation so far (oldest to newest):\")\n",
        "        \n",
        "        # Loop through messages in reverse order to show oldest first\n",
        "        for m in reversed(messages.data):\n",
        "            if m.content:\n",
        "                # Extract the text content from the message\n",
        "                # We only handle text messages for now (not images etc)\n",
        "                last_content = m.content[-1]\n",
        "                if isinstance(last_content, MessageTextContent):\n",
        "                    print(f\"{m.role.upper()}: {last_content.text.value}\\n\")\n",
        "        print(\"-----------------------------------\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error viewing thread: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0865cdf",
      "metadata": {},
      "source": [
        "### Example Queries\n",
        "Let's do some quick queries now to see the agent's disclaimers and how it handles typical health questions. We'll ask about **BMI** and about **balanced meal** for an active lifestyle.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8357bc8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# First verify that we have valid agent and thread objects before proceeding\n",
        "if health_advisor and health_thread:\n",
        "    # 1) Ask about BMI calculation and interpretation\n",
        "    # This demonstrates how the agent handles technical health questions\n",
        "    msg1 = ask_health_question(health_thread.id, \"How do I calculate my BMI, and what does it mean?\")\n",
        "    # Process the BMI question and wait for agent's response\n",
        "    run1 = process_thread_run(health_thread.id, health_advisor.id)\n",
        "\n",
        "    # 2) Ask about personalized meal planning\n",
        "    # This shows how the agent provides customized nutrition advice\n",
        "    msg2 = ask_health_question(health_thread.id, \"Can you give me a balanced meal plan for someone who exercises 3x a week?\")\n",
        "    # Process the meal plan question and wait for agent's response\n",
        "    run2 = process_thread_run(health_thread.id, health_advisor.id)\n",
        "\n",
        "    # Display the full conversation history to see both Q&As\n",
        "    # This will show the agent's responses including any health disclaimers\n",
        "    view_thread_messages(health_thread.id)\n",
        "else:\n",
        "    # Error handling if agent or thread initialization failed\n",
        "    print(\"‚ùå Could not run example queries because agent or thread is None.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2729872",
      "metadata": {},
      "source": [
        "## 5. Cleanup üßπ\n",
        "If you'd like to remove your agent from the service once finished, you can do so below. (In production, you might keep your agent around!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "820fa511",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to clean up and delete the agent when we're done\n",
        "def cleanup(agent):\n",
        "    # Only attempt cleanup if we have a valid agent\n",
        "    if agent:\n",
        "        try:\n",
        "            # Delete the agent using the project client\n",
        "            project_client.agents.delete_agent(agent.id)\n",
        "            # Print confirmation message with the agent name\n",
        "            print(f\"üóëÔ∏è Deleted health advisor agent: {agent.name}\")\n",
        "        except Exception as e:\n",
        "            # Handle any errors that occur during deletion\n",
        "            print(f\"Error cleaning up agent: {e}\")\n",
        "    else:\n",
        "        # If no agent was provided, inform the user\n",
        "        print(\"No agent to clean up.\")\n",
        "\n",
        "# Call cleanup function to delete our health advisor agent\n",
        "cleanup(health_advisor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5263afbe",
      "metadata": {},
      "source": [
        "# Congratulations! üèÜ\n",
        "You've successfully built a **Fun & Fit Health Advisor** that can:\n",
        "1. **Respond** to basic health and fitness questions.\n",
        "2. **Use disclaimers** to encourage safe, professional consultation.\n",
        "3. **Provide** general diet and wellness information.\n",
        "4. **Use** the synergy of **Azure AI Foundry** modules to power the conversation.\n",
        "\n",
        "## Next Steps\n",
        "- Explore adding more advanced tools (like **FileSearchTool** or **CodeInterpreterTool**) to provide more specialized info.\n",
        "- Evaluate your AI's performance with **azure-ai-evaluation**!\n",
        "- Add **OpenTelemetry** or Azure Monitor for deeper insights.\n",
        "- Incorporate **function calling** if you want to handle things like advanced calculation or direct data analysis.\n",
        "\n",
        "[Let's proceed to 2-agent_code-interpreter.ipynb](2-agent_code-interpreter.ipynb)\n",
        "\n",
        "Happy (healthy) coding! üí™"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

================
File: 2-notebooks/2-agent_service/2-code_interpreter.ipynb
================
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fun-fit-intro",
      "metadata": {},
      "source": [
        "# üçé Health Calculator Agent Tutorial üçè\n",
        "\n",
        "Welcome to the **Health Calculator Agent** tutorial, where we'll showcase how to:\n",
        "1. **Initialize** a project and use the Azure AI Foundry ecosystem\n",
        "2. **Create an Agent** with **Code Interpreter** capabilities\n",
        "3. **Perform BMI calculations** and **analyze nutritional data** with sample CSV files\n",
        "4. **Generate** basic health insights and disclaimers\n",
        "\n",
        "### ‚ö†Ô∏è Important Medical Disclaimer ‚ö†Ô∏è\n",
        "> **The information provided by this notebook is for general educational and entertainment purposes only and is not intended as a substitute for professional medical advice, diagnosis, or treatment.** Always seek the advice of your physician or other qualified health provider with any questions you may have regarding a medical condition.\n",
        "\n",
        "## Prerequisites\n",
        "- Python 3.8+\n",
        "- `azure-ai-projects` and `azure-ai-inference` (for advanced inference)\n",
        "- `opentelemetry-sdk` and `azure-core-tracing-opentelemetry` (optional, for tracing)\n",
        "- A `.env` file in the parent directory of this notebook containing:\n",
        "  ```bash\n",
        "  PROJECT_CONNECTION_STRING=<your-project-connection-string>\n",
        "  MODEL_DEPLOYMENT_NAME=<your-model-deployment-name>\n",
        "  TENANT_ID=<your-tenant-id>\n",
        "  ```\n",
        "\n",
        "## Let's Dive In\n",
        "We'll walk step-by-step, similar to our **Fun & Fit** sample, but with a focus on using **Code Interpreter** for numeric calculations and data analysis. Let's begin!\n",
        "\n",
        "<img src=\"./seq-diagrams/2-code-interpreter.png\" width=\"30%\"/>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "init-setup",
      "metadata": {},
      "source": [
        "## 1. Initial Setup\n",
        "We'll start by importing libraries, loading environment variables, and initializing an **AIProjectClient**. We'll also create a sample CSV for demonstration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "init-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.projects.models import CodeInterpreterTool, FilePurpose, MessageTextContent\n",
        "\n",
        "# Load environment variables from the parent directory's .env\n",
        "notebook_path = Path().absolute()\n",
        "parent_dir = notebook_path.parent\n",
        "load_dotenv(parent_dir / '.env')\n",
        "\n",
        "# Initialize AIProjectClient\n",
        "try:\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        credential=DefaultAzureCredential(),\n",
        "        conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n",
        "    )\n",
        "    print(\"‚úÖ Successfully initialized AIProjectClient\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing client: {str(e)}\")\n",
        "\n",
        "# Create sample CSV data for demonstration\n",
        "def create_sample_data():\n",
        "    try:\n",
        "        data = {\n",
        "            'Date': pd.date_range(start='2024-01-01', periods=7),\n",
        "            'Calories': [2100, 1950, 2300, 2050, 1900, 2200, 2150],\n",
        "            'Protein_g': [80, 75, 85, 78, 72, 82, 79],\n",
        "            'Carbs_g': [250, 230, 270, 245, 225, 260, 255],\n",
        "            'Fat_g': [70, 65, 75, 68, 63, 73, 71],\n",
        "            'Fiber_g': [25, 22, 28, 24, 21, 26, 23]\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "        filename = \"nutrition_data.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"üìÑ Created sample data file: {filename}\")\n",
        "        return filename\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating sample data: {e}\")\n",
        "        return None\n",
        "\n",
        "sample_file = create_sample_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "create-agent",
      "metadata": {},
      "source": [
        "## 2. Create Health Calculator Agent üë©‚Äçüíª\n",
        "We'll upload our sample CSV and then create an agent with **Code Interpreter** enabled. This agent can read the file, run Python code, and return results and visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-agent-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_health_calculator(file_path):\n",
        "    \"\"\"Create an agent with code interpreter for health/nutrition calculations.\"\"\"\n",
        "    try:\n",
        "        uploaded_file = project_client.agents.upload_file_and_poll(\n",
        "            file_path=file_path,\n",
        "            purpose=FilePurpose.AGENTS\n",
        "        )\n",
        "        print(f\"‚úÖ Uploaded CSV file, ID: {uploaded_file.id}\")\n",
        "\n",
        "        # Create a Code Interpreter tool referencing the uploaded file\n",
        "        code_tool = CodeInterpreterTool(file_ids=[uploaded_file.id])\n",
        "\n",
        "        # Create the agent with instructions to do basic calculations\n",
        "        agent = project_client.agents.create_agent(\n",
        "            model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o-mini\"),\n",
        "            name=\"health-calculator-agent\",\n",
        "            instructions=\"\"\"\n",
        "            You are a health calculator agent that can:\n",
        "            1. Calculate and interpret BMI\n",
        "            2. Analyze provided nutrition data\n",
        "            3. Generate charts/plots\n",
        "            4. Include disclaimers that you are not a medical professional\n",
        "            \"\"\",\n",
        "            tools=code_tool.definitions,\n",
        "            tool_resources=code_tool.resources\n",
        "        )\n",
        "        print(f\"üéâ Created health calculator agent, ID: {agent.id}\")\n",
        "        return agent, uploaded_file\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating health calculator agent: {e}\")\n",
        "        return None, None\n",
        "\n",
        "health_agent, uploaded_file = None, None\n",
        "if sample_file:\n",
        "    health_agent, uploaded_file = create_health_calculator(sample_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bmi-thread",
      "metadata": {},
      "source": [
        "## 3. BMI Calculation with Code Interpreter\n",
        "We'll create a thread for BMI calculations. We'll feed in the user's height/weight, and ask the agent to show how it calculates BMI, interpret the result, and always disclaim professional advice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bmi-thread-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_bmi_with_agent(agent, height_inches, weight_pounds):\n",
        "    \"\"\"Calculate BMI using the code interpreter agent.\"\"\"\n",
        "    try:\n",
        "        # Create a new conversation thread\n",
        "        thread = project_client.agents.create_thread()\n",
        "        print(f\"üìù Created thread for BMI calculation, ID: {thread.id}\")\n",
        "\n",
        "        # Construct user message requesting BMI calculation\n",
        "        user_text = (\n",
        "            f\"Calculate BMI for \\n\"\n",
        "            f\"Height: {height_inches} inches\\n\"\n",
        "            f\"Weight: {weight_pounds} pounds\\n\"\n",
        "            \"Please: \\n\"\n",
        "            \"1. Show calculation \\n\"\n",
        "            \"2. Interpret the result \\n\"\n",
        "            \"3. Include disclaimers \\n\"\n",
        "        )\n",
        "\n",
        "        msg = project_client.agents.create_message(\n",
        "            thread_id=thread.id,\n",
        "            role=\"user\",\n",
        "            content=user_text\n",
        "        )\n",
        "        print(f\"‚ûï Created BMI request message, ID: {msg.id}\")\n",
        "\n",
        "        # Create and process the run, letting the agent handle code\n",
        "        run = project_client.agents.create_and_process_run(\n",
        "            thread_id=thread.id,\n",
        "            assistant_id=agent.id\n",
        "        )\n",
        "        print(f\"ü§ñ BMI run finished with status: {run.status}\")\n",
        "        return thread, run\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during BMI calculation: {e}\")\n",
        "        return None, None\n",
        "\n",
        "if health_agent:\n",
        "    bmi_thread, bmi_run = calculate_bmi_with_agent(health_agent, 70, 180)  # example: 5'10\" and 180 lbs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nutrition-analysis",
      "metadata": {},
      "source": [
        "## 4. Nutrition Analysis\n",
        "We'll create another thread where the user can ask the agent to analyze the **`nutrition_data.csv`** we uploaded. The agent can read the file, compute macros, produce charts, and disclaim that it's not offering personalized medical advice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nutrition-analysis-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_nutrition_data(agent):\n",
        "    \"\"\"Ask the agent to analyze the uploaded nutrition data.\"\"\"\n",
        "    try:\n",
        "        thread = project_client.agents.create_thread()\n",
        "        print(f\"üìù Created thread for nutrition analysis, ID: {thread.id}\")\n",
        "\n",
        "        user_text = (\n",
        "            \"Analyze the CSV file with daily nutrition data.\\n\"\n",
        "            \"1. Compute average daily macros (calories, protein, carbs, fat, fiber).\\n\"\n",
        "            \"2. Create a chart to show trends.\\n\"\n",
        "            \"3. Discuss any insights or disclaimers.\\n\"\n",
        "        )\n",
        "\n",
        "        msg = project_client.agents.create_message(\n",
        "            thread_id=thread.id,\n",
        "            role=\"user\",\n",
        "            content=user_text\n",
        "        )\n",
        "        print(f\"‚ûï Created nutrition request message, ID: {msg.id}\")\n",
        "\n",
        "        run = project_client.agents.create_and_process_run(\n",
        "            thread_id=thread.id,\n",
        "            assistant_id=agent.id\n",
        "        )\n",
        "        print(f\"ü§ñ Nutrition run finished with status: {run.status}\")\n",
        "        return thread, run\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error analyzing nutrition data: {e}\")\n",
        "        return None, None\n",
        "\n",
        "if health_agent:\n",
        "    nutrition_thread, nutrition_run = analyze_nutrition_data(health_agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "view-results",
      "metadata": {},
      "source": [
        "## 5. Viewing Results & Visualizations üìä\n",
        "The agent may produce text insights, disclaimers, and even images with charts. Let's fetch them from our threads!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "view-results-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def view_agent_responses(thread_id):\n",
        "    try:\n",
        "        messages = project_client.agents.list_messages(thread_id=thread_id)\n",
        "        print(\"\\nüîé Agent Responses:\")\n",
        "        for msg in messages.data:\n",
        "            if msg.role == \"assistant\" and msg.content:\n",
        "                for c in msg.content:\n",
        "                    if hasattr(c, \"text\"):\n",
        "                        print(\"Response:\", c.text.value, \"\\n\")\n",
        "\n",
        "        # If images were generated, let's save them\n",
        "        for img in messages.image_contents:\n",
        "            file_id = img.image_file.file_id\n",
        "            outname = f\"chart_{file_id}.png\"\n",
        "            project_client.agents.save_file(file_id=file_id, file_name=outname)\n",
        "            print(f\"üñºÔ∏è Saved image output: {outname}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error viewing agent responses: {e}\")\n",
        "\n",
        "# Display BMI calculations\n",
        "if bmi_thread and bmi_run:\n",
        "    print(\"\\n=== BMI Calculation Results ===\")\n",
        "    view_agent_responses(bmi_thread.id)\n",
        "\n",
        "# Display nutrition analyses\n",
        "if nutrition_thread and nutrition_run:\n",
        "    print(\"\\n=== Nutrition Analysis Results ===\")\n",
        "    view_agent_responses(nutrition_thread.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cleanup",
      "metadata": {},
      "source": [
        "## 6. Cleanup & Best Practices\n",
        "We can remove our agent and sample data if desired. In production, you might keep them for repeated usage.\n",
        "\n",
        "### Best Practices in a Nutshell\n",
        "1. **Data Handling** ‚Äì Validate input data, handle missing values, properly manage file attachments.\n",
        "2. **Calculations** ‚Äì Provide formula steps, disclaimers, limit scope to general wellness, remind user you're not a doctor.\n",
        "3. **Visualizations** ‚Äì Use clear labeling and disclaimers that charts are for educational demonstrations.\n",
        "4. **Security** ‚Äì Monitor usage, limit access to code interpreter if dealing with proprietary data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cleanup-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleanup_all():\n",
        "    try:\n",
        "        # Delete the uploaded CSV file from the service\n",
        "        if 'uploaded_file' in globals() and uploaded_file:\n",
        "            project_client.agents.delete_file(uploaded_file.id)\n",
        "            print(\"üóëÔ∏è Deleted uploaded file from agent service.\")\n",
        "\n",
        "        # Delete the agent if we created one\n",
        "        if 'health_agent' in globals() and health_agent:\n",
        "            project_client.agents.delete_agent(health_agent.id)\n",
        "            print(\"üóëÔ∏è Deleted health calculator agent.\")\n",
        "\n",
        "        # Delete local CSV file\n",
        "        if 'sample_file' in globals() and sample_file and os.path.exists(sample_file):\n",
        "            os.remove(sample_file)\n",
        "            print(\"üóëÔ∏è Deleted local sample CSV file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during cleanup: {e}\")\n",
        "\n",
        "cleanup_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {},
      "source": [
        "# Congratulations! üéâ\n",
        "You now have a **Health Calculator Agent** with the **Code Interpreter** tool that can:\n",
        "- Perform **BMI calculations** and disclaim that it's not a doctor.\n",
        "- **Analyze** simple CSV-based nutrition data and produce insights + charts.\n",
        "- Return images (charts) and text-based insights.\n",
        "\n",
        "Feel free to extend this example to handle more advanced **file search**, **function calling**, or **evaluation** using the `azure-ai-evaluation` library!\n",
        "\n",
        "> **Happy (healthy) coding!**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

================
File: 2-notebooks/2-agent_service/3-file-search.ipynb
================
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-health-file-search",
      "metadata": {},
      "source": [
        "# üçè Health Resource Search Agent Tutorial üçé\n",
        "\n",
        "Welcome to the **Health Resource Search Agent** tutorial! We'll use **Azure AI Foundry** SDKs to build an assistant that can:\n",
        "\n",
        "1. **Upload** health and recipe files into a vector store.\n",
        "2. **Create an Agent** with a **File Search** tool.\n",
        "3. **Search** these documents for relevant dietary info.\n",
        "4. **Answer** health and wellness questions (with disclaimers!).\n",
        "\n",
        "### ‚ö†Ô∏è Important Medical Disclaimer ‚ö†Ô∏è\n",
        "> **All health information in this notebook is for general educational purposes only and is not a substitute for professional medical advice, diagnosis, or treatment.** Always seek the advice of a qualified healthcare professional with any questions you may have.\n",
        "\n",
        "## Prerequisites\n",
        "- Python 3.8 or later.\n",
        "- `azure-ai-projects`, `azure-ai-inference`, `azure-ai-evaluation` (optional), and other typical libraries.\n",
        "- `.env` file with your `PROJECT_CONNECTION_STRING`, `MODEL_DEPLOYMENT_NAME`, etc.\n",
        "\n",
        "## Let's Get Searching!\n",
        "We'll show you how to upload some sample files, create a vector store for them, then spin up an agent that can search these resources for dietary guidelines, recipes, and more. Enjoy!\n",
        "\n",
        "<img src=\"./seq-diagrams/3-file-search.png\" width=\"30%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "initial-setup",
      "metadata": {},
      "source": [
        "## 1. Initial Setup\n",
        "Here we import needed libraries, load environment variables from `.env`, and initialize our **AIProjectClient**. Let's do this! üéâ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "init-client-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.projects.models import (\n",
        "    FileSearchTool,\n",
        "    MessageTextContent,\n",
        "    MessageRole\n",
        ")\n",
        "\n",
        "# Load environment variables from parent .env\n",
        "notebook_path = Path().absolute()\n",
        "parent_dir = notebook_path.parent\n",
        "load_dotenv(parent_dir / '.env')\n",
        "\n",
        "# Initialize AIProjectClient\n",
        "try:\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        credential=DefaultAzureCredential(),\n",
        "        conn_str=os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
        "    )\n",
        "    print(\"‚úÖ Successfully initialized AIProjectClient\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing project client: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "create-sample-resources",
      "metadata": {},
      "source": [
        "## 2. Prepare Sample Files üç≤üóí\n",
        "We'll create some dummy .md files (for recipes and guidelines). Then we'll store them in a vector store for searching.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-files-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sample_files():\n",
        "    recipes_md = (\n",
        "        \"\"\"# Healthy Recipes Database\\n\\n\"\n",
        "        \"## Gluten-Free Recipes\\n\"\n",
        "        \"1. Quinoa Bowl\\n\"\n",
        "        \"   - Ingredients: quinoa, vegetables, olive oil\\n\"\n",
        "        \"   - Instructions: Cook quinoa, add vegetables\\n\\n\"\n",
        "        \"2. Rice Pasta with Vegetables\\n\"\n",
        "        \"   - Ingredients: rice pasta, mixed vegetables\\n\"\n",
        "        \"   - Instructions: Boil pasta, saut√© vegetables\\n\\n\"\n",
        "        \"## Diabetic-Friendly Recipes\\n\"\n",
        "        \"1. Low-Carb Stir Fry\\n\"\n",
        "        \"   - Ingredients: chicken, vegetables, tamari sauce\\n\"\n",
        "        \"   - Instructions: Cook chicken, add vegetables\\n\\n\"\n",
        "        \"2. Greek Salad\\n\"\n",
        "        \"   - Ingredients: cucumber, tomatoes, feta, olives\\n\"\n",
        "        \"   - Instructions: Chop vegetables, combine\\n\\n\"\n",
        "        \"## Heart-Healthy Recipes\\n\"\n",
        "        \"1. Baked Salmon\\n\"\n",
        "        \"   - Ingredients: salmon, lemon, herbs\\n\"\n",
        "        \"   - Instructions: Season salmon, bake\\n\\n\"\n",
        "        \"2. Mediterranean Bowl\\n\"\n",
        "        \"   - Ingredients: chickpeas, vegetables, tahini\\n\"\n",
        "        \"   - Instructions: Combine ingredients\\n\"\"\"\n",
        "    )\n",
        "\n",
        "    guidelines_md = (\n",
        "        \"\"\"# Dietary Guidelines\\n\\n\"\n",
        "        \"## General Guidelines\\n\"\n",
        "        \"- Eat a variety of foods\\n\"\n",
        "        \"- Control portion sizes\\n\"\n",
        "        \"- Stay hydrated\\n\\n\"\n",
        "        \"## Special Diets\\n\"\n",
        "        \"1. Gluten-Free Diet\\n\"\n",
        "        \"   - Avoid wheat, barley, rye\\n\"\n",
        "        \"   - Focus on naturally gluten-free foods\\n\\n\"\n",
        "        \"2. Diabetic Diet\\n\"\n",
        "        \"   - Monitor carbohydrate intake\\n\"\n",
        "        \"   - Choose low glycemic foods\\n\\n\"\n",
        "        \"3. Heart-Healthy Diet\\n\"\n",
        "        \"   - Limit saturated fats\\n\"\n",
        "        \"   - Choose lean proteins\\n\"\"\"\n",
        "    )\n",
        "\n",
        "    # Save to local .md files\n",
        "    with open(\"recipes.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(recipes_md)\n",
        "    with open(\"guidelines.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(guidelines_md)\n",
        "\n",
        "    print(\"üìÑ Created sample resource files: recipes.md, guidelines.md\")\n",
        "    return [\"recipes.md\", \"guidelines.md\"]\n",
        "\n",
        "sample_files = create_sample_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "create-vector-store",
      "metadata": {},
      "source": [
        "## 3. Create a Vector Store üìö\n",
        "We'll upload our newly created files and group them into a single vector store for searching. This is how the agent can later find relevant text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vector-store-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_vector_store(files, store_name=\"my_health_resources\"):\n",
        "    try:\n",
        "        # Upload each file\n",
        "        uploaded_ids = []\n",
        "        for fp in files:\n",
        "            upl = project_client.agents.upload_file_and_poll(file_path=fp, purpose=\"assistants\")\n",
        "            uploaded_ids.append(upl.id)\n",
        "            print(f\"‚úÖ Uploaded: {fp} -> File ID: {upl.id}\")\n",
        "\n",
        "        # Now create the vector store from these file IDs\n",
        "        vs = project_client.agents.create_vector_store_and_poll(file_ids=uploaded_ids, name=store_name)\n",
        "        print(f\"üéâ Created vector store '{store_name}', ID: {vs.id}\")\n",
        "        return vs, uploaded_ids\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating vector store: {e}\")\n",
        "        return None, []\n",
        "\n",
        "vector_store, file_ids = None, []\n",
        "if sample_files:\n",
        "    vector_store, file_ids = create_vector_store(sample_files, \"health_resources_example\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "create-agent-file-search",
      "metadata": {},
      "source": [
        "## 4. Create the Health Resource Agent üîé\n",
        "We use a **FileSearchTool** pointing to our newly created vector store, then create the Agent with instructions about disclaimers, dietary help, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-agent-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_health_resource_agent(vstore_id):\n",
        "    try:\n",
        "        # Instantiate the file search tool\n",
        "        file_search_tool = FileSearchTool(vector_store_ids=[vstore_id])\n",
        "\n",
        "        # Create the agent\n",
        "        agent = project_client.agents.create_agent(\n",
        "            model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o-mini\"),\n",
        "            name=\"health-search-agent\",\n",
        "            instructions=\"\"\"\n",
        "                You are a health resource advisor with access to dietary and recipe files.\n",
        "                You:\n",
        "                1. Always present disclaimers (you're not a doctor!)\n",
        "                2. Provide references to the files when possible\n",
        "                3. Focus on general nutrition or recipe tips.\n",
        "                4. Encourage professional consultation for more detailed advice.\n",
        "            \"\"\",\n",
        "            tools=file_search_tool.definitions,\n",
        "            tool_resources=file_search_tool.resources\n",
        "        )\n",
        "        print(f\"üéâ Created health resource agent, ID: {agent.id}\")\n",
        "        return agent\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating health resource agent: {e}\")\n",
        "        return None\n",
        "\n",
        "health_agent = None\n",
        "if vector_store:\n",
        "    health_agent = create_health_resource_agent(vector_store.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "health-search-conversation",
      "metadata": {},
      "source": [
        "## 5. Searching Health Resources üèãÔ∏èüë©‚Äçüç≥\n",
        "We'll create a new conversation thread and ask queries like ‚ÄúGluten-free recipe ideas?‚Äù or ‚ÄúHeart-healthy meal plan?‚Äù The agent will do file search on the vector store to find relevant info.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "search-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_search_thread(agent):\n",
        "    try:\n",
        "        # Create a new conversation thread\n",
        "        thread = project_client.agents.create_thread()\n",
        "        print(f\"üìù Created new search thread, ID: {thread.id}\")\n",
        "        return thread\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating search thread: {e}\")\n",
        "        return None\n",
        "\n",
        "def ask_search_question(thread_id, agent_id, user_question):\n",
        "    try:\n",
        "        # Add a user message to the thread with the search query\n",
        "        message = project_client.agents.create_message(\n",
        "            thread_id=thread_id,\n",
        "            role=\"user\",\n",
        "            content=user_question\n",
        "        )\n",
        "        print(f\"üîé Searching: '{user_question}'\")\n",
        "\n",
        "        # Now create_and_process_run to let the agent answer\n",
        "        run = project_client.agents.create_and_process_run(\n",
        "            thread_id=thread_id,\n",
        "            assistant_id=agent_id\n",
        "        )\n",
        "        print(f\"ü§ñ Run finished with status: {run.status}\")\n",
        "        if run.last_error:\n",
        "            print(f\"Error details: {run.last_error}\")\n",
        "        return run\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error searching question: {e}\")\n",
        "        return None\n",
        "\n",
        "# Let's do a couple of sample queries!\n",
        "if health_agent:\n",
        "    search_thread = create_search_thread(health_agent)\n",
        "\n",
        "    if search_thread:\n",
        "        queries = [\n",
        "            \"Could you suggest a gluten-free lunch recipe?\",\n",
        "            \"Show me some heart-healthy meal ideas.\",\n",
        "            \"What guidelines do you have for someone with diabetes?\"\n",
        "        ]\n",
        "\n",
        "        for q in queries:\n",
        "            ask_search_question(search_thread.id, health_agent.id, q)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "view-search-results",
      "metadata": {},
      "source": [
        "## 6. View Results & Citations üìÑ\n",
        "We'll read the conversation thread to see how the agent responded and see if it cited the correct files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "view-search-results-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_thread_messages(thread_id):\n",
        "    try:\n",
        "        messages = project_client.agents.list_messages(thread_id=thread_id)\n",
        "        print(\"\\nüó£Ô∏è Conversation so far:\")\n",
        "        for m in reversed(messages.data):\n",
        "            if m.content:\n",
        "                last_content = m.content[-1]\n",
        "                if hasattr(last_content, \"text\"):\n",
        "                    print(f\"{m.role.upper()}: {last_content.text.value}\\n\")\n",
        "\n",
        "        # Print any file citations\n",
        "        print(\"\\nüìé Checking for citations...\")\n",
        "        for c in messages.file_citation_annotations:\n",
        "            # The file ID is nested in c.file_citation, as a dict with key 'file_id'\n",
        "            print(f\"- Citation snippet: '{c.text}' from file ID: {c.file_citation['file_id']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error displaying messages: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cleanup",
      "metadata": {},
      "source": [
        "## 7. Cleanup & Best Practices üßπ\n",
        "We'll optionally remove the vector store, the uploaded files, and the agent. In a production environment, you might keep them around longer. Meanwhile, here are some tips:\n",
        "\n",
        "1. **Resource Management**\n",
        "   - Keep files grouped by category, regularly prune old or irrelevant files.\n",
        "   - Clear out test agents or vector stores once you're done.\n",
        "\n",
        "2. **Search Queries**\n",
        "   - Provide precise or multi-part queries.\n",
        "   - Consider synonyms or alternative keywords (\"gluten-free\" vs \"celiac\").\n",
        "   \n",
        "3. **Health Information**\n",
        "   - Always disclaim that you are not a medical professional.\n",
        "   - Encourage users to see doctors for specific diagnoses.\n",
        "\n",
        "4. **Performance**\n",
        "   - Keep an eye on vector store size.\n",
        "   - Evaluate search accuracy with `azure-ai-evaluation`!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cleanup-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleanup_all():\n",
        "    try:\n",
        "        # Delete vector store\n",
        "        if 'vector_store' in globals() and vector_store:\n",
        "            project_client.agents.delete_vector_store(vector_store.id)\n",
        "            print(\"üóëÔ∏è Deleted vector store.\")\n",
        "\n",
        "        # Delete uploaded files from service\n",
        "        if 'file_ids' in globals() and file_ids:\n",
        "            for fid in file_ids:\n",
        "                project_client.agents.delete_file(fid)\n",
        "            print(\"üóëÔ∏è Deleted uploaded files from the service.\")\n",
        "\n",
        "        # Delete the agent\n",
        "        if 'health_agent' in globals() and health_agent:\n",
        "            project_client.agents.delete_agent(health_agent.id)\n",
        "            print(\"üóëÔ∏è Deleted health resource agent.\")\n",
        "\n",
        "        # Delete local sample files\n",
        "        if 'sample_files' in globals() and sample_files:\n",
        "            for sf in sample_files:\n",
        "                if os.path.exists(sf):\n",
        "                    os.remove(sf)\n",
        "            print(\"üóëÔ∏è Deleted local sample files.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during cleanup: {e}\")\n",
        "\n",
        "\n",
        "cleanup_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {},
      "source": [
        "# Congratulations! üéâ\n",
        "You've created a **Health Resource Search Agent** that:\n",
        "1. Uses a **Vector Store** to store sample recipes & guidelines.\n",
        "2. **Searches** them to answer queries.\n",
        "3. **Provides disclaimers** reminding users to consult professionals.\n",
        "\n",
        "Feel free to adapt this approach for your own corporate documents, product manuals, or custom health resources. And remember to check out advanced features like **OpenTelemetry** tracing or **Azure AI Evaluation** for continuous improvement.\n",
        "\n",
        "Happy Searching! üéâ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

================
File: 2-notebooks/2-agent_service/4-bing_grounding.ipynb
================
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-bing-grounding",
      "metadata": {},
      "source": [
        "# üçè Health & Fitness Agent with Bing Grounding üçé\n",
        "\n",
        "Welcome to our **Health & Fitness Agent with Bing Grounding** tutorial! In this notebook, we'll demonstrate how to:\n",
        "\n",
        "1. **Initialize** a project using Azure AI Foundry.\n",
        "2. **Create an Agent** with the **BingGroundingTool** for web search.\n",
        "3. **Ask real-world questions** about health and fitness.\n",
        "4. **Showcase disclaimers** that it's not professional medical advice.\n",
        "\n",
        "### ‚ö†Ô∏è Important Medical Disclaimer ‚ö†Ô∏è\n",
        "> **All health information in this notebook is for general educational purposes only and does not replace professional medical advice, diagnosis, or treatment.** Always consult a qualified healthcare professional for personal medical concerns.\n",
        "\n",
        "## Prerequisites\n",
        "- Python 3.8 or later.\n",
        "- `azure-ai-projects`, `azure-ai-inference`, `opentelemetry-sdk`, `azure-core-tracing-opentelemetry` (optional for tracing).\n",
        "- A `.env` file in the parent directory containing:\n",
        "  ```bash\n",
        "  PROJECT_CONNECTION_STRING=<your-connection-string>\n",
        "  MODEL_DEPLOYMENT_NAME=<your-model>\n",
        "  BING_CONNECTION_NAME=<the-name-of-your-bing-connection>\n",
        "  ```\n",
        "\n",
        "## Let's Explore Bing Grounding!\n",
        "We'll integrate **Bing** search results into our agent so it can gather extra context from the web. Let's get started! üéâ\n",
        "\n",
        "<img src=\"./seq-diagrams/4-bing-grounding.png\" width=\"30%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-initialize",
      "metadata": {},
      "source": [
        "## 1. Initial Setup\n",
        "We'll load environment variables from `.env` and initialize our **AIProjectClient** to manage agents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "init-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.projects.models import BingGroundingTool, MessageTextContent\n",
        "\n",
        "# Load environment variables\n",
        "notebook_path = Path().absolute()\n",
        "parent_dir = notebook_path.parent\n",
        "load_dotenv(parent_dir / '.env')\n",
        "\n",
        "# Initialize AIProjectClient\n",
        "try:\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        credential=DefaultAzureCredential(),\n",
        "        conn_str=os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
        "    )\n",
        "    print(\"‚úÖ Successfully initialized AIProjectClient\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing project client: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "create-agent-with-bing-grounding",
      "metadata": {},
      "source": [
        "## 2. Create Bing-Grounded Agent üåê\n",
        "We'll fetch our Bing connection from AI Foundry and use `BingGroundingTool` to let our agent search the web. Then we'll create a new agent with disclaimers about not being a doctor, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "agent-bing-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_bing_grounded_agent():\n",
        "    \"\"\"Create an agent that can use Bing to ground queries with up-to-date info.\"\"\"\n",
        "    try:\n",
        "        # 1. Retrieve Bing connection from your AI Foundry project\n",
        "        bing_conn_name = os.environ.get(\"BING_CONNECTION_NAME\")\n",
        "        if not bing_conn_name:\n",
        "            raise ValueError(\"BING_CONNECTION_NAME not set in .env\")\n",
        "\n",
        "        bing_connection = project_client.connections.get(connection_name=bing_conn_name)\n",
        "        conn_id = bing_connection.id\n",
        "        print(f\"Bing Connection ID: {conn_id}\")\n",
        "\n",
        "        # 2. Initialize Bing grounding tool\n",
        "        bing_tool = BingGroundingTool(connection_id=conn_id)\n",
        "\n",
        "        # 3. Create an agent that can search with Bing\n",
        "        agent = project_client.agents.create_agent(\n",
        "            model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o-mini\"),\n",
        "            name=\"health-bing-agent\",\n",
        "            instructions=\"\"\"\n",
        "                You are a health and fitness assistant with Bing search capabilities.\n",
        "                Always:\n",
        "                1. Provide disclaimers that you are not a medical professional.\n",
        "                2. Encourage professional consultation.\n",
        "                3. Use Bing for real-time references.\n",
        "                4. Provide brief, helpful answers.\n",
        "            \"\"\",\n",
        "            tools=bing_tool.definitions,\n",
        "            # Must pass special preview header to use Bing grounding (subject to change)\n",
        "            headers={\"x-ms-enable-preview\": \"true\"},\n",
        "        )\n",
        "\n",
        "        print(f\"üéâ Created Bing-grounded agent, ID: {agent.id}\")\n",
        "        return agent\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating Bing-grounded agent: {e}\")\n",
        "        return None\n",
        "\n",
        "# Create our Bing-based agent\n",
        "bing_agent = create_bing_grounded_agent()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "create-thread",
      "metadata": {},
      "source": [
        "## 3. Starting a Thread & Asking Questions üí¨\n",
        "Let's create a conversation thread, pose some health/fitness questions that might benefit from searching the web for up-to-date info.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-thread-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_bing_question(agent, user_query):\n",
        "    try:\n",
        "        # Create a thread if we haven't yet\n",
        "        thread = project_client.agents.create_thread()\n",
        "        print(f\"üìù Created a conversation thread, ID: {thread.id}\")\n",
        "\n",
        "        # Post user query as a message\n",
        "        user_message = project_client.agents.create_message(\n",
        "            thread_id=thread.id,\n",
        "            role=\"user\",\n",
        "            content=user_query\n",
        "        )\n",
        "        print(f\"üì® Created user message with query: '{user_query}'\")\n",
        "\n",
        "        # Process the query with the agent\n",
        "        run = project_client.agents.create_and_process_run(\n",
        "            thread_id=thread.id,\n",
        "            assistant_id=agent.id\n",
        "        )\n",
        "        print(f\"ü§ñ Run finished with status: {run.status}\")\n",
        "        if run.last_error:\n",
        "            print(f\"Error detail: {run.last_error}\")\n",
        "\n",
        "        return thread, run\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error asking Bing question: {e}\")\n",
        "        return None, None\n",
        "\n",
        "if bing_agent:\n",
        "    # Let's ask a few fun questions!\n",
        "    questions = [\n",
        "        \"What are some new HIIT workout trends I should know about?\",\n",
        "        \"What's the current WHO recommendation for sugar intake?\",\n",
        "        \"Any news on intermittent fasting for weight management?\"\n",
        "    ]\n",
        "\n",
        "    for q in questions:\n",
        "        ask_bing_question(bing_agent, q)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "view-thread",
      "metadata": {},
      "source": [
        "## 4. Viewing Bing-Grounded Answers\n",
        "We'll see if the agent pulled in external knowledge from Bing. Note that real-time results may vary, and disclaimers should always be present.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "view-thread-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def view_bing_responses(thread_id):\n",
        "    try:\n",
        "        messages = project_client.agents.list_messages(thread_id=thread_id)\n",
        "        print(\"\\nüó£Ô∏è Conversation so far (oldest to newest):\")\n",
        "        for m in reversed(messages.data):\n",
        "            if m.content:\n",
        "                last_content = m.content[-1]\n",
        "                if hasattr(last_content, \"text\"):\n",
        "                    print(f\"{m.role.upper()}: {last_content.text.value}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error viewing Bing responses: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cleanup",
      "metadata": {},
      "source": [
        "## 5. Cleanup & Best Practices\n",
        "You can optionally delete the agent once you're done. In production, you might keep it around for repeated usage.\n",
        "\n",
        "### Best Practices\n",
        "1. **Accuracy** ‚Äì Bing search results may contain disclaimers or partial info. Encourage verification with credible medical sources.\n",
        "2. **Limits** ‚Äì Keep an eye on usage, rate limits, or policy constraints for Bing.\n",
        "3. **Privacy** ‚Äì Filtering search queries is recommended to avoid sending sensitive data.\n",
        "4. **Evaluations** ‚Äì Use `azure-ai-evaluation` for iterative improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cleanup-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleanup_bing_agent(agent):\n",
        "    if agent:\n",
        "        try:\n",
        "            project_client.agents.delete_agent(agent.id)\n",
        "            print(f\"üóëÔ∏è Deleted Bing-grounded agent: {agent.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error cleaning up agent: {e}\")\n",
        "    else:\n",
        "        print(\"No agent to clean up.\")\n",
        "\n",
        "# Uncomment if you want to remove the agent now\n",
        "# cleanup_bing_agent(bing_agent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "final-note",
      "metadata": {},
      "source": [
        "# Congratulations! üéâ\n",
        "You've built a **Bing-Grounded Health & Fitness Agent** that can:\n",
        "1. **Search** the web with Bing.\n",
        "2. **Answer** health/fitness questions with disclaimers.\n",
        "3. **Combine** local instructions with up-to-date external references.\n",
        "\n",
        "Feel free to expand this approach by combining the BingGroundingTool with other tools (e.g., **FileSearchTool**, **CodeInterpreterTool**) to build a robust health advisor. Always keep disclaimers front and center. Have fun exploring!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

================
File: 2-notebooks/2-agent_service/5-agents-aisearch.ipynb
================
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4b6569b8",
      "metadata": {},
      "source": [
        "# üèãÔ∏è AI Search + Agent Service: Fitness-Fun Example ü§∏\n",
        "\n",
        "Welcome to our **AI Search + AI Agent** tutorial, where we'll:\n",
        "\n",
        "1. **Create** an Azure AI Search index with some fitness-oriented sample data\n",
        "2. **Demonstrate** how to connect that index to an Agent via the `AzureAISearchTool`\n",
        "3. **Show** how to query the Agent for health and fitness info in a fun scenario (with disclaimers!)\n",
        "\n",
        "## üè• Health & Fitness Disclaimer\n",
        "> **This notebook is for general demonstration and entertainment purposes, NOT a substitute for professional medical advice.**\n",
        "> Always seek the advice of certified health professionals.\n",
        "\n",
        "## Prerequisites\n",
        "1. A **Microsoft Azure** subscription.\n",
        "2. An **Azure AI Search** resource (formerly \"Cognitive Search\"), with admin API key or role-based access.\n",
        "3. **Python 3.8+**, plus these libraries:\n",
        "   ```bash\n",
        "   pip install azure-search-documents==11.4.0\n",
        "   pip install azure-ai-projects azure-identity\n",
        "   pip install opentelemetry-sdk azure-core-tracing-opentelemetry\n",
        "   ```\n",
        "4. Environment variables or a way to store your secrets:\n",
        "   - `SEARCH_ENDPOINT` - The endpoint URL of your Azure AI Search service\n",
        "   - `SEARCH_API_KEY` - The admin key for your Azure AI Search service\n",
        "   - `PROJECT_CONNECTION_STRING` - Your Azure AI Foundry project connection string (from the overview page)\n",
        "   - `MODEL_DEPLOYMENT_NAME` - The name of your deployed model in Azure AI Foundry\n",
        "\n",
        "## High-Level Flow\n",
        "We'll do the following:\n",
        "1. **Create** an AI Search index programmatically with sample fitness data.\n",
        "2. **Upload** documents (fitness items) to the index.\n",
        "3. **Create** an Agent that references our new index using `AzureAISearchTool`.\n",
        "4. **Run queries** to see how it fetches from the index.\n",
        " \n",
        " <img src=\"./seq-diagrams/5-ai-search.png\" width=\"30%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b27cec",
      "metadata": {},
      "source": [
        "## 1. Create & Populate Azure AI Search Index\n",
        "We'll create a minimal index called `myfitnessindex`, containing a few example items.\n",
        "Make sure to set your environment variables for `SEARCH_ENDPOINT` and `SEARCH_API_KEY`. We'll use the `azure.search.documents.indexes` classes to manage the index schema. We'll also upload some sample data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b555e42d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents.indexes import SearchIndexClient\n",
        "from azure.search.documents.indexes.models import SearchIndex, SimpleField, SearchFieldDataType\n",
        "from azure.search.documents import SearchClient\n",
        "\n",
        "search_endpoint = os.environ.get(\"SEARCH_ENDPOINT\")\n",
        "search_api_key = os.environ.get(\"SEARCH_API_KEY\")\n",
        "\n",
        "index_name = \"myfitnessindex\"\n",
        "\n",
        "try:\n",
        "    credential = AzureKeyCredential(search_api_key)\n",
        "    index_client = SearchIndexClient(endpoint=search_endpoint, credential=credential)\n",
        "    print(\"‚úÖ SearchIndexClient created\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating SearchIndexClient: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8c418b5",
      "metadata": {},
      "source": [
        "**Define the index** schema with a `FitnessItemID` key and a few fields to store product info.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eed5e598",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_fitness_index():\n",
        "    fields = [\n",
        "        SimpleField(name=\"FitnessItemID\", type=SearchFieldDataType.String, key=True),\n",
        "        SimpleField(name=\"Name\", type=SearchFieldDataType.String, filterable=True, searchable=True),\n",
        "        SimpleField(name=\"Category\", type=SearchFieldDataType.String, filterable=True, facetable=True, searchable=True),\n",
        "        SimpleField(name=\"Price\", type=SearchFieldDataType.Double, filterable=True, sortable=True, facetable=True),\n",
        "        SimpleField(name=\"Description\", type=SearchFieldDataType.String, searchable=True)\n",
        "    ]\n",
        "\n",
        "    index = SearchIndex(name=index_name, fields=fields)\n",
        "\n",
        "    # If index already exists, we can delete first to start fresh (optional)\n",
        "    if index_name in [x.name for x in index_client.list_indexes()]:\n",
        "        index_client.delete_index(index_name)\n",
        "        print(f\"üóëÔ∏è Deleted existing index: {index_name}\")\n",
        "\n",
        "    # Create brand-new index\n",
        "    created = index_client.create_index(index)\n",
        "    print(f\"üéâ Created index: {created.name}\")\n",
        "\n",
        "# Create the index\n",
        "create_fitness_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17035c71",
      "metadata": {},
      "source": [
        "**Upload some sample documents** to `myfitnessindex`. We'll add a few items for demonstration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af38671",
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_fitness_docs():\n",
        "    # Construct a search client for data upload\n",
        "    search_client = SearchClient(endpoint=search_endpoint,\n",
        "                                 index_name=index_name,\n",
        "                                 credential=AzureKeyCredential(search_api_key))\n",
        "\n",
        "    sample_docs = [\n",
        "        {\n",
        "            \"FitnessItemID\": \"1\",\n",
        "            \"Name\": \"Adjustable Dumbbell\",\n",
        "            \"Category\": \"Strength\",\n",
        "            \"Price\": 59.99,\n",
        "            \"Description\": \"A compact, adjustable weight for targeted muscle workouts.\"\n",
        "        },\n",
        "        {\n",
        "            \"FitnessItemID\": \"2\",\n",
        "            \"Name\": \"Yoga Mat\",\n",
        "            \"Category\": \"Flexibility\",\n",
        "            \"Price\": 25.0,\n",
        "            \"Description\": \"Non-slip mat designed for yoga, Pilates, and other exercises.\"\n",
        "        },\n",
        "        {\n",
        "            \"FitnessItemID\": \"3\",\n",
        "            \"Name\": \"Treadmill\",\n",
        "            \"Category\": \"Cardio\",\n",
        "            \"Price\": 499.0,\n",
        "            \"Description\": \"A sturdy treadmill with adjustable speed and incline settings.\"\n",
        "        },\n",
        "        {\n",
        "            \"FitnessItemID\": \"4\",\n",
        "            \"Name\": \"Resistance Bands\",\n",
        "            \"Category\": \"Strength\",\n",
        "            \"Price\": 15.0,\n",
        "            \"Description\": \"Set of colorful bands for light to moderate resistance workouts.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Upload in one go\n",
        "    result = search_client.upload_documents(documents=sample_docs)\n",
        "    print(f\"üöÄ Upload result: {result}\")\n",
        "\n",
        "upload_fitness_docs()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d95b1386",
      "metadata": {},
      "source": [
        "### Verify the documents via a basic query\n",
        "Let's do a quick search for **Strength** items.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af93084",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick check\n",
        "search_client = SearchClient(endpoint=search_endpoint,\n",
        "                             index_name=index_name,\n",
        "                             credential=AzureKeyCredential(search_api_key))\n",
        "\n",
        "results = search_client.search(query=\"Strength\", filter=None, top=10)\n",
        "for doc in results:\n",
        "    print(doc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed67b2f4",
      "metadata": {},
      "source": [
        "## 2. Create Agent With AI Search Tool\n",
        "We'll create a new agent and attach an `AzureAISearchTool` referencing **myfitnessindex**.\n",
        "In your environment, you need:\n",
        "- `PROJECT_CONNECTION_STRING` - from your AI Foundry project overview\n",
        "- `MODEL_DEPLOYMENT_NAME` - from the deployed model name\n",
        "\n",
        "Let's initialize the `AIProjectClient` with `DefaultAzureCredential`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c31d0e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.projects.models import AzureAISearchTool, ConnectionType\n",
        "\n",
        "try:\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        credential=DefaultAzureCredential(),\n",
        "        conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n",
        "    )\n",
        "    print(\"‚úÖ Successfully initialized AIProjectClient\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing project client: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4988b6c",
      "metadata": {},
      "source": [
        "### Find or create the Azure AI Search connection within your Foundry project\n",
        "If you've already created a connection for your Azure AI Search resource in the Foundry project, you can list them and pick the correct one. If not, you can create a new connection in the [Portal or CLI usage for Foundry](https://learn.microsoft.com/azure/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de13ece",
      "metadata": {},
      "outputs": [],
      "source": [
        "conn_id = None\n",
        "all_connections = project_client.connections.list()\n",
        "for c in all_connections:\n",
        "    if c.connection_type == ConnectionType.AZURE_AI_SEARCH:\n",
        "        conn_id = c.id\n",
        "        print(f\"Found existing Azure AI Search connection: {conn_id}\")\n",
        "        break\n",
        "\n",
        "if not conn_id:\n",
        "    print(\"‚ùå No Azure AI Search connection found in your project.\\n\",\n",
        "          \"Please create one or ask your admin to do so.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e921f6c7",
      "metadata": {},
      "source": [
        "### Create the Agent with `AzureAISearchTool`\n",
        "We'll attach the tool, specifying the index name we created.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e2fe256",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace with your model deployment name from environment\n",
        "model_name = os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"my-model-deployment\")\n",
        "\n",
        "agent = None\n",
        "if conn_id:\n",
        "    # Initialize the search tool with your index\n",
        "    ai_search_tool = AzureAISearchTool(\n",
        "        index_connection_id=conn_id,\n",
        "        index_name=index_name  # our myfitnessindex\n",
        "    )\n",
        "\n",
        "    # Create the agent\n",
        "    with project_client:\n",
        "        agent = project_client.agents.create_agent(\n",
        "            model=model_name,\n",
        "            name=\"fitness-agent-search\",\n",
        "            instructions=\"\"\"\n",
        "            You are a Fitness Shopping Assistant. You help users find items, but always disclaim not to provide medical advice.\n",
        "            \"\"\",\n",
        "            tools=ai_search_tool.definitions,\n",
        "            tool_resources=ai_search_tool.resources,\n",
        "            headers={\"x-ms-enable-preview\": \"true\"},\n",
        "        )\n",
        "        print(f\"üéâ Created agent, ID: {agent.id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "704445fc",
      "metadata": {},
      "source": [
        "## 3. Run a Conversation with the Agent\n",
        "We'll open a new thread, post a question, and let the agent search the index for relevant items.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ba65c9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_agent_query(question: str):\n",
        "    # Create a new thread\n",
        "    thread = project_client.agents.create_thread()\n",
        "    print(f\"üìù Created thread, ID: {thread.id}\")\n",
        "\n",
        "    # Create a user message\n",
        "    message = project_client.agents.create_message(\n",
        "        thread_id=thread.id,\n",
        "        role=\"user\",\n",
        "        content=question\n",
        "    )\n",
        "    print(f\"üí¨ Created user message, ID: {message.id}\")\n",
        "\n",
        "    # Create and process agent run\n",
        "    run = project_client.agents.create_and_process_run(\n",
        "        thread_id=thread.id,\n",
        "        assistant_id=agent.id\n",
        "    )\n",
        "    print(f\"ü§ñ Agent run status: {run.status}\")\n",
        "\n",
        "    if run.last_error:\n",
        "        print(\"‚ö†Ô∏è Run error:\", run.last_error)\n",
        "\n",
        "    # Retrieve all messages in the thread\n",
        "    msg_list = project_client.agents.list_messages(thread_id=thread.id)\n",
        "    # We'll print the assistant's last reply\n",
        "    for m in reversed(msg_list.data):\n",
        "        if m.role == \"assistant\" and m.content:\n",
        "            print(\"\\nAssistant says:\")\n",
        "            for c in m.content:\n",
        "                if hasattr(c, \"text\"):\n",
        "                    print(c.text.value)\n",
        "            break\n",
        "\n",
        "# Let's try some queries\n",
        "if agent:\n",
        "    run_agent_query(\"Which items are good for strength training?\")\n",
        "    run_agent_query(\"I need something for cardio under $300, any suggestions?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0292c370",
      "metadata": {},
      "source": [
        "## 4. Cleanup\n",
        "We'll clean up the agent. (In production, you might want to keep it!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "285fdc95",
      "metadata": {},
      "outputs": [],
      "source": [
        "if agent:\n",
        "    project_client.agents.delete_agent(agent.id)\n",
        "    print(\"üóëÔ∏è Deleted agent\")\n",
        "\n",
        "index_client.delete_index(index_name)\n",
        "print(f\"üóëÔ∏è Deleted index {index_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcf584e3",
      "metadata": {},
      "source": [
        "# üéâ Congrats!\n",
        "You've successfully:\n",
        "1. **Created** an Azure AI Search index programmatically.\n",
        "2. **Populated** it with sample fitness data.\n",
        "3. **Created** an Agent that queries the index using `AzureAISearchTool`.\n",
        "4. **Asked** the agent for item recommendations.\n",
        "\n",
        "Continue exploring how to integrate **OpenTelemetry** or the `azure-ai-evaluation` library for advanced tracing and evaluation capabilities. Have fun, and stay fit! üèÜ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

================
File: 2-notebooks/2-agent_service/6-agents-az-functions.ipynb
================
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5683064f",
      "metadata": {},
      "source": [
        "# üçè Fitness Fun: Azure Functions + AI Agent Tutorial üçé\n",
        "\n",
        "In this notebook, we'll explore how to use **Azure Functions** with the **Azure AI Foundry** SDKs (`azure-ai-projects`, `azure-ai-inference`, `azure-ai-evaluation`, `opentelemetry-sdk`, and `azure-core-tracing-opentelemetry`). We'll demonstrate how to:\n",
        "\n",
        "1. **Set up** an Azure Function that listens on a storage queue.\n",
        "2. **Create** an AI Agent that can invoke this function.\n",
        "3. **Send** a prompt to the agent, which then calls the Azure Function.\n",
        "4. **Retrieve** the processed result from the output queue.\n",
        "\n",
        "All with a fun, health-and-fitness-themed example! We'll keep it whimsical, but remember:\n",
        "\n",
        "### ‚ö†Ô∏è Important Disclaimer\n",
        "> **This example is for demonstration purposes only and does not provide genuine medical or health advice.** Always consult a professional for real medical or fitness advice.\n",
        "\n",
        "## Prerequisites\n",
        "1. Azure Subscription.\n",
        "2. **Azure AI Foundry** project. (You'll need your `PROJECT_CONNECTION_STRING` and `MODEL_DEPLOYMENT_NAME`.)\n",
        "3. **Azure Functions** environment or local emulator (Azurite), plus storage queue knowledge.\n",
        "4. **Python 3.8+** with `azure-ai-projects`, `azure-identity`, `opentelemetry-sdk`, and `azure-core-tracing-opentelemetry` installed.\n",
        "\n",
        "## Overview\n",
        "We'll do a high-level sequence of events:\n",
        "\n",
        "1. **Azure Function** is set up to read messages from an **input queue** and write responses to an **output queue**.\n",
        "2. **AI Agent** is created with an `AzureFunctionTool` that references these queues.\n",
        "3. **User** provides a question or command; the agent decides whether or not to call the function.\n",
        "4. The agent sends a message to the **input queue**, which triggers the function.\n",
        "5. **Azure Function** processes the message, sends back a response to the **output queue**.\n",
        "6. The agent picks up the response from the output queue.\n",
        "7. The **User** sees the final answer from the agent.\n",
        "\n",
        "<img src=\"./seq-diagrams/6-az-function.png\" width=\"30%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "104a0796",
      "metadata": {},
      "source": [
        "## 1. Azure Function Setup (Example)\n",
        "Below is a snippet of how you'd implement the Azure Function that receives a message from the **input queue** and posts a result to the **output queue**.\n",
        "\n",
        "You can adapt this code to a local or cloud Azure Functions environment. The function's real logic can be anything ‚Äì let's pretend it returns a comedic \"foo-based\" answer or some silly \"fitness advice\" snippet for demonstration. \n",
        "\n",
        "```python\n",
        "# This code might live in your Azure Functions project in a file named: __init__.py\n",
        "# or similar.\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import azure.functions as func\n",
        "from azure.storage.queue import QueueClient\n",
        "from azure.core.pipeline.policies import BinaryBase64EncodePolicy, BinaryBase64DecodePolicy\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "app = func.FunctionApp()\n",
        "\n",
        "@app.function_name(name=\"FooReply\")\n",
        "@app.queue_trigger(\n",
        "    arg_name=\"inmsg\",\n",
        "    queue_name=\"azure-function-foo-input\",\n",
        "    connection=\"STORAGE_SERVICE_ENDPOINT\"  # or connection string setting name\n",
        ")\n",
        "def run_foo(inmsg: func.QueueMessage) -> None:\n",
        "    logging.info(\"Azure Function triggered with a queue item.\")\n",
        "\n",
        "    # This is the queue for output\n",
        "    out_queue = QueueClient(\n",
        "        os.environ[\"STORAGE_SERVICE_ENDPOINT\"],  # or read from config\n",
        "        queue_name=\"azure-function-tool-output\",\n",
        "        credential=DefaultAzureCredential(),\n",
        "        message_encode_policy=BinaryBase64EncodePolicy(),\n",
        "        message_decode_policy=BinaryBase64DecodePolicy()\n",
        "    )\n",
        "\n",
        "    # Parse the function call payload, e.g. { \"query\": \"Hello?\", \"outputqueueuri\":\"...\"}\n",
        "    payload = json.loads(inmsg.get_body().decode('utf-8'))\n",
        "    user_query = payload.get(\"query\", \"\")\n",
        "\n",
        "    # Example: We'll return a comedic 'Foo says: <some witty line>'\n",
        "    result_message = {\n",
        "        \"FooReply\": f\"This is Foo, responding to: {user_query}! Stay strong üí™!\",\n",
        "        \"CorrelationId\": payload.get(\"CorrelationId\", \"\")\n",
        "    }\n",
        "\n",
        "    # Put the result on the output queue\n",
        "    out_queue.send_message(json.dumps(result_message).encode('utf-8'))\n",
        "    logging.info(f\"Sent message: {result_message}\")\n",
        "```\n",
        "\n",
        "### Notes\n",
        "- The input queue name is `azure-function-foo-input`.\n",
        "- The output queue name is `azure-function-tool-output`.\n",
        "- We used environment variables like `STORAGE_SERVICE_ENDPOINT` for the queue storage endpoint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94446a72",
      "metadata": {},
      "source": [
        "## 2. Notebook Setup\n",
        "Now let's switch back to this notebook environment. We'll:\n",
        "1. Import libraries.\n",
        "2. Initialize `AIProjectClient`.\n",
        "3. Create the Azure Function tool definition and the Agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b719c505",
      "metadata": {},
      "outputs": [],
      "source": [
        "# We'll do our standard imports\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.projects.models import AzureFunctionTool, AzureFunctionStorageQueue, MessageRole\n",
        "\n",
        "# Load env variables from .env in parent dir\n",
        "notebook_path = Path().absolute()\n",
        "parent_dir = notebook_path.parent\n",
        "load_dotenv(parent_dir / '.env')\n",
        "\n",
        "# Create AI Project Client\n",
        "try:\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        credential=DefaultAzureCredential(exclude_managed_identity_credential=True, exclude_environment_credential=True),\n",
        "        conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n",
        "    )\n",
        "    print(\"‚úÖ Successfully initialized AIProjectClient\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing AIProjectClient: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c919c6f",
      "metadata": {},
      "source": [
        "### Create Agent with Azure Function Tool\n",
        "We'll define a tool that references our function name (`foo` or `FooReply` from the sample) and the input + output queues. In this example, we'll store the queue endpoint in an env variable called `STORAGE_SERVICE_ENDPOINT`.\n",
        "\n",
        "You can adapt it to your own naming scheme. The agent instructions tell it to use the function whenever it sees certain keywords, or you could just let it call the function on its own.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78008ac0",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    storage_endpoint = os.environ[\"STORAGE_SERVICE_ENDPONT\"]  # Notice it's spelled STORAGE_SERVICE_ENDPONT in sample\n",
        "except KeyError:\n",
        "    print(\"‚ùå Please ensure STORAGE_SERVICE_ENDPONT is set in your environment.\")\n",
        "    storage_endpoint = None\n",
        "\n",
        "agent = None\n",
        "if storage_endpoint:\n",
        "    # Create the AzureFunctionTool object\n",
        "    azure_function_tool = AzureFunctionTool(\n",
        "        name=\"foo\",\n",
        "        description=\"Get comedic or silly advice from 'Foo'.\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query\": {\"type\": \"string\", \"description\": \"The question to ask Foo.\"},\n",
        "                \"outputqueueuri\": {\"type\": \"string\", \"description\": \"The output queue URI.\"}\n",
        "            },\n",
        "        },\n",
        "        input_queue=AzureFunctionStorageQueue(\n",
        "            queue_name=\"azure-function-foo-input\",\n",
        "            storage_service_endpoint=storage_endpoint,\n",
        "        ),\n",
        "        output_queue=AzureFunctionStorageQueue(\n",
        "            queue_name=\"azure-function-tool-output\",\n",
        "            storage_service_endpoint=storage_endpoint,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Construct the agent with the function tool attached\n",
        "    with project_client:\n",
        "        agent = project_client.agents.create_agent(\n",
        "            model=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
        "            name=\"azure-function-agent-foo\",\n",
        "            instructions=(\n",
        "                \"You are a helpful health and fitness support agent.\\n\" \n",
        "                \"If the user says 'What would foo say?' then call the foo function.\\n\" \n",
        "                \"Always specify the outputqueueuri as '\" + storage_endpoint + \"/azure-function-tool-output'.\\n\"\n",
        "                \"Respond with 'Foo says: <response>' after the tool call.\"\n",
        "            ),\n",
        "            tools=azure_function_tool.definitions,\n",
        "        )\n",
        "    print(f\"üéâ Created agent, agent ID: {agent.id}\")\n",
        "else:\n",
        "    print(\"Skipping agent creation, no storage_endpoint.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7908348f",
      "metadata": {},
      "source": [
        "## 3. Test the Agent\n",
        "Now let's simulate a user message that triggers the function call. We'll create a conversation **thread**, post a user question that includes \"What would foo say?\", then run the agent. \n",
        "\n",
        "The Agent Service will place a message on the `azure-function-foo-input` queue. The function will handle it and place a response in `azure-function-tool-output`. The agent will pick that up automatically and produce a final answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f551bf4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_foo_question(user_question: str, agent_id: str):\n",
        "    # 1) Create a new thread\n",
        "    thread = project_client.agents.create_thread()\n",
        "    print(f\"üìù Created thread, thread ID: {thread.id}\")\n",
        "\n",
        "    # 2) Create a user message\n",
        "    message = project_client.agents.create_message(\n",
        "        thread_id=thread.id,\n",
        "        role=\"user\",\n",
        "        content=user_question\n",
        "    )\n",
        "    print(f\"üí¨ Created user message, ID: {message.id}\")\n",
        "\n",
        "    # 3) Create and process agent run\n",
        "    run = project_client.agents.create_and_process_run(\n",
        "        thread_id=thread.id,\n",
        "        assistant_id=agent_id\n",
        "    )\n",
        "    print(f\"ü§ñ Run finished with status: {run.status}\")\n",
        "    if run.status == \"failed\":\n",
        "        print(f\"Run failed: {run.last_error}\")\n",
        "\n",
        "    # 4) Retrieve messages\n",
        "    messages = project_client.agents.list_messages(thread_id=thread.id)\n",
        "    print(\"\\nüó£Ô∏è Conversation:\")\n",
        "    for m in reversed(messages.data):  # oldest first\n",
        "        msg_str = \"\"\n",
        "        if m.content:\n",
        "            msg_str = m.content[-1].text.value if len(m.content) > 0 else \"\"\n",
        "        print(f\"{m.role.upper()}: {msg_str}\\n\")\n",
        "\n",
        "    return thread, run\n",
        "\n",
        "# If the agent was created, let's test it!\n",
        "if agent:\n",
        "    my_thread, my_run = run_foo_question(\n",
        "        user_question=\"What is the best post-workout snack? What would foo say?\",\n",
        "        agent_id=agent.id\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3157768",
      "metadata": {},
      "source": [
        "## 4. Cleanup\n",
        "We'll remove the agent when done. In real scenarios, you might keep your agent for repeated usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e125d5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "if agent:\n",
        "    try:\n",
        "        project_client.agents.delete_agent(agent.id)\n",
        "        print(f\"üóëÔ∏è Deleted agent: {agent.name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error deleting agent: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17e7ab6d",
      "metadata": {},
      "source": [
        "# üéâ Congratulations!\n",
        "You just saw how to combine **Azure Functions** with **AI Agent Service** to create a dynamic, queue-driven workflow. In this whimsical example, your function returned comedic \"Foo says...\" lines, but in real applications, you can harness the power of Azure Functions to run anything from **database lookups** to **complex calculations**, returning the result seamlessly to your AI agent.\n",
        "\n",
        "## Next Steps\n",
        "- **Add OpenTelemetry** to gather end-to-end tracing across your function and agent.\n",
        "- Incorporate an **evaluation** pipeline with `azure-ai-evaluation` to measure how well your agent + function workflow addresses user queries.\n",
        "- Explore **parallel function calls** or more advanced logic in your Azure Functions.\n",
        "\n",
        "Happy coding and stay fit! ü§∏"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

================
File: 2-notebooks/3-quality_attributes/1-Observability.ipynb
================
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e15e24d8",
      "metadata": {},
      "source": [
        "# üçè Observability & Tracing Demo with `azure-ai-projects` and `azure-ai-inference` üçé\n",
        "\n",
        "Welcome to this **Health & Fitness**-themed notebook, where we'll explore:\n",
        "\n",
        "1. **Getting Model Info** with an `AIProjectClient`\n",
        "2. **Listing Connections** to show how we can manage and check all our resources\n",
        "3. **Observability** and tracing examples, showing how to set up:\n",
        "   - Console tracing (OpenTelemetry logs printed to stdout)\n",
        "   - Azure Monitor tracing (sending your logs to an Application Insights resource)\n",
        "   - Viewing your traces in **Azure AI Foundry** üéâ\n",
        "\n",
        "> **Disclaimer**: This is a fun demonstration of AI and observability! Any references to workouts, diets, or health routines in the code or prompts are purely for **educational** purposes. Always consult a professional for health advice. üôå\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8bb63be",
      "metadata": {},
      "source": [
        "\n",
        "<img src=\"./seq-diagrams/1-observability.png\" width=\"50%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65d3f516",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports üõ†Ô∏è\n",
        "In this step, we'll load environment variables (like `PROJECT_CONNECTION_STRING`), then initialize the **`AIProjectClient`**. We'll confirm we can retrieve **model info**. The sample environment variables are typically stored in an `.env` file or in your shell environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c385cb21",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.inference.models import UserMessage\n",
        "from pathlib import Path  # For cross-platform path handling\n",
        "\n",
        "# Get the path to the .env file which is in the parent directory\n",
        "notebook_path = Path().absolute()  # Get absolute path of current notebook\n",
        "parent_dir = notebook_path.parent  # Get parent directory\n",
        "load_dotenv(parent_dir / '.env')  # Load environment variables from .env file\n",
        "\n",
        "connection_string = os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
        "if not connection_string:\n",
        "    raise ValueError(\"üö® PROJECT_CONNECTION_STRING not found in environment. Please set it in your .env.\")\n",
        "\n",
        "try:\n",
        "    # Create the AIProjectClient\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        credential=DefaultAzureCredential(),\n",
        "        conn_str=connection_string\n",
        "    )\n",
        "    print(\"‚úÖ Successfully created AIProjectClient\")\n",
        "    \n",
        "    # Get chat completions client and make request\n",
        "    with project_client.inference.get_chat_completions_client() as inference_client:\n",
        "        response = inference_client.complete(\n",
        "            model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\"),  # Get model name from env or use default\n",
        "            messages=[UserMessage(content=\"How many feet are in a mile?\")]\n",
        "        )\n",
        "        print(\"üí° Response:\")\n",
        "        print(response.choices[0].message.content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Failed to initialize client or get response:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03514f9d",
      "metadata": {},
      "source": [
        "## 2. List & Inspect Connections üîå\n",
        "We'll now demonstrate how to **list connections** in your AI Foundry project. This can help you see all the resources connected, or just a subset (like `AZURE_OPEN_AI` connections).\n",
        "\n",
        "*Note*: We'll just print them out so you can see the details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b39a5984",
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.projects.models import ConnectionType\n",
        "\n",
        "with project_client:\n",
        "    # List all connections\n",
        "    all_conns = project_client.connections.list()\n",
        "    print(f\"üîé Found {len(all_conns)} total connections.\")\n",
        "    for idx, c in enumerate(all_conns):\n",
        "        print(f\"{idx+1}) Name: {c.name}, Type: {c.type}, IsDefault: {c.is_default}\")\n",
        "\n",
        "    # Filter for Azure OpenAI type, as an example\n",
        "    aoai_conns = project_client.connections.list(connection_type=ConnectionType.AZURE_OPEN_AI)\n",
        "    print(f\"\\nüåÄ Found {len(aoai_conns)} Azure OpenAI connections:\")\n",
        "    for c in aoai_conns:\n",
        "        print(f\"   -> {c.name}\")\n",
        "\n",
        "    # Get the default Azure AI Services connection\n",
        "    default_conn = project_client.connections.get_default(\n",
        "        connection_type=ConnectionType.AZURE_AI_SERVICES,\n",
        "        include_credentials=False\n",
        "    )\n",
        "    if default_conn:\n",
        "        print(\"\\n‚≠ê Default Azure AI Services connection:\")\n",
        "        print(default_conn)\n",
        "    else:\n",
        "        print(\"No default connection found for Azure AI Services.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90145015",
      "metadata": {},
      "source": [
        "## 3. Observability & Tracing üåê\n",
        "\n",
        "### 3.1 Console Tracing Example\n",
        "We'll set up **console** tracing with the `opentelemetry` library so that logs are printed to `sys.stdout`. This is helpful for local debugging or minimal setups. We'll do a small chat completion example for fun (like asking a health question)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c389dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from azure.ai.inference.models import UserMessage\n",
        "from opentelemetry import trace\n",
        "\n",
        "# We'll enable local console tracing so we can see the telemetries in our terminal\n",
        "project_client.telemetry.enable(destination=sys.stdout)\n",
        "\n",
        "# We'll do a small LLM call example:\n",
        "try:\n",
        "    with project_client.inference.get_chat_completions_client() as client:\n",
        "        prompt_msg = \"I'd like to start a simple home workout routine. Any tips?\"\n",
        "        response = client.complete(\n",
        "            model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"some-deployment-name\"),\n",
        "            messages=[UserMessage(content=prompt_msg)]\n",
        "        )\n",
        "        print(\"\\nü§ñ Response:\", response.choices[0].message.content)\n",
        "except Exception as exc:\n",
        "    print(f\"‚ùå Chat Completions example failed: {exc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27e07fa7",
      "metadata": {},
      "source": [
        "### 3.2 Azure Monitor Tracing Example\n",
        "Now, instead of just console logs, we can push these logs to **Application Insights** (Azure Monitor) for deeper **APM** (application performance monitoring) and persistent logs.\n",
        "\n",
        "In order to do this, ensure you have an Application Insights **Connection String** associated with your AI Foundry project. Then configure your local environment to pull that connection string and set up `opentelemetry` for remote ingestion.\n",
        "\n",
        "We'll do a quick demonstration of how to do that (similar to the official sample)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88b7d563",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from azure.monitor.opentelemetry import configure_azure_monitor\n",
        "from azure.ai.inference.models import UserMessage\n",
        "\n",
        "# Enable Azure Monitor tracing if available\n",
        "connection_str = project_client.telemetry.get_connection_string()\n",
        "if connection_str:\n",
        "    print(\"üîß Found App Insights connection string. Configuring...\")\n",
        "    configure_azure_monitor(connection_string=connection_str)\n",
        "    project_client.telemetry.enable()  # add optional additional instrumentations\n",
        "    \n",
        "    # We'll do a test chat call again, which should get logged to Azure Monitor\n",
        "    try:\n",
        "        with project_client.inference.get_chat_completions_client() as client:\n",
        "            prompt_msg = \"Any low-impact exercises recommended for knee issues?\"\n",
        "            response = client.complete(\n",
        "                model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"some-deployment-name\"),\n",
        "                messages=[UserMessage(content=prompt_msg)]\n",
        "            )\n",
        "            print(\"\\nü§ñ Response (logged to App Insights):\", response.choices[0].message.content)\n",
        "    except Exception as exc:\n",
        "        print(f\"‚ùå Chat Completions with Azure Monitor example failed: {exc}\")\n",
        "else:\n",
        "    print(\"No Application Insights connection string is configured in this project.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6b572ab",
      "metadata": {},
      "source": [
        "## 4. Wrap-Up & Next Steps üéâ\n",
        "\n",
        "Congrats on exploring:\n",
        "1. Basic usage of **AIProjectClient** (model info, listing connections)\n",
        "2. **Observability** with console tracing\n",
        "3. **Application Insights**-based tracing for deeper logs & APM\n",
        "\n",
        "**Where to go next?**\n",
        "- **AI Foundry Portal**: Under the **Tracing** tab, you can see your traces in an easy UI.\n",
        "- **Azure Monitor**: Head into the Application Insights resource for advanced metrics, logs, and dashboards.\n",
        "- **azure-ai-evaluation**: Evaluate the quality of your LLM outputs, get scoring metrics, or embed it in your CI/CD pipeline.\n",
        "\n",
        "> üçÄ **Health Reminder**: All suggestions from the LLM are for demonstration only. Always consult professionals for health and fitness guidance.\n",
        "\n",
        "Enjoy building robust, observable GenAI apps! üèãÔ∏è‚Äç‚ôÇÔ∏è"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "name": "Observability_and_Tracing_Demo"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

================
File: 2-notebooks/3-quality_attributes/2-evaluation.ipynb
================
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "29901878",
      "metadata": {},
      "source": [
        "# üèãÔ∏è‚Äç‚ôÄÔ∏è Health & Fitness Evaluations with Azure AI Foundry üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
        "\n",
        "This notebook demonstrates how to **evaluate** a Generative AI model using the **Azure AI Foundry** ecosystem. We'll highlight the interplay of three key SDKs:\n",
        "\n",
        "1. **`azure-ai-projects`** (`AIProjectClient`): to manage & orchestrate evaluations from the cloud.\n",
        "2. **`azure-ai-inference`**: to perform model inference (optional, but relevant if you want to generate responses for evaluation).\n",
        "3. **`azure-ai-evaluation`**: to run automated metrics for LLM output quality & safety.\n",
        "\n",
        "We'll create or use some synthetic *health & fitness* Q&A data, then measure how well your model is answering. We'll do a **local** evaluation and a **cloud** evaluation on an **Azure AI Project**. üöÄ\n",
        "\n",
        "## üçâ Notebook Contents\n",
        "1. [Setup & Imports](#1-Setup-and-Imports)\n",
        "2. [Mermaid Diagram of the Flow](#2-Mermaid-Diagram)\n",
        "3. [Local Evaluation Example](#3-Local-Evaluation)\n",
        "4. [Cloud Evaluation with `AIProjectClient`](#4-Cloud-Evaluation)\n",
        "5. [Conclusion](#5-Conclusion)\n",
        "\n",
        "## ‚ö†Ô∏èDisclaimer\n",
        "> This notebook deals with a hypothetical **health & fitness** scenario. **No real medical advice** is provided. Always seek professional guidance when needed!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf2060ce",
      "metadata": {
        "id": "1-Setup-and-Imports"
      },
      "source": [
        "## 1. Setup and Imports\n",
        "We'll install necessary libraries, import them, and define some synthetic data. \n",
        "\n",
        "### Dependencies\n",
        "- `azure-ai-projects` (manages project-based evaluations in the cloud)\n",
        "- `azure-ai-evaluation` (provides built-in metrics like `F1ScoreEvaluator`, `RelevanceEvaluator`, etc.)\n",
        "- `azure-ai-inference` (optionally used if you want to generate completions/chats to produce data to evaluate)\n",
        "- `azure-identity` (for Azure authentication)\n",
        "- `opentelemetry-sdk` and `azure-core-tracing-opentelemetry` if you want to enable advanced tracing (optional).\n",
        "\n",
        "### Synthetic Data\n",
        "We'll create a small JSONL with *health & fitness* Q&A pairs for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b21da07c",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# If you need to install these, uncomment:\n",
        "# !pip install azure-ai-projects azure-ai-evaluation azure-ai-inference azure-identity\n",
        "\n",
        "import json\n",
        "import os\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# We'll create a synthetic dataset in JSON Lines format\n",
        "synthetic_eval_data = [\n",
        "    {\n",
        "        \"query\": \"How can I start a beginner workout routine at home?\",\n",
        "        \"context\": \"Workout routines can include push-ups, bodyweight squats, lunges, and planks.\",\n",
        "        \"response\": \"You can just go for 10 push-ups total.\",\n",
        "        \"ground_truth\": \"At home, you can start with short, low-intensity workouts. Examples: push-ups, lunges, and planks in short sets.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Are diet sodas healthy for daily consumption?\",\n",
        "        \"context\": \"Sugar-free or diet drinks may reduce sugar intake, but they still contain artificial sweeteners.\",\n",
        "        \"response\": \"Yes, diet sodas are 100% healthy.\",\n",
        "        \"ground_truth\": \"Diet sodas are lower in sugar than regular soda, but they're not necessarily 'healthy' for daily consumption due to artificial additives.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What's the capital of France?\",\n",
        "        \"context\": \"France is a country in Europe. Paris is the capital.\",\n",
        "        \"response\": \"London.\",\n",
        "        \"ground_truth\": \"Paris.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Write them to a local JSONL file\n",
        "eval_data_path = Path(\"./health_fitness_eval_data.jsonl\")\n",
        "with eval_data_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for row in synthetic_eval_data:\n",
        "        f.write(json.dumps(row) + \"\\n\")\n",
        "\n",
        "print(f\"Sample evaluation data written to {eval_data_path.resolve()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01cd6ec0",
      "metadata": {
        "id": "2-Mermaid-Diagram"
      },
      "source": [
        "<img src=\"./seq-diagrams/2-evals.png\" width=\"50%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c00480fe",
      "metadata": {
        "id": "3-Local-Evaluation"
      },
      "source": [
        "# 3. Local Evaluation\n",
        "\n",
        "We'll show how to run local, code-based evaluation. Specifically, we'll combine a couple of built-in evaluators:\n",
        "- [**F1ScoreEvaluator**](https://aka.ms/azureaieval-python-ref/f1score)\n",
        "- [**RelevanceEvaluator**](https://aka.ms/azureaieval-python-ref/relevance)\n",
        "\n",
        "Then we'll see how they do on each row in our synthetic data.\n",
        "\n",
        "## Steps\n",
        "1. Import the evaluators\n",
        "2. Construct a local `evaluate(...)` run specifying each evaluator.\n",
        "3. Inspect results.\n",
        "\n",
        "### Note\n",
        "For RelevanceEvaluator (and other AI-assisted evaluators like Groundedness or Coherence), we need a GPT model config. We'll skip real model endpoints here, but the code is shown.\n",
        "\n",
        "We'll also show how we can do code-based custom evaluators (like a simple function that checks length!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac5e5d66",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from azure.ai.evaluation import (\n",
        "    evaluate,\n",
        "    F1ScoreEvaluator,\n",
        "    RelevanceEvaluator\n",
        ")\n",
        "\n",
        "# We'll define a dummy custom evaluator that just logs the length of the response.\n",
        "def response_length_eval(response, **kwargs):\n",
        "    return {\"resp_length\": len(response)}\n",
        "\n",
        "# If you have a real Azure OpenAI model, fill in:\n",
        "my_model_config = {\n",
        "    \"azure_endpoint\": os.environ.get(\"MY_AOAI_ENDPOINT\", \"https://dummy-endpoint.azure.com/\"),\n",
        "    \"api_key\": os.environ.get(\"MY_AOAI_KEY\", \"fake-key\"),\n",
        "    \"azure_deployment\": os.environ.get(\"MY_AOAI_DEPLOYMENT_NAME\", \"gpt-4\"),\n",
        "    \"api_version\": os.environ.get(\"MY_AOAI_API_VERSION\", \"2023-03-15-preview\"),\n",
        "}\n",
        "\n",
        "# Let's instantiate them\n",
        "f1_eval = F1ScoreEvaluator()  # no GPT needed\n",
        "relevance_eval = RelevanceEvaluator(model_config=my_model_config)  # GPT-based\n",
        "\n",
        "result = evaluate(\n",
        "    data=str(eval_data_path),\n",
        "    evaluators={\n",
        "        \"f1_score\": f1_eval,\n",
        "        \"relevance\": relevance_eval,\n",
        "        \"resp_len\": response_length_eval\n",
        "    },\n",
        "    # We can specify how to map the columns from our data to the arguments of each evaluator.\n",
        "    evaluator_config={\n",
        "        \"f1_score\": {\n",
        "            \"column_mapping\": {\n",
        "                \"response\": \"${data.response}\",\n",
        "                \"ground_truth\": \"${data.ground_truth}\"\n",
        "            }\n",
        "        },\n",
        "        \"relevance\": {\n",
        "            \"column_mapping\": {\n",
        "                \"query\": \"${data.query}\",\n",
        "                \"response\": \"${data.response}\"\n",
        "            }\n",
        "        },\n",
        "        \"resp_len\": {\n",
        "            \"column_mapping\": {\n",
        "                \"response\": \"${data.response}\"\n",
        "            }\n",
        "        },\n",
        "    },\n",
        "    # We won't specify azure_ai_project or output_path to keep it local.\n",
        ")\n",
        "\n",
        "print(\"Local evaluation result =>\\n\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "935ff841",
      "metadata": {},
      "source": [
        "### Inspecting Local Results\n",
        "The `evaluate(...)` function returns a dictionary with:\n",
        "- **`metrics`**: aggregated metrics across rows (like average `f1_score` or average `relevance`)\n",
        "- **`rows`**: row-by-row results with inputs and the computed evaluator outputs\n",
        "- **`traces`**: if you had debugging or additional info\n",
        "\n",
        "Example:\n",
        "```python\n",
        "{\n",
        "  'metrics': { ... },\n",
        "  'rows': [\n",
        "     {\n",
        "       'inputs.response': 'Yes, diet sodas are 100% healthy.',\n",
        "       'outputs.f1_score.f1_score': 0.0,\n",
        "       'outputs.relevance.relevance': 3.0,\n",
        "       'outputs.resp_len.resp_length': 31,\n",
        "       ...\n",
        "     },\n",
        "     ...\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "Our `result` object can be used for analysis or exported to another location."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97f624ac",
      "metadata": {
        "id": "4-Cloud-Evaluation"
      },
      "source": [
        "# 4. Azure Evaluation with `AIProjectClient`\n",
        "\n",
        "Sometimes, we want to scale our evaluation to the cloud, track the results in an **Azure AI Project**, or schedule recurring evaluations. We'll do that by using:\n",
        "- `AIProjectClient` from `azure-ai-projects`\n",
        "- `Evaluation` from `azure.ai.projects.models`.\n",
        "\n",
        "We'll show how you might:\n",
        "1. **Upload** the local JSONL to your Azure AI Project\n",
        "2. **Create** an `Evaluation` referencing a built-in evaluator\n",
        "3. **Submit** the evaluation & poll for results\n",
        "4. **Fetch** & check the final status and (optionally) get a link to AI Studio.\n",
        "\n",
        "## Prerequisites\n",
        "- Azure AI Foundry (AI Hub / Project) with a project-level connection string.\n",
        "- A GPT-based Azure OpenAI deployment if you want to do GPT-based evaluators like Relevance.\n",
        "  \n",
        "### Let's demonstrate now. üéâ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25095dbb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.projects.models import (\n",
        "    Evaluation, Dataset, EvaluatorConfiguration, ConnectionType\n",
        ")\n",
        "from azure.ai.evaluation import F1ScoreEvaluator, RelevanceEvaluator, ViolenceEvaluator\n",
        "\n",
        "# We'll show a minimal example, referencing code from sample.\n",
        "\n",
        "# 1. Connect to Azure AI Project\n",
        "project_connection_string = os.environ.get(\"PROJECT_CONNECTION_STRING\", \"<YOUR_CONNECTION_STRING>\")\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "project_client = AIProjectClient.from_connection_string(\n",
        "    credential=credential,\n",
        "    conn_str=project_connection_string,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Created AIProjectClient.\")\n",
        "\n",
        "# 2. Upload data for evaluation\n",
        "data_id, _ = project_client.upload_file(str(eval_data_path))\n",
        "print(\"‚úÖ Uploaded local JSONL. Data asset ID:\", data_id)\n",
        "\n",
        "# 3. Let's define a connection for GPT-based evaluator (Relevance). We'll assume you have a default AOAI conn.\n",
        "default_connection = project_client.connections.get_default(connection_type=ConnectionType.AZURE_OPEN_AI)\n",
        "deployment_name = os.environ.get(\"DEPLOYMENT_NAME\", \"gpt-4\")\n",
        "api_version = os.environ.get(\"AOAI_API_VERSION\", \"2023-06-01-preview\")\n",
        "\n",
        "# 4. Construct the evaluation object\n",
        "evaluation = Evaluation(\n",
        "    display_name=\"Health Fitness Remote Evaluation\",\n",
        "    description=\"Evaluating dataset for correctness and violence.\",\n",
        "    data=Dataset(id=data_id),\n",
        "    evaluators={\n",
        "        # NLP-based\n",
        "        \"f1_score\": EvaluatorConfiguration(\n",
        "            id=F1ScoreEvaluator.id,\n",
        "        ),\n",
        "        # GPT-based\n",
        "        \"relevance\": EvaluatorConfiguration(\n",
        "            id=RelevanceEvaluator.id,\n",
        "            init_params={\n",
        "                \"model_config\": default_connection.to_evaluator_model_config(\n",
        "                    deployment_name=deployment_name, api_version=api_version\n",
        "                )\n",
        "            },\n",
        "        ),\n",
        "        # Safety-based (violence)\n",
        "        \"violence\": EvaluatorConfiguration(\n",
        "            id=ViolenceEvaluator.id,\n",
        "            init_params={\"azure_ai_project\": project_client.scope},\n",
        "        ),\n",
        "    },\n",
        ")\n",
        "\n",
        "evaluation_response = project_client.evaluations.create(\n",
        "    evaluation=evaluation,\n",
        ")\n",
        "print(\"‚úÖ Created evaluation job. ID:\", evaluation_response.id)\n",
        "\n",
        "# Optionally, we can fetch the status.\n",
        "get_evaluation_response = project_client.evaluations.get(evaluation_response.id)\n",
        "print(\"Current status:\", get_evaluation_response.status)\n",
        "\n",
        "# The evaluation may still be in progress. We can poll or just wait.\n",
        "print(\"You can check the Azure AI Project UI to see the final results!\")\n",
        "if isinstance(get_evaluation_response.properties, dict):\n",
        "    print(\"AI Studio link:\", get_evaluation_response.properties.get(\"AiStudioEvaluationUri\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b09de21e",
      "metadata": {},
      "source": [
        "### Observing AI Foundry Evaluation Results\n",
        "In the output logs, you'll see an `AiStudioEvaluationUri` that links to your Azure AI Project in the Azure portal. There, you can:\n",
        "- View aggregated metrics (like average F1 Score or average Relevance Score)\n",
        "- Inspect row-level details to see which queries had the highest or lowest performance.\n",
        "\n",
        "Once the job completes, the final status is `Succeeded`, `Failed`, or `Cancelled`. You can store these metrics for auditing or continuous improvement.\n",
        "\n",
        "## Scheduling Evaluations\n",
        "Using `AIProjectClient`, you can also schedule recurring evaluations (e.g., daily) on new or streaming data. Check out `EvaluationSchedule` in the docs for more advanced usage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d6e583",
      "metadata": {
        "id": "5-Conclusion"
      },
      "source": [
        "# 5. Conclusion üèÅ\n",
        "\n",
        "In this notebook, we:\n",
        "1. Created *synthetic* health & fitness Q&A data.\n",
        "2. Ran a **local evaluation** with the `evaluate()` function from `azure-ai-evaluation`.\n",
        "3. Demonstrated a **cloud evaluation** using the `AIProjectClient` to create an `Evaluation`.\n",
        "\n",
        "## Next Steps & Tips\n",
        "- Add **OpenTelemetry** to trace your calls for advanced debugging.\n",
        "- Combine more metrics: `GroundednessEvaluator`, `SelfHarmEvaluator`, etc. for a thorough analysis.\n",
        "- Create your own **custom** code-based or prompt-based evaluators to handle domain-specific success metrics.\n",
        "- Explore the **Adversarial** or **Simulator** features in `azure-ai-evaluation` to generate test data.\n",
        "\n",
        "## Resources\n",
        "- [azure-ai-evaluation Documentation](https://aka.ms/azureaieval-python-ref)\n",
        "- [azure-ai-projects Documentation](https://aka.ms/azure-aiprojects)\n",
        "- [azure-ai-inference Documentation](https://aka.ms/azure-ai-inference)\n",
        "\n",
        "Thanks for following along ‚Äì keep building healthy, high-quality AI apps! üçè‚ú®"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

================
File: 3-ai-native-e2e-sample/backend/README.md
================
# Drug Development Platform Backend üß¨

## Overview üéØ
Backend for the Drug Development Platform, leveraging Azure AI Foundry SDKs for molecular analysis and clinical trial monitoring.

## Azure AI Integration ü§ñ

### SDKs Used
- üéØ **azure-ai-projects**: Project and agent management
  ```python
  # main.py
  project_client = AIProjectClient.from_connection_string(
      credential=credential,
      conn_str=PROJECT_CONNECTION_STRING
  )
  ```

- üî¨ **azure-ai-inference**: Molecular analysis and predictions
  ```python
  # molecular_design.py
  inference_client = project_client.inference.get_chat_completions_client()
  ```

- üìä **azure-ai-evaluation**: Result analysis and confidence scoring
  ```python
  # automated_testing.py
  evaluation = Evaluation(
      display_name="Drug Analysis Evaluation",
      description="Evaluation of drug analysis outputs",
      data=Dataset(id=data_id),
      evaluators={
          "f1_score": EvaluatorConfiguration(id=F1ScoreEvaluator.id),
          "relevance": EvaluatorConfiguration(id=RelevanceEvaluator.id)
      }
  )
  ```

- üìà **opentelemetry-sdk**: Performance monitoring and tracing
  ```python
  # main.py
  trace.set_tracer_provider(TracerProvider())
  tracer = trace.get_tracer(__name__)
  ```

- üîê **azure-identity**: Secure Azure authentication
  ```python
  # main.py
  credential = DefaultAzureCredential()
  ```

## Getting Started üöÄ

1. **Create Virtual Environment**:
   ```bash
   python -m venv venv
   source venv/bin/activate  # Windows: venv\Scripts\activate
   ```

2. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure Environment**:
   - Copy `.env.example` to `.env`
   - Update with your Azure credentials:
   ```env
   # Azure Authentication (DO NOT commit actual values!)
   AZURE_CLIENT_ID=your_client_id_here
   AZURE_CLIENT_SECRET=your_client_secret_here
   AZURE_TENANT_ID=your_tenant_id_here
   
   # AI Foundry Configuration (use your project-specific values)
   PROJECT_CONNECTION_STRING=your_connection_string_here
   MODEL_DEPLOYMENT_NAME=your_model_name_here
   
   # OpenTelemetry Settings (adjust based on your environment)
   OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
   OTEL_SERVICE_NAME=drug-discovery-platform
   OTEL_RESOURCE_ATTRIBUTES=deployment.environment=development
   ```

   > ‚ö†Ô∏è **Security Note**: Never commit real credentials or sensitive information to version control.
   > Always use environment variables or secure secret management solutions in production.

   Required Environment Variables:
   - `AZURE_CLIENT_ID`: Azure AD application ID for authentication
   - `AZURE_CLIENT_SECRET`: Azure AD application secret for authentication
   - `AZURE_TENANT_ID`: Azure AD tenant ID for authentication
   - `PROJECT_CONNECTION_STRING`: AI Foundry project connection string for agent and model access
   - `MODEL_DEPLOYMENT_NAME`: Name of your deployed model for agent creation
   - `OTEL_EXPORTER_OTLP_ENDPOINT`: OpenTelemetry collector endpoint for tracing
   - `OTEL_SERVICE_NAME`: Service name for tracing (default: drug-discovery-platform)
   - `OTEL_RESOURCE_ATTRIBUTES`: Additional tracing attributes (default: deployment.environment=development)

4. **Run the Server**:
   ```bash
   uvicorn main:app --reload
   ```

5. **Access API Documentation**:
   - OpenAPI: [http://localhost:8000/docs](http://localhost:8000/docs)
   - ReDoc: [http://localhost:8000/redoc](http://localhost:8000/redoc)

## API Architecture üèóÔ∏è

```mermaid
flowchart TB
    subgraph Client
        FE[Frontend]
    end
    
    subgraph Backend
        API[FastAPI]
        AI[Azure AI Clients]
        TEL[OpenTelemetry]
    end
    
    subgraph Azure["Azure AI Foundry"]
        PRJ[AI Projects]
        AGT[AI Agents]
        INF[AI Inference]
        EVAL[AI Evaluation]
        subgraph Agents["AI Agent Tools"]
            BING[Bing Grounding]
            FUNC[Function Calling]
            CODE[Code Interpreter]
        end
    end
    
    FE <--> API
    API <--> AI
    AI <--> PRJ
    PRJ <--> AGT
    AGT <--> BING
    AGT <--> FUNC
    AGT <--> CODE
    PRJ <--> INF
    PRJ <--> EVAL
    API --> TEL
```

## Endpoints üõ†Ô∏è

### Molecular Design Endpoints

#### 1. Analyze Molecule
```bash
POST /molecular-design/analyze
```
Analyzes a drug candidate's molecular properties using Azure AI Inference.

Request:
```json
{
    "id": "DRUG-001",
    "molecule_type": "Small Molecule",
    "therapeutic_area": "Oncology",
    "target_proteins": ["EGFR", "HER2"],
    "development_stage": "Phase 1"
}
```

Response:
```json
{
    "message": "Molecular analysis complete",
    "analysis": {
        "efficacy_score": 0.85,
        "safety_score": 0.92,
        "confidence": 0.89
    }
}
```

### Azure AI Agent Endpoints

#### 1. Literature Search
```bash
POST /agents/literature-search
```
Uses Azure AI Agent's Bing grounding capability to search and analyze scientific literature.

Request:
```json
{
    "query": "EGFR inhibitors in lung cancer"
}
```

Response:
```json
{
    "query": "EGFR inhibitors in lung cancer",
    "summary": "Analysis of recent publications...",
    "agent_id": "agent-123"
}
```

#### 2. Molecule Analysis
```bash
POST /agents/molecule-analysis
```
Uses Azure AI Agent's function calling capability to analyze molecular properties.

Request:
```json
{
    "smiles": "CC1=CC=C(C=C1)NC(=O)C2=CC=C(Cl)C=C2",
    "target_proteins": ["EGFR", "HER2"],
    "therapeutic_area": "Oncology"
}
```

Response:
```json
{
    "molecule": "CC1=CC=C(C=C1)NC(=O)C2=CC=C(Cl)C=C2",
    "analysis": "Detailed molecular property analysis...",
    "agent_id": "agent-456"
}
```

#### 3. Clinical Trial Data Analysis
```bash
POST /agents/data-analysis
```
Uses Azure AI Agent's code interpreter capability to analyze trial data.

Request:
```
multipart/form-data
- file: trial_data.csv
```

Response:
```json
{
    "filename": "trial_data.csv",
    "analysis": "Statistical analysis and visualizations...",
    "agent_id": "agent-789"
}
```

### Clinical Trials Endpoints

#### 1. Monitor Trials
```bash
GET /clinical-trials/monitor?trial_id=TRIAL-001
```
Real-time monitoring of clinical trial metrics and patient responses.

Response:
```json
{
    "trial_id": "TRIAL-001",
    "phase": "Phase 2",
    "status": "Active",
    "real_time_metrics": {
        "enrollment_rate": 0.75,
        "retention_rate": 0.92,
        "safety_signals": []
    }
}
```

#### 2. Predict Patient Response
```bash
POST /clinical-trials/predict-response
```
Predicts individual patient response using biomarker analysis.

Request:
```json
{
    "trial_id": "TRIAL-001",
    "patient_id": "PAT-001"
}
```

Response:
```json
{
    "predicted_response": 0.85,
    "confidence": 0.92,
    "recommendations": [
        "Continue monitoring key biomarkers",
        "Schedule follow-up in 2 weeks"
    ]
}
```

### Automated Testing Endpoints

#### 1. Run Evaluation Demo

```mermaid
sequenceDiagram
    participant Client
    participant API
    participant Evaluator
    participant Dataset
    
    Client->>API: POST /evaluation/run-demo
    API->>Dataset: Upload JSONL Data
    API->>Evaluator: Configure Evaluation
    Evaluator->>Evaluator: Run F1 Score
    Evaluator->>Evaluator: Run Relevance
    Evaluator-->>API: Results
    API-->>Client: Metrics
```

## OpenTelemetry Integration üìä

This project uses OpenTelemetry for distributed tracing to monitor and debug the drug development pipeline. Traces help us understand:

- üîç Performance bottlenecks
- üîó Request flows through the system
- ‚ùå Error patterns and their context
- üìà AI model inference timing

### Viewing Traces

1. Traces are collected by the OpenTelemetry collector at:
   ```
   http://localhost:4318/v1/traces
   ```

2. Key spans to monitor:
   - `molecular_design.analyze`: Molecule analysis and AI inference
   - `clinical_trials.monitor`: Trial monitoring and metrics
   - `clinical_trials.predict_response`: Patient response predictions

3. Important attributes in traces:
   - `molecule.id`: Unique identifier for drug candidates
   - `molecule.type`: Type of molecule being analyzed
   - `therapeutic.area`: Target therapeutic area
   - `analysis.efficacy`: Predicted efficacy score
   - `analysis.safety`: Safety assessment score
   - `trial.id`: Clinical trial identifier
   - `patient.id`: Patient identifier for specific analyses

### Example Trace Analysis

```mermaid
sequenceDiagram
    participant C as Client
    participant A as API
    participant AI as Azure AI
    participant DB as Database
    
    C->>A: POST /molecular-design/analyze
    activate A
    A->>AI: Analyze molecule
    AI-->>A: Inference results
    A->>DB: Store results
    A-->>C: Analysis response
    deactivate A
```

## Security üîí

1. **Environment Variables**:
   - All secrets stored in `.env`
   - Never commit sensitive data

2. **Authentication**:
   - Azure AD integration
   - Role-based access control

3. **Data Protection**:
   - Encryption at rest
   - Secure communication

## Contributing ü§ù
1. Fork the repository
2. Create a feature branch
3. Submit a Pull Request

## Setup and Running

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Start the server (runs on port 8002):

   Windows (PowerShell):
   ```powershell
   .\start.ps1
   ```
   
   Unix/Linux/MacOS:
   ```bash
   ./start.sh
   ```

## Environment Variables

The backend server uses the following environment variables:

- `PORT`: Server port (defaults to 8002)
- `PROJECT_CONNECTION_STRING`: Azure project connection string
- `MODEL_DEPLOYMENT_NAME`: Azure model deployment name
- Other environment variables as specified in main.py

## API Documentation

Once the server is running, you can access the API documentation at:
- http://localhost:8002/ (ReDoc interface)

## Development

The server includes:
- FastAPI framework
- OpenTelemetry instrumentation
- CORS middleware configured for frontend at http://localhost:3000
- Health check endpoint at /health

## Deployment with Azure Developer CLI (azd) üöÄ

1. **Install Azure Developer CLI**:
   ```bash
   curl -fsSL https://aka.ms/install-azd.sh | bash
   ```

2. **Login to Azure**:
   ```bash
   azd auth login
   ```

3. **Deploy from Root Directory**:
   From the root of "3-e2e-drug-discovery-sample/", run:
   ```bash
   azd init
   azd up
   ```

This will:
- Create Azure Container App for the backend
- Configure environment variables from your .env
- Deploy the backend service
- Output the public endpoint URL

After deployment:
- Access API documentation at `https://<your-app>.azurecontainerapps.io/docs`
- Use the endpoint URL in your frontend configuration

## Learn More üìö
- [Azure AI Projects SDK](https://learn.microsoft.com/python/api/overview/azure/ai-projects-readme?view=azure-python-preview)
- [Azure AI Inference SDK](https://learn.microsoft.com/python/api/overview/azure/ai-inference-readme?view=azure-python-preview)
- [Azure AI Evaluation SDK](https://learn.microsoft.com/python/api/overview/azure/ai-evaluation-readme?view=azure-python-preview)
- [OpenTelemetry Documentation](https://opentelemetry.io/docs/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

================
File: 3-ai-native-e2e-sample/frontend/README.md
================
# Drug Development Platform Frontend üß¨

## Overview üéØ
Frontend for the Drug Development Platform, built with Next.js and React. Features a modern UI for molecular analysis, literature search, and clinical trial monitoring.

## Key Features üåü

- üî¨ **Molecular Analysis**: Interactive 3D molecule viewer using 3Dmol.js
- üìö **Literature Search**: AI-powered scientific literature analysis
- üìä **Clinical Trial Analysis**: Data visualization and analysis tools
- üé® **Theme Support**: Light/dark mode with next-themes
- üéØ **Responsive Design**: Mobile-first approach using Tailwind CSS

## Tech Stack üíª

- üöÄ **Next.js**: React framework for production
- üé® **Tailwind CSS**: Utility-first CSS framework
- üìä **Recharts**: Composable charting library
- üéØ **Radix UI**: Accessible component primitives
- üîÑ **Zustand**: State management
- üé≠ **Lucide**: Beautiful icons
- üß™ **3Dmol.js**: Molecular visualization

## Getting Started üöÄ

1. **Clone and Install Dependencies**:
   ```bash
   cd frontend
   npm install
   ```

2. **Environment Setup**:
   Create a `.env.local` file:
   ```env
   # Backend API URL
   NEXT_PUBLIC_API_URL=http://localhost:8000
   
   # Default theme (light/dark)
   NEXT_PUBLIC_DEFAULT_THEME=dark
   ```

3. **Development Server**:
   ```bash
   npm run dev
   ```
   Visit [http://localhost:3000](http://localhost:3000)

4. **Build for Production**:
   ```bash
   npm run build
   npm run start
   ```

## Project Structure üìÅ

```
frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app/                 # Next.js app router
‚îÇ   ‚îú‚îÄ‚îÄ components/         
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ui/             # Reusable UI components
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ molecule-viewer  # 3D molecule visualization
‚îÇ   ‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ store/          # Zustand state management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/          # Utility functions
‚îÇ   ‚îî‚îÄ‚îÄ styles/             # Global styles
‚îú‚îÄ‚îÄ public/                 # Static assets
‚îî‚îÄ‚îÄ package.json           # Dependencies and scripts
```

## Key Components üîß

### Molecule Viewer
```typescript
<MoleculeViewer
  smiles="CC1=CC=C(C=C1)NC(=O)C2=CC=C(Cl)C=C2"
  width="100%"
  height="400px"
/>
```

### Literature Search
```typescript
<LiteratureSearch
  onSearch={async (query) => {
    const results = await searchLiterature(query);
    // Handle results...
  }}
/>
```

### Clinical Trial Analysis
```typescript
<TrialAnalysis
  data={trialData}
  onAnalyze={async (data) => {
    const analysis = await analyzeTrialData(data);
    // Handle analysis...
  }}
/>
```

## API Integration üîå

The frontend communicates with the backend through RESTful endpoints:

```typescript
// Example API call
const searchLiterature = async (query: string) => {
  const response = await fetch(`${API_URL}/agents/literature-search`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ query })
  });
  return response.json();
};
```

## State Management üîÑ

Using Zustand for global state management:

```typescript
const useDrugDiscoveryStore = create((set) => ({
  literatureResults: null,
  moleculeAnalysis: null,
  trialAnalysis: null,
  // ... actions and state updates
}));
```

## Contributing ü§ù

1. Fork the repository
2. Create a feature branch
3. Submit a Pull Request

## Learn More üìö

- [Next.js Documentation](https://nextjs.org/docs)
- [Tailwind CSS Documentation](https://tailwindcss.com/docs)
- [Radix UI Documentation](https://www.radix-ui.com/docs/primitives)
- [3Dmol.js Documentation](https://3dmol.csb.pitt.edu/)
- [Zustand Documentation](https://github.com/pmndrs/zustand)

## Security üîí

- All API calls use HTTPS
- Environment variables for sensitive data
- CSP headers for security
- Input sanitization and validation

## Deployment with Azure Developer CLI (azd) üöÄ

1. **Install Azure Developer CLI**:
   ```bash
   curl -fsSL https://aka.ms/install-azd.sh | bash
   ```

2. **Login to Azure**:
   ```bash
   azd auth login
   ```

3. **Deploy from Root Directory**:
   From the root of "3-e2e-drug-discovery-sample/", run:
   ```bash
   azd init
   azd up
   ```

This will:
- Create Azure Static Web App for the frontend
- Configure environment variables
- Deploy the frontend application
- Output the public URL for your site

After deployment:
- Access your site at the provided Static Web App URL
- The backend API URL will be automatically configured

## License üìÑ

This project is licensed under the MIT License - see the LICENSE file for details.

================
File: assets/diagrams/agent-tools-flow.mmd
================
sequenceDiagram
    participant User
    participant Agent
    participant Search as AI Search
    participant Code as Code Interpreter
    participant Bing as Bing Search
    
    User->>Agent: Query with data analysis
    
    par Search Knowledge Base
        Agent->>Search: Vector search request
        Search-->>Agent: Relevant documents
    and Execute Analysis
        Agent->>Code: Process data
        Code-->>Agent: Analysis results
    and Ground Information
        Agent->>Bing: Verify claims
        Bing-->>Agent: Current data
    end
    
    Agent->>User: Comprehensive response

================
File: assets/diagrams/agent-tools.mmd
================
sequenceDiagram
    participant User
    participant Agent
    participant FileSearch as File Search Tool
    participant CodeInterp as Code Interpreter
    participant Bing as Bing Search
    
    User->>Agent: Health analysis request
    alt Search Documents
        Agent->>FileSearch: Search health records
        FileSearch-->>Agent: Return relevant docs
    else Calculate Metrics
        Agent->>CodeInterp: Process health data
        CodeInterp-->>Agent: Return calculations
    else Ground Information
        Agent->>Bing: Verify health claims
        Bing-->>Agent: Return current data
    end
    Agent->>User: Provide comprehensive response

================
File: assets/diagrams/e2e-architecture.mmd
================
graph TB
    subgraph Frontend["Frontend (React + Vite)"]
        UI[React UI]
        State[Application State]
        Events[Event Handlers]
    end
    
    subgraph Backend["Backend (FastAPI)"]
        API[FastAPI]
        AI[Azure AI SDKs]
        EventHub[Event Hub]
        Monitor[OpenTelemetry]
    end
    
    subgraph Azure["Azure AI Foundry"]
        Agents[AI Agents]
        Models[AI Models]
        Search[AI Search]
        EventProcessor[Event Processor]
    end
    
    UI --> API
    API --> AI
    API --> EventHub
    EventHub --> EventProcessor
    EventProcessor --> Agents
    Agents --> Models
    Agents --> Search

================
File: assets/diagrams/rag-flow.mmd
================
sequenceDiagram
    participant User
    participant Agent
    participant AISearch as Azure AI Search
    participant LLM as Azure AI Model
    
    User->>Agent: Query about health topic
    Agent->>AISearch: Search relevant documents
    AISearch-->>Agent: Return matching content
    Agent->>LLM: Generate response with context
    LLM-->>Agent: Return grounded response
    Agent->>User: Provide answer with citations

================
File: assets/overrides/index.html
================
{% extends "base.html" %}

{% block scripts %}
  <!-- Add scripts that need to run before here -->
  {{ super() }}
  <!-- Add scripts that need to run afterwards here -->
  <script type="text/javascript">
    (function(c,l,a,r,i,t,y){
        c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
        t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
        y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
    })(window, document, "clarity", "script", "gpxwnp6rhi");
    </script>
{% endblock %}

<!-- Meta tags -->
{% block site_meta %}
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />

<!-- Page description -->
    {% if page.meta and page.meta.description %}
        <meta name="description" content="{{ page.meta.description }}" />
    {% elif config.site_description %}
        <meta name="description" content="{{ config.site_description }}" />
    {% endif %}

    <!-- Page author -->
    {% if page.meta and page.meta.author %}
        <meta name="author" content="{{ page.meta.author }}" />
    {% elif config.site_author %}
        <meta name="author" content="{{ config.site_author }}" />
    {% endif %}

    <!-- Canonical -->
    {% if page and page.meta and page.meta.canonical_url %}
        <link rel="canonical" href="{{ page.meta.canonical_url }}">
    {% elif page.canonical_url %}
        <link rel="canonical" href="{{ page.canonical_url }}">
    {% endif %}

    <!-- Previous page -->
    {% if page.previous_page %}
        <link rel="prev" href="{{ page.previous_page.url | url }}" />
    {% endif %}

    <!-- Next page -->
    {% if page.next_page %}
        <link rel="next" href="{{ page.next_page.url | url }}" />
    {% endif %}

    <!-- Favicon -->
    <link rel="icon" href="{{ config.theme.favicon | url }}" />

    <!-- Generator banner -->
    <meta
            name="generator"
            content="mkdocs-{{ mkdocs_version }}, $md-name$-$md-version$"
    />
{% endblock %}

{% block extrahead %}
    {% if page and page.meta and page.meta.canonical_url %}
        <meta name="og:url" content="{{ page.meta.canonical_url }}" />
    {% endif %}
{% endblock %}

================
File: index.md
================
# Building a Health & Fitness AI Advisor with Azure AI Foundry üèÉ‚Äç‚ôÇÔ∏è

Welcome to this hands-on, 2-hour workshop where you'll build a practical health and fitness AI advisor using Azure AI Foundry! You'll learn how to deploy an AI model, create an intelligent agent, and evaluate its performance - all through an engaging health-focused use case. üí™

## The Use Case: Smart Health Advisory

You'll build an AI agent that can:
- Provide personalized fitness guidance
- Handle nutrition and exercise inquiries
- Access health and wellness resources
- Learn from user interactions
- Provide safe, accurate health advice with disclaimers

## Workshop Timeline (2 hours)

1. **Setup and Model Deployment (30 min)**
   - Quick platform overview
   - Deploy Azure OpenAI model
   - Basic configuration and testing

2. **Agent Development (45 min)**
   - Create health advisor agent
   - Implement health guidance system
   - Add health knowledge base

3. **Evaluation and Monitoring (45 min)**
   - Set up key metrics
   - Monitor performance
   - Analyze and improve responses

## Prerequisites

- Azure subscription with AI services access
- Python 3.8 or later
- Basic Python knowledge
- Text editor or IDE

## What You'll Learn

Through this practical example, you'll understand:
- How to use the AI Foundry SDK
- Model deployment and configuration
- Agent creation and management
- Performance evaluation and monitoring
- Best practices for AI applications

Let's start by [setting up your environment](introduction/overview.md)!

================
File: integrations/azure-services.md
================
# Azure Service Integrations

## Data Storage Services

### Azure Cosmos DB „Äê‚Ä†L5„Äë
[Azure Cosmos DB](https://learn.microsoft.com/azure/cosmos-db) provides vector search capabilities and scalable data storage:
- Vector search for semantic queries
- Global distribution
- Multi-model support
Learn more about [vector search in Cosmos DB](https://learn.microsoft.com/azure/cosmos-db/vector-search).

### Azure Database for PostgreSQL „Äê‚Ä†L6„Äë
[Azure Database for PostgreSQL](https://learn.microsoft.com/azure/postgresql) with pgvector extension:
- Vector storage and search
- SQL + vector queries
- Managed service benefits
Learn more about [vector support in PostgreSQL](https://learn.microsoft.com/azure/postgresql/flexible-server/concepts-vectors).

## API and Compute Services

### Azure API Management „Äê‚Ä†L7„Äë
[Azure API Management](https://learn.microsoft.com/azure/api-management) for managing AI endpoints:
- API gateway for AI services
- Request/response transformation
- Security and monitoring
Learn more about [AI integration in APIM](https://learn.microsoft.com/azure/api-management/ai-integration).

### Azure Logic Apps „Äê‚Ä†L8„Äë
[Azure Logic Apps](https://learn.microsoft.com/azure/logic-apps) for AI workflow automation:
- AI workflow orchestration
- Event-driven processing
- Service integration
Learn more about [AI integration in Logic Apps](https://learn.microsoft.com/azure/logic-apps/ai-integration).

### Azure Functions „Äê‚Ä†L9„Äë
[Azure Functions](https://learn.microsoft.com/azure/azure-functions) for serverless AI:
- Serverless compute for AI
- Event-driven processing
- Automatic scaling
Learn more about [AI integration in Functions](https://learn.microsoft.com/azure/azure-functions/ai-integration).

================
File: llms.txt
================
# Azure AI Foundry Workshop Context

## Core SDKs
- azure-ai-projects: Project and agent management
  - Connection string format: "{region}.api.azureml.ms;{subscription_id};{resource_group};{workspace}"
  - Used for creating and managing AI projects and agents
  - Handles model deployment and monitoring
  - Supports agent tool configuration and coordination

- azure-ai-inference: Chat completions and inference
  - Supports streaming responses and async operations
  - Handles token management and rate limiting
  - Integrates with OpenTelemetry for monitoring
  - Provides content safety filters and validation

- azure-ai-evaluation: Performance evaluation
  - Groundedness assessment for response validation
  - Relevance metrics for response quality
  - Response validation against source context
  - Integration with Azure Monitor

## Agent Tools
- File Search: Vector search using Azure AI Search
  - Document retrieval and ranking
  - Semantic search capabilities
  - Context-aware responses
  - Multi-format support

- Code Interpreter: Python code execution
  - Secure sandboxed environment
  - Data analysis and visualization
  - Package management
  - Result interpretation

- Bing Grounding: Real-time web data
  - Current information verification
  - Source credibility checking
  - Multi-source validation
  - Safe search integration

## Implementation Patterns
- RAG with Azure AI Search
  - Vector storage and retrieval
  - Semantic ranking
  - Context augmentation
  - Response generation

- Multi-tool Agent Coordination
  - Tool selection and routing
  - Context sharing
  - Error handling
  - State management

- Event-driven Processing
  - Real-time event handling
  - Asynchronous operations
  - Message queuing
  - State persistence

- Real-time Monitoring
  - OpenTelemetry integration
  - Performance metrics
  - Error tracking
  - Resource utilization

## E2E Sample Architecture
- FastAPI Backend
  - Azure SDK integration
  - Authentication handling
  - API versioning
  - Request validation

- React Frontend
  - Real-time updates
  - Streaming responses
  - Error handling
  - Progress indicators

- Event-driven Architecture
  - Message queuing
  - State management
  - Error recovery
  - Scale handling

- Observability
  - Distributed tracing
  - Metric collection
  - Log aggregation
  - Performance monitoring

================
File: README.md
================
# Azure AI Foundry Workshop

<div align="center">

[üì¶Prerequisites](#-prerequisites) | [üöÄQuick Start](#-quick-start) | [ü§ñOverview](#-overview) | [üìîWorkshop Content](#-workshop-content) | [üß©Project Structure](#-project-structure) | [‚ùìSupport](#-support) | [ü§ùContributing](#-contributing)

</div>


## ü§ñ Overview

A hands-on workshop that guides you through building intelligent apps and AI agents on top of Azure AI Foundry, with fun examples related to health and dietary advice. You will:
- Learn Azure AI Foundry fundamentals
- Set up authentication and project configuration
- Deploy and test AI models
- Build AI agents (health advisor examples)
- Implement health calculations and dietary planning
- Evaluate agent performance and monitor quality attributes
- Deploy an end-to-end AI native sample app that incorporates all these capabilities and design patterns

> **Duration**: 4-5 hours  
> **Focus**: Hands-on exercises, interactive notebooks, practical examples, end-to-end project

## üì∏ Visuals

### Sections Overview
![Sections](sections.png)
*Figure 1: Overview of different sections in this workshop.*

### End-to-End Sample UI
![e2esample](./3-ai-native-e2e-sample/assets/ui.png)
*Figure 2: User interface of the end-to-end AI native sample project.*

---

## üì¶ Prerequisites    

Before starting the workshop, ensure you have:

- [Python 3.10](https://www.python.org/downloads/) or higher installed
- An active Azure subscription with access to [Azure AI Foundry](https://ai.azure.com)
- [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) installed
- [Git](https://git-scm.com/downloads) installed
- [VS Code](https://code.visualstudio.com/), [GitHub Codespaces](https://github.com/features/codespaces), or [Jupyter Notebook](https://jupyter.org/install) environment
- Basic Python programming knowledge
- Model deployment and [AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) connection configured in Azure AI Foundry

---

## üöÄ Quick Start

1. **Clone the repo**:
   ```bash
   git clone https://github.com/Azure/ai-foundry-workshop.git
   cd ai-foundry-workshop
   ```

2. **Install uv**:
   ```bash
   # Unix/Linux/macOS
   curl -LsSf https://astral.sh/uv/install.sh | sh

   # Windows (PowerShell)
   (Invoke-WebRequest -Uri https://astral.sh/uv/install.ps1 -UseBasicParsing).Content | pwsh
   ```

3. **Create & activate a virtual environment**:
   ```bash
   uv venv
   source .venv/bin/activate  # Windows: .venv\Scripts\activate
   ```

4. **Set up Azure AI Foundry**:

   a. **Create Project and Deploy Resources**:
      1. Go to [Azure AI Foundry](https://ai.azure.com)
      2. Create a new AI Hub and Project using the AI Foundry Wizard
      3. Deploy required models:
         - GPT models(gpt-4o, gpt-4o-mini) for chat/completion (**set TPM to max** to avoid issues with Agents notebooks)
         - Embedding model for vector search
         - Ensure the model is deployed in `Global-Standard` or `DataZone-Standard`
      4. Set up connections:
         - Configure [Grounding with Bing](https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/bing-grounding?view=azure-python-preview&tabs=python&pivots=overview) connection
         - Configure Azure AI Search connection
      5. Add your user account to the `Azure AI Developer` role from Azure AI Foundry Management Portal

   b. **Configure Environment Variables**:
      ```bash
      cp .env.example .env
      ```
      Update `.env` with your Azure AI Foundry values:
      - `PROJECT_CONNECTION_STRING`: Your project connection string from Azure ML workspace
      - `MODEL_DEPLOYMENT_NAME`: Your model deployment name
      - `EMBEDDING_MODEL_DEPLOYMENT_NAME`: Your embedding model deployment name
      - `TENANT_ID`: Your tenant ID from Azure portal
      - `BING_CONNECTION_NAME`: Your Bing search connection name
      - `SERVERLESS_MODEL_NAME`: Your serverless model name

      > **Note**: The model specified in `MODEL_DEPLOYMENT_NAME` must be supported by Azure AI Agents Service or Assistants API. See [supported models](https://learn.microsoft.com/en-us/azure/ai-services/agents/concepts/model-region-support?tabs=python#azure-openai-models) for details. For Grounding with Bing Search, you need to use `gpt-4o-mini` model.

5. **Install dependencies**:
   ```bash
   # Install core Azure AI SDKs and Jupyter requirements
   uv pip install azure-identity azure-ai-projects azure-ai-inference[opentelemetry] azure-search-documents azure-ai-evaluation azure-monitor-opentelemetry

   # Install Jupyter requirements
   uv pip install ipykernel jupyterlab notebook

   # Register the kernel with Jupyter
   python -m ipykernel install --user --name=.venv --display-name="Python (.venv)"

   # Install additional requirements (optional - for deploying repo or running mkdocs)
   uv pip install -r requirements.txt
   ```

   > **Note**: If you encounter kernel errors in VS Code, try:
   > 1. Select kernel: Click "Select Kernel" > "Python Environments" > "Python (.venv)"
   > 2. If kernel is not listed, run `python -m ipykernel install --user --name=.venv` again, or use the "Create New Kernel" wizard in VS Code to create a new Python environment
   > 3. Reload VS Code if needed

6. **Choose your notebook environment**:

   **Option A: VS Code**
   - Install [VS Code Python extension](https://marketplace.visualstudio.com/items?itemName=ms-python.python)
   - Install either:
     - [Jupyter extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) for .ipynb files
     - [Polyglot Notebooks extension](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.dotnet-interactive-vscode) for .dib files
   - Open any notebook and select your Python kernel (.venv)

   **Option B: GitHub Codespaces**
   - Click "Code" > "Create codespace" on the repository
   - Wait for environment setup
   - Notebooks will be ready to run

   **Option C: Jupyter Lab/Notebook**
   ```bash
   # Install Jupyter if you haven't already
   uv pip install jupyterlab notebook

   # Start Jupyter Lab (recommended)
   jupyter lab

   # Or start Jupyter Notebook
   jupyter notebook
   ```

7. **Follow the Learning Path**:
    1. **Introduction** (`1-introduction/`)
       - `1-authentication.ipynb`: Set up your Azure credentials
       - `2-environment_setup.ipynb`: Configure your environment
       - `3-quick_start.ipynb`: Learn basic operations

    2. **Main Workshop** (`2-notebooks/`)
       - Chat Completion & RAG (`1-chat_completion/`)
       - Agent Development (`2-agent_service/`)
       - Quality Attributes (`3-quality_attributes/`)

---

## üìî Workshop Learning Path

Follow these notebooks in sequence to complete the workshop:

### 1. Introduction (`1-introduction/`)
| Notebook | Description |
|----------|-------------|
| [1. Authentication](1-introduction/1-authentication.ipynb) | Set up Azure credentials and access |
| [2. Environment Setup](1-introduction/2-environment_setup.ipynb) | Configure your development environment |
| [3. Quick Start](1-introduction/3-quick_start.ipynb) | Learn basic Azure AI Foundry operations |

### 2. Main Workshop (`2-notebooks/`)
| Topic | Notebooks |
|-------|-----------|
| **Chat Completion & RAG** | ‚Ä¢ [Chat Completion & RaG](2-notebooks/1-chat_completion/) |
| **Agent Development** | ‚Ä¢ [Agent Development](2-notebooks/2-agent_service/) |
| **Quality Attributes** | ‚Ä¢ [Observability & Evaluations](2-notebooks/3-quality_attributes/) |

---

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details on how to:
- Submit bug reports and feature requests
- Submit pull requests
- Follow our coding standards
- Participate in code reviews

---

## ‚ùì Support

If you need help or have questions:

---

================
File: workshop/architecture.md
================
# E2E Sample Architecture üèóÔ∏è

## Overview
The end-to-end sample demonstrates a complete AI-native application using Azure AI Foundry services.

## Architecture Diagram

```mermaid
graph TB
    subgraph Frontend["Frontend (React + Vite)"]
        UI[React UI]
        State[Application State]
        Events[Event Handlers]
    end
    
    subgraph Backend["Backend (FastAPI)"]
        API[FastAPI]
        AI[Azure AI SDKs]
        EventHub[Event Hub]
        Monitor[OpenTelemetry]
    end
    
    subgraph Azure["Azure AI Foundry"]
        Agents[AI Agents]
        Models[AI Models]
        Search[AI Search]
        EventProcessor[Event Processor]
    end
    
    UI --> API
    API --> AI
    API --> EventHub
    EventHub --> EventProcessor
    EventProcessor --> Agents
    Agents --> Models
    Agents --> Search
```

## Components

### Backend Services
- FastAPI application
- [Azure AI Foundry](https://learn.microsoft.com/azure/ai-foundry) SDKs integration „Äê‚Ä†L1„Äë
- [Azure Functions](https://learn.microsoft.com/azure/azure-functions) for serverless compute „Äê‚Ä†L4„Äë
- OpenTelemetry instrumentation
- Event-driven architecture with [Logic Apps](https://learn.microsoft.com/azure/logic-apps) „Äê‚Ä†L5„Äë

### Frontend Application
- React with Vite
- Real-time updates
- Interactive visualizations
- Dark/light theme support

For implementation details, see:
- [Backend Implementation](../3-ai-native-e2e-sample/backend/README.md)
- [Frontend Implementation](../3-ai-native-e2e-sample/frontend/README.md)

================
File: workshop/README.md
================
# Azure AI Foundry Workshop (4-5 hours)

This hands-on workshop guides you through building AI-native applications using Azure AI Foundry, with examples related to health and dietary advice.

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/Azure/ai-foundry-workshop)

You can run this workshop in several ways:
1. **GitHub Codespaces**: Click the button above to start coding in your browser
2. **Visual Studio Code**: Visit [aka.ms/wksp](https://aka.ms/wksp) to set up your local environment
3. **Your IDE**: Fork and clone this repository to work locally

üëâ **Get Started**:
1. ‚≠ê Star this repository to show your support
2. üîÑ Fork it to your account to start building
3. üìù Follow the learning path below

## Learning Path

### 1. Introduction (30 minutes)
- [Authentication](../1-introduction/1-authentication.ipynb): Set up Azure credentials
- [Environment Setup](../1-introduction/2-environment_setup.ipynb): Configure development environment
- [Quick Start](../1-introduction/3-quick_start.ipynb): Basic Azure AI operations

### 2. Chat Completion & RAG (1 hour)
- [Basic Chat Completion](../2-notebooks/1-chat_completion/1-basic-chat-completion.ipynb)
- [Embeddings](../2-notebooks/1-chat_completion/2-embeddings.ipynb)
- [Basic RAG](../2-notebooks/1-chat_completion/3-basic-rag.ipynb)
- [Phi-4 Model](../2-notebooks/1-chat_completion/4-phi-4.ipynb)

### 3. Building AI Agents (2 hours)
- [Agent Basics](../2-notebooks/2-agent_service/1-basics.ipynb)
- [Code Interpreter](../2-notebooks/2-agent_service/2-code_interpreter.ipynb)
- [File Search](../2-notebooks/2-agent_service/3-file-search.ipynb)
- [Bing Grounding](../2-notebooks/2-agent_service/4-bing_grounding.ipynb)
- [AI Search Integration](../2-notebooks/2-agent_service/5-agents-aisearch.ipynb)
- [Azure Functions Deployment](../2-notebooks/2-agent_service/6-agents-az-functions.ipynb)

### 4. Quality & Monitoring (30 minutes)
- [Observability](../2-notebooks/3-quality_attributes/1-Observability.ipynb)
- [Evaluation](../2-notebooks/3-quality_attributes/2-evaluation.ipynb)

### 5. E2E Sample Application (1 hour)
Build a complete AI-native health advisor application:
- [Sample Architecture](architecture.md)
- [Backend Implementation](../3-ai-native-e2e-sample/backend/README.md)
- [Frontend Implementation](../3-ai-native-e2e-sample/frontend/README.md)

## Prerequisites
See the [Prerequisites](../README.md) section for detailed setup instructions.

## Azure Documentation
For detailed service documentation, refer to:
- [Azure AI Foundry](https://learn.microsoft.com/azure/ai-foundry) „Äê‚Ä†L1„Äë
- [Azure AI Services](https://learn.microsoft.com/azure/ai-services) „Äê‚Ä†L2„Äë
- [Azure AI Search](https://learn.microsoft.com/azure/search/search-what-is-azure-search) „Äê‚Ä†L3„Äë

## Copy Context for LLMs
To get AI-friendly context about Azure AI SDKs and implementation patterns, click the "Copy Context for LLMs" button at the top of this page. This will copy relevant information about:
- Azure AI Projects SDK
- Azure AI Inference SDK
- Azure AI Agent patterns and tools



================================================================
End of Codebase
================================================================

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0c0ebe",
   "metadata": {},
   "source": [
    "# ğŸ‹ï¸â€â™€ï¸ Azure AI Foundryë¥¼ ì‚¬ìš©í•œ ê±´ê°• ë° í”¼íŠ¸ë‹ˆìŠ¤ í‰ê°€ ğŸ‹ï¸â€â™‚ï¸\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **Azure AI Foundry** ì—ì½”ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ì—¬ ìƒì„± AI ëª¨ë¸(ë˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜)ì„ **í‰ê°€**í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì„¸ ê°€ì§€ ì£¼ìš” Python SDKë¥¼ ì¤‘ì ì ìœ¼ë¡œ ì‚´í´ë´…ë‹ˆë‹¤:\n",
    "1. **`azure-ai-projects`** (`AIProjectClient`): í´ë¼ìš°ë“œì—ì„œ í‰ê°€ë¥¼ ê´€ë¦¬ ë° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.\n",
    "2. **`azure-ai-inference`**: ëª¨ë¸ ì¶”ë¡  ìˆ˜í–‰(ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ í‰ê°€ìš© ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê²½ìš° ìœ ìš©í•¨).\n",
    "3. **`azure-ai-evaluation`**: LLM ì¶œë ¥ í’ˆì§ˆ ë° ì•ˆì „ì„±ì— ëŒ€í•œ ìë™í™”ëœ ë©”íŠ¸ë¦­ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "\"ê±´ê°• ë° í”¼íŠ¸ë‹ˆìŠ¤\" Q&A ë°ì´í„°ë¥¼ ê°€ìƒìœ¼ë¡œ ìƒì„±í•˜ê±°ë‚˜ ì‚¬ìš©í•œ ë‹¤ìŒ, ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì˜ ëŒ€ë‹µí•˜ëŠ”ì§€ ì¸¡ì •í•©ë‹ˆë‹¤. **ë¡œì»¬** í‰ê°€ì™€ **í´ë¼ìš°ë“œ** í‰ê°€ë¥¼ ëª¨ë‘ ìˆ˜í–‰í•©ë‹ˆë‹¤(Azure AI Foundry í”„ë¡œì íŠ¸ì—ì„œ).\n",
    "\n",
    "> **ê³ ì§€ ì‚¬í•­**: ì´ ë‚´ìš©ì€ ê°€ìƒì˜ ê±´ê°• ë° í”¼íŠ¸ë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. **ì‹¤ì œ ì˜í•™ì  ì¡°ì–¸**ì€ ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í•­ìƒ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.\n",
    "\n",
    "## ë…¸íŠ¸ë¶ ë‚´ìš©\n",
    "1. [ì„¤ì • ë° ê°€ì ¸ì˜¤ê¸°](#1-Setup-and-Imports)\n",
    "2. [ë¡œì»¬ í‰ê°€ ì˜ˆì‹œ](#3-Local-Evaluation)\n",
    "3. [`AIProjectClient`ë¡œ í´ë¼ìš°ë“œ í‰ê°€](#4-Cloud-Evaluation)\n",
    "4. [ì¶”ê°€ ì£¼ì œ](#5-Extra-Topics)\n",
    "   - [ìœ„í—˜ ë° ì•ˆì „ í‰ê°€ì](#5.1-Risk-and-Safety)\n",
    "   - [ë” ë§ì€ í’ˆì§ˆ í‰ê°€ì](#5.2-Quality)\n",
    "   - [ì‚¬ìš©ì ì§€ì • í‰ê°€ì](#5.3-Custom)\n",
    "   - [ì‹œë®¬ë ˆì´í„° ë° ì ëŒ€ì  ë°ì´í„°](#5.4-Simulators)\n",
    "5. [ê²°ë¡ ](#6-ê²°ë¡ )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfadf84",
   "metadata": {
    "id": "1-Setup-and-Imports"
   },
   "source": [
    "## 1. ì„¤ì • ë° ê°€ì ¸ì˜¤ê¸°\n",
    "We'll install necessary libraries, import them, and define some synthetic data. \n",
    "\n",
    "### Dependencies\n",
    "- `azure-ai-projects` for orchestrating evaluations in your Azure AI Foundry Project.\n",
    "- `azure-ai-evaluation` for built-in or custom metrics (like Relevance, Groundedness, F1Score, etc.).\n",
    "- `azure-ai-inference` (optional) if you'd like to generate completions to produce data to evaluate.\n",
    "- `azure-identity` (for Azure authentication via `DefaultAzureCredential`).\n",
    "\n",
    "### Synthetic Data\n",
    "We'll create a small JSONL with *health & fitness* Q&A pairs, including `query`, `response`, `context`, and `ground_truth`. This simulates a scenario where we have user questions, the model's answers, plus a reference ground truth.\n",
    "\n",
    "You can adapt this approach to any domain: e.g., finance, e-commerce, etc.\n",
    "\n",
    "<img src=\"./seq-diagrams/2-evals.png\" alt=\"Evaluation Flow\" width=\"30%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b889daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# If you need to install these, uncomment:\n",
    "# !pip install azure-ai-projects azure-ai-evaluation azure-ai-inference azure-identity\n",
    "# !pip install opentelemetry-sdk azure-core-tracing-opentelemetry  # optional for advanced tracing\n",
    "\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# We'll create a synthetic dataset in JSON Lines format\n",
    "synthetic_eval_data = [\n",
    "    {\n",
    "        \"query\": \"How can I start a beginner workout routine at home?\",\n",
    "        \"context\": \"Workout routines can include push-ups, bodyweight squats, lunges, and planks.\",\n",
    "        \"response\": \"You can just go for 10 push-ups total.\",\n",
    "        \"ground_truth\": \"At home, you can start with short, low-intensity workouts: push-ups, lunges, planks.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Are diet sodas healthy for daily consumption?\",\n",
    "        \"context\": \"Sugar-free or diet drinks may reduce sugar intake, but they still contain artificial sweeteners.\",\n",
    "        \"response\": \"Yes, diet sodas are 100% healthy.\",\n",
    "        \"ground_truth\": \"Diet sodas have fewer sugars than regular soda, but 'healthy' is not guaranteed due to artificial additives.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the capital of France?\",\n",
    "        \"context\": \"France is in Europe. Paris is the capital.\",\n",
    "        \"response\": \"London.\",\n",
    "        \"ground_truth\": \"Paris.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Write them to a local JSONL file\n",
    "eval_data_path = Path(\"./health_fitness_eval_data.jsonl\")\n",
    "with eval_data_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for row in synthetic_eval_data:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "print(f\"Sample evaluation data written to {eval_data_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d5598",
   "metadata": {
    "id": "3-Local-Evaluation"
   },
   "source": [
    "# 3. ë¡œì»¬ í‰ê°€ ì˜ˆì‹œ\n",
    "\n",
    "We'll show how to run local, code-based evaluation on a JSONL dataset. We'll:\n",
    "1. **Load** the data.\n",
    "2. **Define** one or more evaluators. (e.g. `F1ScoreEvaluator`, `RelevanceEvaluator`, `GroundednessEvaluator`, or custom.)\n",
    "3. **Run** `evaluate(...)` to produce a dictionary of metrics.\n",
    "\n",
    "> We can also do multi-turn conversation data or add extra columns like `ground_truth` for advanced metrics.\n",
    "\n",
    "## Example 1: Combining F1Score, Relevance & Groundedness\n",
    "We'll combine:\n",
    "- `F1ScoreEvaluator` (NLP-based, compares `response` to `ground_truth`)\n",
    "- `RelevanceEvaluator` (AI-assisted, uses GPT to judge how well `response` addresses `query`)\n",
    "- `GroundednessEvaluator` (checks how well the response is anchored in the provided `context`)\n",
    "- A custom code-based evaluator that logs response length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f04f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.evaluation import (\n",
    "    evaluate,\n",
    "    F1ScoreEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    GroundednessEvaluator\n",
    ")\n",
    "\n",
    "# Our custom evaluator to measure response length.\n",
    "def response_length_eval(response, **kwargs):\n",
    "    return {\"resp_length\": len(response)}\n",
    "\n",
    "# We'll define an example GPT-based config (if we want AI-assisted evaluators). \n",
    "# This is needed for AI-assisted evaluators. Fill with your Azure OpenAI config.\n",
    "# If you skip some evaluators, you can omit.\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AOAI_ENDPOINT\", \"https://dummy-endpoint.azure.com\"),\n",
    "    \"api_key\": os.environ.get(\"AOAI_API_KEY\", \"fake-key\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AOAI_DEPLOYMENT\", \"gpt-4\"),\n",
    "    \"api_version\": os.environ.get(\"AOAI_API_VERSION\", \"2023-07-01-preview\"),\n",
    "}\n",
    "\n",
    "f1_eval = F1ScoreEvaluator()\n",
    "rel_eval = RelevanceEvaluator(model_config=model_config)\n",
    "ground_eval = GroundednessEvaluator(model_config=model_config)\n",
    "\n",
    "# We'll run evaluate(...) with these evaluators.\n",
    "results = evaluate(\n",
    "    data=str(eval_data_path),\n",
    "    evaluators={\n",
    "        \"f1_score\": f1_eval,\n",
    "        \"relevance\": rel_eval,\n",
    "        \"groundedness\": ground_eval,\n",
    "        \"resp_len\": response_length_eval\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"f1_score\": {\n",
    "            \"column_mapping\": {\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\"\n",
    "            }\n",
    "        },\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        },\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        },\n",
    "        \"resp_len\": {\n",
    "            \"column_mapping\": {\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Local evaluation result =>\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d525400",
   "metadata": {},
   "source": [
    "**Inspecting Local Results**\n",
    "\n",
    "The `evaluate(...)` call returns a dictionary with:\n",
    "- **`metrics`**: aggregated metrics across rows (like average F1, Relevance, or Groundedness)\n",
    "- **`rows`**: row-by-row results with inputs and evaluator outputs\n",
    "- **`traces`**: debugging info (if any)\n",
    "\n",
    "You can further analyze these results, store them in a database, or integrate them into your CI/CD pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b903ea",
   "metadata": {
    "id": "4-Cloud-Evaluation"
   },
   "source": [
    "# 4. `AIProjectClient`ë¡œ í´ë¼ìš°ë“œ í‰ê°€\n",
    "\n",
    "Sometimes, we want to:\n",
    "- Evaluate large or sensitive datasets in the cloud (scalability, governed access).\n",
    "- Keep track of evaluation results in an Azure AI Foundry project.\n",
    "- Optionally schedule recurring evaluations.\n",
    "\n",
    "We'll do that by:\n",
    "1. **Upload** the local JSONL to your Azure AI Foundry project.\n",
    "2. **Create** an `Evaluation` referencing built-in or custom evaluator definitions.\n",
    "3. **Poll** until the job is done (with retry logic for resilience).\n",
    "4. **Review** the results in the portal or via `project_client.evaluations.get(...)`.\n",
    "\n",
    "### Prerequisites\n",
    "- An Azure AI Foundry project with a valid **Connection String** (from your projectâ€™s Overview page).\n",
    "- An Azure OpenAI deployment (if using AI-assisted evaluators).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d936ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation, Dataset, EvaluatorConfiguration, ConnectionType\n",
    ")\n",
    "from azure.ai.evaluation import F1ScoreEvaluator, RelevanceEvaluator, ViolenceEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.exceptions import ServiceResponseError\n",
    "import time\n",
    "\n",
    "# 1) Connect to Azure AI Foundry project\n",
    "project_conn_str = os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=credential,\n",
    "    conn_str=project_conn_str\n",
    ")\n",
    "print(\"âœ… Created AIProjectClient.\")\n",
    "\n",
    "# 2) Upload data for evaluation\n",
    "uploaded_data_id, _ = project_client.upload_file(str(eval_data_path))\n",
    "print(\"âœ… Uploaded JSONL to project. Data asset ID:\", uploaded_data_id)\n",
    "\n",
    "# 3) Prepare an Azure OpenAI connection for AI-assisted evaluators\n",
    "default_conn = project_client.connections.get_default(ConnectionType.AZURE_OPEN_AI)\n",
    "\n",
    "deployment_name = os.environ.get(\"AOAI_DEPLOYMENT\", \"gpt-4\")\n",
    "api_version = os.environ.get(\"AOAI_API_VERSION\", \"2023-07-01-preview\")\n",
    "\n",
    "# 4) Construct the evaluation object\n",
    "model_config = default_conn.to_evaluator_model_config(\n",
    "    deployment_name=deployment_name,\n",
    "    api_version=api_version\n",
    ")\n",
    "\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Health Fitness Remote Evaluation\",\n",
    "    description=\"Evaluating dataset for correctness.\",\n",
    "    data=Dataset(id=uploaded_data_id),\n",
    "    evaluators={\n",
    "        \"f1_score\": EvaluatorConfiguration(id=F1ScoreEvaluator.id),\n",
    "        \"relevance\": EvaluatorConfiguration(\n",
    "            id=RelevanceEvaluator.id,\n",
    "            init_params={\"model_config\": model_config}\n",
    "        ),\n",
    "        \"violence\": EvaluatorConfiguration(\n",
    "            id=ViolenceEvaluator.id,\n",
    "            init_params={\"azure_ai_project\": project_client.scope}\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# Helper: Create evaluation with retry logic\n",
    "def create_evaluation_with_retry(project_client, evaluation, max_retries=3, retry_delay=5):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = project_client.evaluations.create(evaluation=evaluation)\n",
    "            return result\n",
    "        except ServiceResponseError as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            print(f\"âš ï¸ Attempt {attempt+1} failed: {str(e)}. Retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "# 5) Create & track the evaluation using retry logic\n",
    "cloud_eval = create_evaluation_with_retry(project_client, evaluation)\n",
    "print(\"âœ… Created evaluation job. ID:\", cloud_eval.id)\n",
    "\n",
    "# 6) Poll or fetch final status\n",
    "fetched_eval = project_client.evaluations.get(cloud_eval.id)\n",
    "print(\"Current status:\", fetched_eval.status)\n",
    "if hasattr(fetched_eval, 'properties'):\n",
    "    link = fetched_eval.properties.get(\"AiStudioEvaluationUri\", \"\")\n",
    "    if link:\n",
    "        print(\"View details in Foundry:\", link)\n",
    "else:\n",
    "    print(\"No link found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091290cd",
   "metadata": {},
   "source": [
    "### Viewing Cloud Evaluation Results\n",
    "- Navigate to the **Evaluations** tab in your AI Foundry project to see your evaluation job.\n",
    "- Open the evaluation to view aggregated metrics and row-level details.\n",
    "- For AI-assisted or risk & safety evaluators, you'll see both average scores and detailed per-row results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e3a2c4",
   "metadata": {},
   "source": [
    "# 5. ì¶”ê°€ ì£¼ì œ\n",
    "ëª‡ ê°€ì§€ ê³ ê¸‰ ê¸°ëŠ¥ì— ëŒ€í•´ ê°„ë‹¨íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "1. [ìœ„í—˜ ë° ì•ˆì „ í‰ê°€ì](#5.1-Risk-and-Safety)\n",
    "2. [ë” ë§ì€ í’ˆì§ˆ í‰ê°€ì](#5.2-Quality)\n",
    "3. [ì‚¬ìš©ì ì§€ì • í‰ê°€ì](#5.3-Custom)\n",
    "4. [ì‹œë®¬ë ˆì´í„° ë° ì ëŒ€ì  ë°ì´í„°](#5.4-Simulators)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7490e0eb",
   "metadata": {
    "id": "5.1-Risk-and-Safety"
   },
   "source": [
    "## 5.1 ìœ„í—˜ ë° ì•ˆì „ í‰ê°€ì\n",
    "\n",
    "Azure AI Foundryì—ëŠ” ì½˜í…ì¸  ìœ„í—˜ì„ ê°ì§€í•˜ëŠ” ê¸°ë³¸ ì œê³µ í‰ê°€ìê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì˜ˆëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "- **ViolenceEvaluator**: í­ë ¥ì ì´ê±°ë‚˜ ìœ í•´í•œ ì½˜í…ì¸ ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.\n",
    "- **SexualEvaluator**: ë…¸ê³¨ì ì¸ ì½˜í…ì¸ ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "- **HateUnfairnessEvaluator**: í˜ì˜¤ ì½˜í…ì¸ ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.\n",
    "- **SelfHarmEvaluator**: ìí•´ ê´€ë ¨ ì½˜í…ì¸ ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.\n",
    "- **ProtectedMaterialEvaluator**: ì €ì‘ê¶Œì´ ìˆê±°ë‚˜ ë³´í˜¸ë˜ëŠ” ì½˜í…ì¸ ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ëŸ¬í•œ í‰ê°€ìëŠ” `query`ì™€ `response`(ë•Œë¡œëŠ” `context`)ì„ ë°›ì•„ ì‹¬ê°ë„ ë ˆì´ë¸”ê³¼ ì ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "```python\n",
    "from azure.ai.evaluation import ViolenceEvaluator\n",
    "\n",
    "violence_eval = ViolenceEvaluator(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": \"...\",\n",
    "        \"resource_group_name\": \"...\",\n",
    "        \"project_name\": \"...\"\n",
    "    }\n",
    ")\n",
    "result = violence_eval(query=\"What is the capital of France?\", response=\"Paris\")\n",
    "print(result)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a94f46",
   "metadata": {
    "id": "5.2-Quality"
   },
   "source": [
    "## 5.2 ë” ë§ì€ í’ˆì§ˆ í‰ê°€ì\n",
    "`F1Score`ì™€ `Relevance` ì™¸ì—ë„ ë§ì€ ë‚´ì¥ ê¸°ëŠ¥ì´ ìˆìŠµë‹ˆë‹¤:\n",
    "- **GroundednessEvaluator**: ì‘ë‹µì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ê³ ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "- **CoherenceEvaluator**: ì‘ë‹µì˜ ë…¼ë¦¬ì  íë¦„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
    "- **FluencyEvaluator**: ë¬¸ë²•ì  ì •í™•ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ëŸ¬í•œ ë©”íŠ¸ë¦­ì€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f718e",
   "metadata": {
    "id": "5.3-Custom"
   },
   "source": [
    "## 5.3 ì‚¬ìš©ì ì§€ì • í‰ê°€ì\n",
    "í‰ê°€ìë¥¼ ì§ì ‘ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì‘ë‹µì˜ ê¸¸ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ê°„ë‹¨í•œ í‰ê°€ìë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "```python\n",
    "class AnswerLengthEvaluator:\n",
    "    def __call__(self, response: str, **kwargs):\n",
    "        return {\"answer_length\": len(response)}\n",
    "```\n",
    "\n",
    "ê·¸ëŸ° ë‹¤ìŒ ë¡œì»¬ ë˜ëŠ” í´ë¼ìš°ë“œ í‰ê°€ ì›Œí¬í”Œë¡œì™€ í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde67f1f",
   "metadata": {
    "id": "5.4-Simulators"
   },
   "source": [
    "## 5.4 ì‹œë®¬ë ˆì´í„° ë° ì ëŒ€ì  ë°ì´í„°\n",
    "í•©ì„± ë˜ëŠ” ì ëŒ€ì  í‰ê°€ ë°ì´í„°ë¥¼ ìƒì„±í•´ì•¼ í•˜ëŠ” ê²½ìš° `azure-ai-evaluation` íŒ¨í‚¤ì§€ëŠ” ì‹œë®¬ë ˆì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤. \n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´, ëª¨ë¸ ì•ˆì „ì„±ê³¼ ê²¬ê³ ì„±ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ `AdversarialSimulator`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ëŒ€ì  ì¿¼ë¦¬ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63eee3",
   "metadata": {
    "id": "6-Conclusion"
   },
   "source": [
    "# 6. ê²°ë¡  ğŸ\n",
    "\n",
    "ì—¬íƒœê¹Œì§€ ë‹¤ìŒ ì‚¬í•­ì„ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤:\n",
    "1. JSONL ë°ì´í„°ì—ì„œ `evaluate(...)`ë¥¼ ì‚¬ìš©í•œ **Local** í‰ê°€(ì´ì œ ê·¼ê±°ì„± ë©”íŠ¸ë¦­ í¬í•¨).\n",
    "2. ê²¬ê³ ì„±ì„ ìœ„í•œ ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•œ `AIProjectClient`ë¥¼ ì‚¬ìš©í•œ **Cloud** í‰ê°€.\n",
    "3. ë‚´ì¥ëœ **ìœ„í—˜ ë° ì•ˆì „** ë° **í’ˆì§ˆ** í‰ê°€ì.\n",
    "4. ê³ ê¸‰ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìœ„í•œ **ì»¤ìŠ¤í…€** í‰ê°€ì.\n",
    "5. ì ëŒ€ì  ë°ì´í„° ìƒì„±ì„ ìœ„í•œ **ì‹œë®¬ë ˆì´í„°**.\n",
    "\n",
    "\n",
    "**ë‹¤ìŒ ë‹¨ê³„**:\n",
    "- í‰ê°€ í”¼ë“œë°±ì— ë”°ë¼ ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.\n",
    "- ì´ëŸ¬í•œ í‰ê°€ë¥¼ CI/CD íŒŒì´í”„ë¼ì¸ì— í†µí•©í•˜ì„¸ìš”.\n",
    "- í†µí•© ê°€ì‹œì„± ë„êµ¬ì™€ ê²°í•©í•˜ì—¬ ë” ì‹¬ì¸µì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ìœ¼ì„¸ìš”.\n",
    "\n",
    "> **Best of lucìµœê³ ì˜ í–‰ìš´ì„ ë¹•ë‹ˆë‹¤** Azure AI Foundryë¡œ ê°•ë ¥í•˜ê³  ì±…ì„ê° ìˆëŠ” AI ì†”ë£¨ì…˜ì„ ë¹Œë“œí•˜ì„¸ìš”!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

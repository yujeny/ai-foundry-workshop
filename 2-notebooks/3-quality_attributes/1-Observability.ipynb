{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "54f0b7d7",
      "metadata": {},
      "source": [
        "# 🍏 `azure-ai-projects` 및 `azure-ai-inference`를 사용한 관찰 가능성 및 추적 데모 🍎\n",
        "\n",
        "이 **건강 및 피트니스** 테마 노트북에서는 **관측 가능성** 및 **추적**을 설정하는 방법을 살펴봅니다:\n",
        "\n",
        "1. `AIProjectClient`를 사용한 **기본 LLM 호출**.\n",
        "2. **에이전트**(예: 헬스 리소스 에이전트)를 사용한 **다단계** 상호작용.\n",
        "3. **console**(stdout) 또는 **OTLP 엔드포인트**(예: **Prompty** 또는 **Aspire**)를 통해 로컬 사용량 **추적**.\n",
        "4. 해당 **트레이스**를 **Azure Monitor**(애플리케이션 인사이트)로 전송하여 **Azure AI Foundry**에서 볼 수 있도록 합니다.\n",
        "\n",
        "> **고지 사항**: 이것은 AI 및 통합 가시성에 대한 재미있는 데모입니다! 코드 또는 프롬프트에서 운동, 식단 또는 건강 루틴에 대한 모든 참조는 순전히 **교육적** 목적으로만 사용됩니다. 건강에 대한 조언은 항상 전문가와 상의하세요.\n",
        "\n",
        "## 내용\n",
        "1. **초기화**: 환경 설정, 클라이언트 생성.\n",
        "2. **기본 LLM 호출**: 모델 completion 검색에 대한 빠른 데모.\n",
        "3. **연결**: 프로젝트 연결 목록 표시.\n",
        "4. **가시성 및 추적**\n",
        "   - **콘솔/로컬** 추적\n",
        "   - **Prompty / Aspire**: 추적을 로컬 OTLP 엔드포인트로 파이프하기\n",
        "   - **Azure Monitor** 추적: 애플리케이션 인사이트에 연결\n",
        "   - Azure AI Foundry에서 추적 **확인**하기\n",
        "5. **에이전트 기반 예제**:\n",
        "   - 샘플 문서를 참조하여 간단한 “상태 리소스 에이전트” 만들기.\n",
        "   - 추적을 사용한 다중 턴 대화.\n",
        "   - 정리.\n",
        "\n",
        "\n",
        "<img src=\"./seq-diagrams/1-observability.png\" width=\"50%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e13f9f3",
      "metadata": {},
      "source": [
        "## 1. 초기화 및 설정\n",
        "**전제 조건**:\n",
        "- 환경 변수 `PROJECT_CONNECTION_STRING`(및 선택적으로 `MODEL_DEPLOYMENT_NAME`)이 포함된 `.env` 파일.\n",
        "- 추론 및 에이전트 생성을 수행할 수 있는 Azure AI Foundry의 역할/권한.\n",
        "- `azure-ai-projects`, `azure-ai-inference`, `opentelemetry` 패키지가 설치된 로컬 환경.\n",
        "\n",
        "**할 일**:\n",
        "- 환경 변수를 로드합니다.\n",
        "- `AIProjectClient`를 초기화.\n",
        "- 모델(예: `gpt-4o`)과 대화할 수 있는지 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1ccdace",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.inference.models import UserMessage, CompletionsFinishReason\n",
        "\n",
        "# Load environment variables\n",
        "notebook_path = Path().absolute()\n",
        "env_path = notebook_path.parent.parent / '.env'  # Adjust path as needed\n",
        "load_dotenv(env_path)\n",
        "\n",
        "connection_string = os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
        "if not connection_string:\n",
        "    raise ValueError(\"🚨 PROJECT_CONNECTION_STRING not set in .env.\")\n",
        "\n",
        "# Initialize AIProjectClient\n",
        "try:\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        credential=DefaultAzureCredential(),\n",
        "        conn_str=connection_string\n",
        "    )\n",
        "    print(\"✅ Successfully created AIProjectClient!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error creating AIProjectClient: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e24461b",
      "metadata": {},
      "source": [
        "## 2. 기본 LLM 호출\n",
        "모든 것이 제대로 작동하는지 확인하기 위해 **빠른** 채팅 완료 요청을 할 것입니다. 간단한 질문을 하겠습니다: \"How many feet are in a mile?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7fcdaba",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    # Create a ChatCompletions client\n",
        "    inference_client = project_client.inference.get_chat_completions_client()\n",
        "    # Default to \"gpt-4o\" if no env var is set\n",
        "    model_name = os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\")\n",
        "\n",
        "    user_question = \"How many feet are in a mile?\"\n",
        "    response = inference_client.complete(\n",
        "        model=model_name,\n",
        "        messages=[UserMessage(content=user_question)]\n",
        "    )\n",
        "    print(\"\\n💡Response:\")\n",
        "    print(response.choices[0].message.content)\n",
        "    print(\"\\nFinish reason:\", response.choices[0].finish_reason)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"❌ Could not complete the chat request:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b83517e",
      "metadata": {},
      "source": [
        "## 3. 연결 나열하고 검사\n",
        "프로젝트에 있는 **연결**을 확인하세요. Azure OpenAI 또는 기타 리소스 첨부 파일일 수 있습니다. 데모를 위해 여기에 나열해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b70793c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.projects.models import ConnectionType\n",
        "\n",
        "all_conns = project_client.connections.list()\n",
        "print(f\"🔎 Found {len(all_conns)} total connections.\")\n",
        "for idx, c in enumerate(all_conns):\n",
        "    print(f\"{idx+1}) Name: {c.name}, Type: {c.connection_type}, Endpoint: {c.endpoint_url}\")\n",
        "\n",
        "# Filter for Azure OpenAI connections\n",
        "aoai_conns = project_client.connections.list(connection_type=ConnectionType.AZURE_OPEN_AI)\n",
        "print(f\"\\n🌀 Found {len(aoai_conns)} Azure OpenAI connections:\")\n",
        "for c in aoai_conns:\n",
        "    print(f\"   -> {c.name}\")\n",
        "\n",
        "# Get default connection of type AZURE_AI_SERVICES\n",
        "default_conn = project_client.connections.get_default(connection_type=ConnectionType.AZURE_AI_SERVICES,\n",
        "                                                     include_credentials=False)\n",
        "if default_conn:\n",
        "    print(\"\\n⭐ Default Azure AI Services connection:\")\n",
        "    print(default_conn)\n",
        "else:\n",
        "    print(\"No default connection found for Azure AI Services.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bce0c8f7",
      "metadata": {},
      "source": [
        "# 4. 가시성 및 추적\n",
        "\n",
        "예를 들어 LLM 통화에서 **텔레메트리를 수집**하려고 합니다. 예를 들어:\n",
        "- 요청의 타임스탬프.\n",
        "- 지연시간.\n",
        "- 잠재적 오류.\n",
        "- 선택 사항으로 실제 프롬프트 및 응답(콘텐츠 녹화를 활성화한 경우).\n",
        "\n",
        "설정 방법은 다음과 같습니다:\n",
        "1. **콘솔** 또는 로컬 OTLP 엔드포인트 계측.\n",
        "2. 애플리케이션 인사이트를 사용한 **Azure Monitor** 계측.\n",
        "3. Azure AI Foundry의 포털에서 추적 **보기**.\n",
        "\n",
        "## 4.1 로컬 콘솔 디버깅\n",
        "계측 패키지를 설치하고 활성화합니다. 그런 다음 빠른 채팅 호출을 통해 **stdout**에 로그가 표시되는지 확인합니다.\n",
        "\n",
        "\n",
        "**참고**: 더 고급 로컬 대시보드를 보고 싶으시다면 다음을 수행합니다::\n",
        "- [Prompty](https://github.com/microsoft/prompty) 사용.\n",
        "- [Aspire Dashboard](https://learn.microsoft.com/dotnet/aspire/fundamentals/dashboard/standalone?tabs=bash) 를 사용하여 OTLP 추적을 시각화합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d366bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# You only need to install these once.\n",
        "!pip install opentelemetry-instrumentation-openai-v2 opentelemetry-exporter-otlp-proto-grpc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4767143a",
      "metadata": {},
      "source": [
        "### 4.1.1 Azure AI 추론을 위한 OpenTelemetry 활성화\n",
        "환경 변수를 설정하여 다음을 보장합니다:\n",
        "1. **프롬프트 콘텐츠**가 캡처됩니다(선택 사항!).\n",
        "2. **Azure SDK**가 추적 구현으로 OpenTelemetry를 사용합니다.\n",
        "3. `AIInferenceInstrumentor().instrument()`를 호출하여 계측을 패치하고 활성화합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef06776",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from azure.ai.inference.tracing import AIInferenceInstrumentor\n",
        "\n",
        "# (Optional) capture prompt & completion contents in traces\n",
        "os.environ[\"AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED\"] = \"true\"  # or 'false'\n",
        "\n",
        "# Let the Azure SDK know we want to use OpenTelemetry\n",
        "os.environ[\"AZURE_SDK_TRACING_IMPLEMENTATION\"] = \"opentelemetry\"\n",
        "\n",
        "# Instrument the Azure AI Inference client library\n",
        "AIInferenceInstrumentor().instrument()\n",
        "print(\"✅ Azure AI Inference instrumentation enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "480fbc30",
      "metadata": {},
      "source": [
        "### 4.1.2 콘솔 또는 로컬 OTLP로 트레이스 지정\n",
        "가장 간단한 방법은 **stdout**으로 파이프하는 것입니다. **Prompty** 또는 **Aspire**로 보내려면 로컬 OTLP 엔드포인트 URL(일반적으로 `\"http://localhost:4317\"` 또는 이와 유사한 URL)을 지정하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d202f67d",
      "metadata": {},
      "outputs": [],
      "source": [
        "project_client.telemetry.enable(destination=sys.stdout)\n",
        "# Or, to send to a local OTLP collector (Prompty/Aspire), do:\n",
        "#   project_client.telemetry.enable(destination=\"http://localhost:4317\")\n",
        "\n",
        "try:\n",
        "    local_client = project_client.inference.get_chat_completions_client()\n",
        "    user_prompt = \"What's a simple 5-minute warmup routine?\"\n",
        "    local_resp = local_client.complete(\n",
        "        model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\"),\n",
        "        messages=[UserMessage(content=user_prompt)]\n",
        "    )\n",
        "    print(\"\\n🤖 Response:\", local_resp.choices[0].message.content)\n",
        "except Exception as exc:\n",
        "    print(f\"❌ Error in local-tracing example: {exc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3c0fdd4",
      "metadata": {},
      "source": [
        "## 4.2 Azure Monitor 추적 (애플리케이션 인사이트)\n",
        "이제 **애플리케이션 인사이트**에 추적을 설정하여 로그를 **Azure AI Foundry** **추적** 페이지로 전달합니다.\n",
        "\n",
        "**단계**:\n",
        "1. 이제 AI Foundry의 프로젝트 **Tracing**탭으로 이동하요, **애플리케이션 인사이트** 자원을 붙이거나 생성합니다.\n",
        "2. 코드에서 `project_client.telemetry.get_connection_string()`을 호출하여 계측 키를 검색합니다.\n",
        "3. 해당 연결과 함께 `azure.monitor.opentelemetry.configure_azure_monitor(...)`를 사용합니다.\n",
        "4. 추론 호출을 수행하면 -> 로그가 Foundry 포털(및 Azure Monitor 자체)에 표시됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0207221c",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install azure-monitor-opentelemetry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "552014a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.monitor.opentelemetry import configure_azure_monitor\n",
        "from azure.ai.inference.models import UserMessage\n",
        "\n",
        "app_insights_conn_str = project_client.telemetry.get_connection_string()\n",
        "if app_insights_conn_str:\n",
        "    print(\"🔧 Found App Insights connection string, configuring...\")\n",
        "    configure_azure_monitor(connection_string=app_insights_conn_str)\n",
        "    # Optionally add more instrumentation (for openai or langchain):\n",
        "    project_client.telemetry.enable()\n",
        "    \n",
        "    # Let's do a test call that logs to AI Foundry's Tracing page\n",
        "    try:\n",
        "        with project_client.inference.get_chat_completions_client() as client:\n",
        "            prompt_msg = \"Any easy at-home cardio exercise recommendations?\"\n",
        "            response = client.complete(\n",
        "                model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\"),\n",
        "                messages=[UserMessage(content=prompt_msg)]\n",
        "            )\n",
        "            print(\"\\n🤖 Response (logged to App Insights):\")\n",
        "            print(response.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print(\"❌ Chat completions with Azure Monitor example failed:\", e)\n",
        "else:\n",
        "    print(\"No Application Insights connection string is configured in this project.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4991833",
      "metadata": {},
      "source": [
        "### 4.3 Azure AI Foundry에서 트레이스 보기\n",
        "위의 코드를 실행한 후:\n",
        "1. AI Foundry 프로젝트로 이동합니다.\n",
        "2. 사이드바에서 **Tracing**을 클릭합니다.\n",
        "3. 호출에서 로그를 봅니다.\n",
        "4. 필요에 따라 로그를 필터링, 확장 또는 탐색합니다.\n",
        "\n",
        "또한 고급 대시보드가 필요한 경우, 파운드리에서 **애플리케이션 인사이트** 리소스를 열 수 있습니다. 앱 인사이트 포털에서는 **엔드투엔드 트랜잭션** 세부 정보, 쿼리 로그 등과 같은 추가 기능을 이용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31dbb932",
      "metadata": {},
      "source": [
        "# 5. 에이전트 기반 예제\n",
        "이제 레시피나 가이드라인에 대한 샘플 문서를 참조하는 **헬스 리소스 에이전트**를 만든 다음 시연해 보겠습니다:\n",
        "1. 지침이 포함된 에이전트 만들기.\n",
        "2. 대화 스레드 만들기.\n",
        "3. **observability**를 사용 설정한 상태에서 다단계 쿼리 실행하기.\n",
        "4. 선택적으로 마지막에 리소스 정리하기.\n",
        "\n",
        "> 에이전트 접근 방식은 보다 정교한 대화 흐름이나 **도구 사용**(예: 파일 검색)을 원할 때 유용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "303ad934",
      "metadata": {},
      "source": [
        "## 5.1 샘플 파일 및 벡터 저장소 만들기\n",
        "레시피/가이드라인에 대한 더미 `.md` 파일을 만든 다음 에이전트가 시맨틱 검색을 할 수 있도록 **벡터 스토어**에 푸시하겠습니다.\n",
        "\n",
        "(*이 부분은 간략하게 요약한 것입니다. 자세한 내용은 [the other file-search tutorial] 을 참고하세요.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e09113d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.projects.models import (\n",
        "    FileSearchTool,\n",
        "    FilePurpose,\n",
        "    MessageTextContent,\n",
        "    MessageRole\n",
        ")\n",
        "\n",
        "def create_sample_files():\n",
        "    \"\"\"Create some local .md files with sample text.\"\"\"\n",
        "    recipes_md = (\n",
        "        \"\"\"# Healthy Recipes Database\\n\\n\"\n",
        "        \"## Gluten-Free Recipes\\n\"\n",
        "        \"1. Quinoa Bowl\\n\"\n",
        "        \"   - Ingredients: quinoa, vegetables, olive oil\\n\"\n",
        "        \"   - Instructions: Cook quinoa, add vegetables\\n\\n\"\n",
        "        \"2. Rice Pasta\\n\"\n",
        "        \"   - Ingredients: rice pasta, mixed vegetables\\n\"\n",
        "        \"   - Instructions: Boil pasta, sauté vegetables\\n\\n\"\n",
        "        \"## Diabetic-Friendly Recipes\\n\"\n",
        "        \"1. Low-Carb Stir Fry\\n\"\n",
        "        \"   - Ingredients: chicken, vegetables, tamari sauce\\n\"\n",
        "        \"   - Instructions: Cook chicken, add vegetables\\n\\n\"\n",
        "        \"## Heart-Healthy Recipes\\n\"\n",
        "        \"1. Baked Salmon\\n\"\n",
        "        \"   - Ingredients: salmon, lemon, herbs\\n\"\n",
        "        \"   - Instructions: Season salmon, bake\\n\\n\"\n",
        "        \"2. Mediterranean Bowl\\n\"\n",
        "        \"   - Ingredients: chickpeas, vegetables, tahini\\n\"\n",
        "        \"   - Instructions: Combine ingredients\\n\"\"\"\n",
        "    )\n",
        "\n",
        "    guidelines_md = (\n",
        "        \"\"\"# Dietary Guidelines\\n\\n\"\n",
        "        \"## General Guidelines\\n\"\n",
        "        \"- Eat a variety of foods\\n\"\n",
        "        \"- Control portion sizes\\n\"\n",
        "        \"- Stay hydrated\\n\\n\"\n",
        "        \"## Special Diets\\n\"\n",
        "        \"1. Gluten-Free Diet\\n\"\n",
        "        \"   - Avoid wheat, barley, rye\\n\"\n",
        "        \"   - Focus on naturally gluten-free foods\\n\\n\"\n",
        "        \"2. Diabetic Diet\\n\"\n",
        "        \"   - Monitor carbohydrate intake\\n\"\n",
        "        \"   - Choose low glycemic foods\\n\\n\"\n",
        "        \"3. Heart-Healthy Diet\\n\"\n",
        "        \"   - Limit saturated fats\\n\"\n",
        "        \"   - Choose lean proteins\\n\"\"\"\n",
        "    )\n",
        "\n",
        "    with open(\"recipes.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(recipes_md)\n",
        "    with open(\"guidelines.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(guidelines_md)\n",
        "\n",
        "    print(\"📄 Created sample resource files: recipes.md, guidelines.md\")\n",
        "    return [\"recipes.md\", \"guidelines.md\"]\n",
        "\n",
        "sample_files = create_sample_files()\n",
        "\n",
        "def create_vector_store(files, store_name=\"my_health_resources\"):\n",
        "    try:\n",
        "        uploaded_ids = []\n",
        "        for fp in files:\n",
        "            upl = project_client.agents.upload_file_and_poll(\n",
        "                file_path=fp,\n",
        "                purpose=FilePurpose.AGENTS  # Add FilePurpose.AGENTS here\n",
        "            )\n",
        "            uploaded_ids.append(upl.id)\n",
        "            print(f\"✅ Uploaded: {fp} -> File ID: {upl.id}\")\n",
        "\n",
        "        # Create vector store from these file IDs\n",
        "        vs = project_client.agents.create_vector_store_and_poll(\n",
        "            file_ids=uploaded_ids,\n",
        "            name=store_name\n",
        "        )\n",
        "        print(f\"🎉 Created vector store '{store_name}', ID: {vs.id}\")\n",
        "        return vs, uploaded_ids\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error creating vector store: {e}\")\n",
        "        return None, []\n",
        "\n",
        "vector_store, file_ids = None, []\n",
        "if sample_files:\n",
        "    vector_store, file_ids = create_vector_store(sample_files, store_name=\"health_resources_example\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "145eb186",
      "metadata": {},
      "source": [
        "## 5.2 상태 리소스 에이전트 생성\n",
        "벡터 저장소를 참조하는 **FileSearchTool**를 생성한 다음, 에이전트에 필요한 지침을 사용하여 에이전트를 생성하겠습니다:\n",
        "1. 면책 조항을 제공합니다.\n",
        "2. 일반적인 영양 또는 레시피 팁을 제공합니다.\n",
        "3. 가능한 경우 출처를 인용합니다.\n",
        "4. 보다 심층적인 의학적 조언을 위해 전문가 상담을 권장합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f604175",
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.projects.models import FileSearchTool, FilePurpose\n",
        "from azure.ai.projects.models import ConnectionType, MessageTextContent, MessageRole\n",
        "\n",
        "def create_health_agent(vs_id):\n",
        "    try:\n",
        "        # The tool references our vector store so the agent can search it\n",
        "        file_search_tool = FileSearchTool(vector_store_ids=[vs_id])\n",
        "        \n",
        "        instructions = \"\"\"\n",
        "            You are a health resource advisor with access to dietary and recipe files.\n",
        "            You:\n",
        "            1. Always present disclaimers (you're not a medical professional)\n",
        "            2. Provide references to files when possible\n",
        "            3. Focus on general nutrition or recipe tips.\n",
        "            4. Encourage professional consultation for more detailed advice.\n",
        "        \"\"\"\n",
        "\n",
        "        agent = project_client.agents.create_agent(\n",
        "            model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\"),\n",
        "            name=\"health-search-agent\",\n",
        "            instructions=instructions,\n",
        "            tools=file_search_tool.definitions,\n",
        "            tool_resources=file_search_tool.resources\n",
        "        )\n",
        "        print(f\"🎉 Created agent '{agent.name}' with ID: {agent.id}\")\n",
        "        return agent\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error creating health agent: {e}\")\n",
        "        return None\n",
        "\n",
        "health_agent = None\n",
        "if vector_store:\n",
        "    health_agent = create_health_agent(vector_store.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f6995a6",
      "metadata": {},
      "source": [
        "## 5.3 에이전트 사용하기\n",
        "새 대화 **쓰레드**를 만들어 상담원에게 몇 가지 질문을 해 보겠습니다. 각 단계를 추적할 수 있도록 이미 구성한 **observability** 설정을 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86e5b4f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_thread():\n",
        "    try:\n",
        "        thread = project_client.agents.create_thread()\n",
        "        print(f\"📝 Created new thread, ID: {thread.id}\")\n",
        "        return thread\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Could not create thread: {e}\")\n",
        "        return None\n",
        "\n",
        "def ask_question(thread_id, agent_id, user_question):\n",
        "    try:\n",
        "        # 1) Add user message\n",
        "        msg = project_client.agents.create_message(\n",
        "            thread_id=thread_id,\n",
        "            role=\"user\",\n",
        "            content=user_question\n",
        "        )\n",
        "        print(f\"User asked: '{user_question}'\")\n",
        "        # 2) Create & process a run\n",
        "        run = project_client.agents.create_and_process_run(\n",
        "            thread_id=thread_id,\n",
        "            assistant_id=agent_id\n",
        "        )\n",
        "        print(f\"Run finished with status: {run.status}\")\n",
        "        if run.last_error:\n",
        "            print(\"Error details:\", run.last_error)\n",
        "        return run\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error asking question: {e}\")\n",
        "        return None\n",
        "\n",
        "if health_agent:\n",
        "    thread = create_thread()\n",
        "    if thread:\n",
        "        # Let's ask a few sample questions\n",
        "        queries = [\n",
        "            \"Could you suggest a gluten-free lunch recipe?\",\n",
        "            \"Show me some heart-healthy meal ideas.\",\n",
        "            \"What guidelines do you have for someone with diabetes?\"\n",
        "        ]\n",
        "        for q in queries:\n",
        "            ask_question(thread.id, health_agent.id, q)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17c61d8d",
      "metadata": {},
      "source": [
        "### 5.3.1 대화 보기\n",
        "대화 메시지를 검색하여 상담원이 어떻게 응답했는지, 파일 내용을 인용했는지 등을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c57935",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_thread(thread_id):\n",
        "    try:\n",
        "        messages = project_client.agents.list_messages(thread_id=thread_id)\n",
        "        print(\"\\n🗣️ Conversation:\")\n",
        "        for m in reversed(messages.data):\n",
        "            if m.content:\n",
        "                last_content = m.content[-1]\n",
        "                if hasattr(last_content, \"text\"):\n",
        "                    print(f\"[{m.role.upper()}]: {last_content.text.value}\\n\")\n",
        "\n",
        "        print(\"\\n📎 Checking for citations...\")\n",
        "        for c in messages.file_citation_annotations:\n",
        "            print(f\"- Citation snippet: '{c.text}' from file ID: {c.file_citation['file_id']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Could not display thread: {e}\")\n",
        "\n",
        "# If we created a thread above, let's read it\n",
        "if health_agent and thread:\n",
        "    display_thread(thread.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7420c39",
      "metadata": {},
      "source": [
        "# 6. 정리\n",
        "원하는 경우 벡터 저장소, 파일 및 에이전트를 제거하여 깔끔하게 정리할 수 있습니다. (프로덕션에서는 계속 유지할 수도 있습니다.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f473cddc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleanup_resources():\n",
        "    try:\n",
        "        if 'vector_store' in globals() and vector_store:\n",
        "            project_client.agents.delete_vector_store(vector_store.id)\n",
        "            print(\"🗑️ Deleted vector store.\")\n",
        "\n",
        "        if 'file_ids' in globals() and file_ids:\n",
        "            for fid in file_ids:\n",
        "                project_client.agents.delete_file(fid)\n",
        "            print(\"🗑️ Deleted uploaded files.\")\n",
        "\n",
        "        if 'health_agent' in globals() and health_agent:\n",
        "            project_client.agents.delete_agent(health_agent.id)\n",
        "            print(\"🗑️ Deleted health agent.\")\n",
        "\n",
        "        if 'sample_files' in globals() and sample_files:\n",
        "            for sf in sample_files:\n",
        "                if os.path.exists(sf):\n",
        "                    os.remove(sf)\n",
        "            print(\"🗑️ Deleted local sample files.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error cleaning up: {e}\")\n",
        "\n",
        "\n",
        "cleanup_resources()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4956d0ec",
      "metadata": {},
      "source": [
        "# 🎉 마무리\n",
        "다음을 시연해 보았습니다:\n",
        "1. `AIProjectClient`로 **기본 LLM 호출**.\n",
        "2. Azure AI Foundry 프로젝트에서 **연결 나열하기**.\n",
        "3. 로컬(콘솔, OTLP 엔드포인트) 및 클라우드(앱 인사이트) 컨텍스트에서 **Observability & tracing**.\n",
        "4. 샘플 문서 검색을 위해 벡터 저장소를 사용하는 빠른 **Agent** 시나리오.\n",
        "\n",
        "## 다음 단계\n",
        "- Azure AI Foundry 포털에서 **Tracing** 탭을 확인하여 로그를 확인합니다.\n",
        "- 애플리케이션 인사이트에서 고급 쿼리를 살펴봅니다.\n",
        "- 로컬 원격 분석 대시보드의 경우 [Prompty](https://github.com/microsoft/prompty) 또는 [Aspire](https://learn.microsoft.com/dotnet/aspire/) 를 사용하세요.\n",
        "- 이 접근 방식을 **프로덕션** GenAI 파이프라인에 통합하세요!\n",
        "\n",
        "> 🏋️ **건강 알림**: LLM의 제안은 데모용으로만 제공됩니다. 실제 건강 관련 결정은 전문가와 상의하세요.\n",
        "\n",
        "행복한 관찰과 추적되세요! 🎉"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.11.11"
    },
    "name": "Observability_and_Tracing_Comprehensive"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

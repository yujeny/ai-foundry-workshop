{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3d8f7b1",
      "metadata": {},
      "source": [
        "# 🚀 Azure AI 추론이 포함된 DeepSeek-R1 모델  🧠\n",
        "\n",
        "**DeepSeek-R1**은 강화 학습과 감독 미세 조정을 결합한 최신 추론 모델로, 37억 개의 활성 매개 변수와 128K 컨텍스트 창으로 복잡한 추론 작업에 탁월한 성능을 발휘합니다.\n",
        "\n",
        "이 노트북에서는 다음을 학습합니다:\n",
        "1. Azure 서버리스 엔드포인트용 ChatCompletionsClient **초기화**하기\n",
        "2. 추론 추출을 사용하여 DeepSeek-R1과 **채팅**하기\n",
        "3. 단계별 추론으로 여행 계획 예제를 **구현**\n",
        "4. 복잡한 시나리오를 위해 128K 컨텍스트 창을 **활용**\n",
        "\n",
        "## 왜 DeepSeek-R1인가?\n",
        "- **고급 추론**: 연쇄적 사고 문제 해결에 특화되어 있습니다.\n",
        "- **매시브 컨텍스트**: 상세한 분석을 위한 128K 토큰 창\n",
        "- **효율적인 아키텍처**: 총 671억 개 중 37억 개의 활성 매개변수\n",
        "- **안전 통합**: 콘텐츠 필터링 기능 내장\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e3a4c2",
      "metadata": {},
      "source": [
        "## 1. 설정 및 인증\n",
        "\n",
        "필수 패키지:\n",
        "- `azure-ai-inference`: 채팅 완료용\n",
        "- `python-dotenv`: 환경 변수용\n",
        "\n",
        ".env 파일 요구 사항:\n",
        "```bash\n",
        "AZURE_INFERENCE_ENDPOINT=<your-endpoint-url>\n",
        "AZURE_INFERENCE_KEY=<your-api-key>\n",
        "MODEL_NAME=DeepSeek-R1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53f8d4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Load environment\n",
        "load_dotenv()\n",
        "endpoint = os.getenv(\"AZURE_INFERENCE_ENDPOINT\")\n",
        "key = os.getenv(\"AZURE_INFERENCE_KEY\")\n",
        "model_name = os.getenv(\"MODEL_NAME\", \"DeepSeek-R1\")\n",
        "\n",
        "# Initialize client\n",
        "try:\n",
        "    client = ChatCompletionsClient(\n",
        "        endpoint=endpoint,\n",
        "        credential=AzureKeyCredential(key)\n",
        "    )\n",
        "    print(\"✅ Client initialized | Model:\", client.get_model_info().model_name)\n",
        "except Exception as e:\n",
        "    print(\"❌ Initialization failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c01d5d9",
      "metadata": {},
      "source": [
        "## 2. 똑똑한 여행 계획 ✈️\n",
        "\n",
        "여행 계획을 위한 DeepSeek-R1의 추론 기능을 시연합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6a5d8d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plan_trip_with_reasoning(query, show_thinking=False):\n",
        "    \"\"\"Get travel recommendations with reasoning extraction\"\"\"\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a travel expert. Provide detailed plans with rationale.\"),\n",
        "        UserMessage(content=f\"{query} Include hidden gems and safety considerations.\")\n",
        "    ]\n",
        "    \n",
        "    response = client.complete(\n",
        "        messages=messages,\n",
        "        model=model_name,\n",
        "        temperature=0.7,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    \n",
        "    content = response.choices[0].message.content\n",
        "    \n",
        "    # Extract reasoning if present\n",
        "    if show_thinking:\n",
        "        match = re.search(r\"<think>(.*?)</think>(.*)\", content, re.DOTALL)\n",
        "        if match:\n",
        "            return {\"thinking\": match.group(1).strip(), \"answer\": match.group(2).strip()}\n",
        "    return content\n",
        "\n",
        "# Example usage\n",
        "query = \"Plan a 5-day cultural trip to Kyoto in April\"\n",
        "result = plan_trip_with_reasoning(query, show_thinking=True)\n",
        "\n",
        "print(\"🗺️ Query:\", query)\n",
        "if isinstance(result, dict):\n",
        "    print(\"\\n🧠 Thinking Process:\", result[\"thinking\"])\n",
        "    print(\"\\n📝 Final Answer:\", result[\"answer\"])\n",
        "else:\n",
        "    print(\"\\n📝 Response:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8f1b3a",
      "metadata": {},
      "source": [
        "## 3. 기술적 문제 해결 💻\n",
        "\n",
        "코딩/최적화 기능을 보여주세요:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5d4a3e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def solve_technical_problem(problem):\n",
        "    \"\"\"Solve complex technical problems with structured reasoning\"\"\"\n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            UserMessage(content=f\"{problem} Please reason step by step, and put your final answer within \\boxed{{}}.\")\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.3,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Database optimization example\n",
        "problem = \"\"\"How can I optimize a PostgreSQL database handling 10k transactions/second?\n",
        "Consider indexing strategies, hardware requirements, and query optimization.\"\"\"\n",
        "\n",
        "print(\"🔧 Problem:\", problem)\n",
        "print(\"\\n⚙️ Solution:\", solve_technical_problem(problem))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b9f7a8c",
      "metadata": {},
      "source": [
        "## 4. 모범 사례 및 고려 사항\n",
        "\n",
        "1. **추론 처리**: 정규식을 사용하여 <think> 콘텐츠와 최종 답변을 구분합니다.\n",
        "2. **안전**: 빌트인 콘텐츠 필터링 - 위반 시 HttpResponseError 처리\n",
        "3. **성능**:\n",
        "   - 최대 토큰: 4096\n",
        "   - 속도 제한: 분당 200K 토큰\n",
        "4. **비용**: 서버리스 배포를 통한 종량제 요금제\n",
        "5. **스트리밍**: 장시간 completion을 위한 응답 스트리밍 구현\n",
        "\n",
        "```python\n",
        "# Streaming example\n",
        "response = client.complete(..., stream=True)\n",
        "for chunk in response:\n",
        "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
        "```\n",
        "\n",
        "## 🎯 주요 요점\n",
        "- 자세한 분석을 위해 128K 컨텍스트 활용\n",
        "- 디버깅/분석을 위한 추론 단계 추출\n",
        "- 프로덕션을 위해 Azure AI Content Safety와 결합\n",
        "- response.usage를 통해 토큰 사용량 모니터링\n",
        "\n",
        "> 중요한 애플리케이션에 대해 항상 모델 출력의 유효성을 검사하세요!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3d8f7b1",
      "metadata": {},
      "source": [
        "# ğŸš€ Azure AI ì¶”ë¡ ì´ í¬í•¨ëœ DeepSeek-R1 ëª¨ë¸  ğŸ§ \n",
        "\n",
        "**DeepSeek-R1**ì€ ê°•í™” í•™ìŠµê³¼ ê°ë… ë¯¸ì„¸ ì¡°ì •ì„ ê²°í•©í•œ ìµœì‹  ì¶”ë¡  ëª¨ë¸ë¡œ, 37ì–µ ê°œì˜ í™œì„± ë§¤ê°œ ë³€ìˆ˜ì™€ 128K ì»¨í…ìŠ¤íŠ¸ ì°½ìœ¼ë¡œ ë³µì¡í•œ ì¶”ë¡  ì‘ì—…ì— íƒì›”í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ë‹¤ìŒì„ í•™ìŠµí•©ë‹ˆë‹¤:\n",
        "1. Azure ì„œë²„ë¦¬ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ìš© ChatCompletionsClient **ì´ˆê¸°í™”**í•˜ê¸°\n",
        "2. ì¶”ë¡  ì¶”ì¶œì„ ì‚¬ìš©í•˜ì—¬ DeepSeek-R1ê³¼ **ì±„íŒ…**í•˜ê¸°\n",
        "3. ë‹¨ê³„ë³„ ì¶”ë¡ ìœ¼ë¡œ ì—¬í–‰ ê³„íš ì˜ˆì œë¥¼ **êµ¬í˜„**\n",
        "4. ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìœ„í•´ 128K ì»¨í…ìŠ¤íŠ¸ ì°½ì„ **í™œìš©**\n",
        "\n",
        "## ì™œ DeepSeek-R1ì¸ê°€?\n",
        "- **ê³ ê¸‰ ì¶”ë¡ **: ì—°ì‡„ì  ì‚¬ê³  ë¬¸ì œ í•´ê²°ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "- **ë§¤ì‹œë¸Œ ì»¨í…ìŠ¤íŠ¸**: ìƒì„¸í•œ ë¶„ì„ì„ ìœ„í•œ 128K í† í° ì°½\n",
        "- **íš¨ìœ¨ì ì¸ ì•„í‚¤í…ì²˜**: ì´ 671ì–µ ê°œ ì¤‘ 37ì–µ ê°œì˜ í™œì„± ë§¤ê°œë³€ìˆ˜\n",
        "- **ì•ˆì „ í†µí•©**: ì½˜í…ì¸  í•„í„°ë§ ê¸°ëŠ¥ ë‚´ì¥\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e3a4c2",
      "metadata": {},
      "source": [
        "## 1. ì„¤ì • ë° ì¸ì¦\n",
        "\n",
        "í•„ìˆ˜ íŒ¨í‚¤ì§€:\n",
        "- `azure-ai-inference`: ì±„íŒ… ì™„ë£Œìš©\n",
        "- `python-dotenv`: í™˜ê²½ ë³€ìˆ˜ìš©\n",
        "\n",
        ".env íŒŒì¼ ìš”êµ¬ ì‚¬í•­:\n",
        "```bash\n",
        "AZURE_INFERENCE_ENDPOINT=<your-endpoint-url>\n",
        "AZURE_INFERENCE_KEY=<your-api-key>\n",
        "MODEL_NAME=DeepSeek-R1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53f8d4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Load environment\n",
        "load_dotenv()\n",
        "endpoint = os.getenv(\"AZURE_INFERENCE_ENDPOINT\")\n",
        "key = os.getenv(\"AZURE_INFERENCE_KEY\")\n",
        "model_name = os.getenv(\"MODEL_NAME\", \"DeepSeek-R1\")\n",
        "\n",
        "# Initialize client\n",
        "try:\n",
        "    client = ChatCompletionsClient(\n",
        "        endpoint=endpoint,\n",
        "        credential=AzureKeyCredential(key)\n",
        "    )\n",
        "    print(\"âœ… Client initialized | Model:\", client.get_model_info().model_name)\n",
        "except Exception as e:\n",
        "    print(\"âŒ Initialization failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c01d5d9",
      "metadata": {},
      "source": [
        "## 2. ë˜‘ë˜‘í•œ ì—¬í–‰ ê³„íš âœˆï¸\n",
        "\n",
        "ì—¬í–‰ ê³„íšì„ ìœ„í•œ DeepSeek-R1ì˜ ì¶”ë¡  ê¸°ëŠ¥ì„ ì‹œì—°í•©ë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6a5d8d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plan_trip_with_reasoning(query, show_thinking=False):\n",
        "    \"\"\"Get travel recommendations with reasoning extraction\"\"\"\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a travel expert. Provide detailed plans with rationale.\"),\n",
        "        UserMessage(content=f\"{query} Include hidden gems and safety considerations.\")\n",
        "    ]\n",
        "    \n",
        "    response = client.complete(\n",
        "        messages=messages,\n",
        "        model=model_name,\n",
        "        temperature=0.7,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    \n",
        "    content = response.choices[0].message.content\n",
        "    \n",
        "    # Extract reasoning if present\n",
        "    if show_thinking:\n",
        "        match = re.search(r\"<think>(.*?)</think>(.*)\", content, re.DOTALL)\n",
        "        if match:\n",
        "            return {\"thinking\": match.group(1).strip(), \"answer\": match.group(2).strip()}\n",
        "    return content\n",
        "\n",
        "# Example usage\n",
        "query = \"Plan a 5-day cultural trip to Kyoto in April\"\n",
        "result = plan_trip_with_reasoning(query, show_thinking=True)\n",
        "\n",
        "print(\"ğŸ—ºï¸ Query:\", query)\n",
        "if isinstance(result, dict):\n",
        "    print(\"\\nğŸ§  Thinking Process:\", result[\"thinking\"])\n",
        "    print(\"\\nğŸ“ Final Answer:\", result[\"answer\"])\n",
        "else:\n",
        "    print(\"\\nğŸ“ Response:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8f1b3a",
      "metadata": {},
      "source": [
        "## 3. ê¸°ìˆ ì  ë¬¸ì œ í•´ê²° ğŸ’»\n",
        "\n",
        "ì½”ë”©/ìµœì í™” ê¸°ëŠ¥ì„ ë³´ì—¬ì£¼ì„¸ìš”:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5d4a3e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def solve_technical_problem(problem):\n",
        "    \"\"\"Solve complex technical problems with structured reasoning\"\"\"\n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            UserMessage(content=f\"{problem} Please reason step by step, and put your final answer within \\boxed{{}}.\")\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.3,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Database optimization example\n",
        "problem = \"\"\"How can I optimize a PostgreSQL database handling 10k transactions/second?\n",
        "Consider indexing strategies, hardware requirements, and query optimization.\"\"\"\n",
        "\n",
        "print(\"ğŸ”§ Problem:\", problem)\n",
        "print(\"\\nâš™ï¸ Solution:\", solve_technical_problem(problem))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b9f7a8c",
      "metadata": {},
      "source": [
        "## 4. ëª¨ë²” ì‚¬ë¡€ ë° ê³ ë ¤ ì‚¬í•­\n",
        "\n",
        "1. **ì¶”ë¡  ì²˜ë¦¬**: ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ <think> ì½˜í…ì¸ ì™€ ìµœì¢… ë‹µë³€ì„ êµ¬ë¶„í•©ë‹ˆë‹¤.\n",
        "2. **ì•ˆì „**: ë¹ŒíŠ¸ì¸ ì½˜í…ì¸  í•„í„°ë§ - ìœ„ë°˜ ì‹œ HttpResponseError ì²˜ë¦¬\n",
        "3. **ì„±ëŠ¥**:\n",
        "   - ìµœëŒ€ í† í°: 4096\n",
        "   - ì†ë„ ì œí•œ: ë¶„ë‹¹ 200K í† í°\n",
        "4. **ë¹„ìš©**: ì„œë²„ë¦¬ìŠ¤ ë°°í¬ë¥¼ í†µí•œ ì¢…ëŸ‰ì œ ìš”ê¸ˆì œ\n",
        "5. **ìŠ¤íŠ¸ë¦¬ë°**: ì¥ì‹œê°„ completionì„ ìœ„í•œ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„\n",
        "\n",
        "```python\n",
        "# Streaming example\n",
        "response = client.complete(..., stream=True)\n",
        "for chunk in response:\n",
        "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
        "```\n",
        "\n",
        "## ğŸ¯ ì£¼ìš” ìš”ì \n",
        "- ìì„¸í•œ ë¶„ì„ì„ ìœ„í•´ 128K ì»¨í…ìŠ¤íŠ¸ í™œìš©\n",
        "- ë””ë²„ê¹…/ë¶„ì„ì„ ìœ„í•œ ì¶”ë¡  ë‹¨ê³„ ì¶”ì¶œ\n",
        "- í”„ë¡œë•ì…˜ì„ ìœ„í•´ Azure AI Content Safetyì™€ ê²°í•©\n",
        "- response.usageë¥¼ í†µí•´ í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§\n",
        "\n",
        "> ì¤‘ìš”í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ëŒ€í•´ í•­ìƒ ëª¨ë¸ ì¶œë ¥ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•˜ì„¸ìš”!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
